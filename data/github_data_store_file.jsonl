{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/_templates/integration.mdx"}, "data": "[comment: Please, a reference example here \"docs/integrations/arxiv.md\"]:: [comment: Use this template to create a new .md file in \"docs/integrations/\"]:: # Title_REPLACE_ME [comment: Only one Tile/H1 is allowed!]:: > [comment: Description: After reading this description, a reader should decide if this integration is good enough to try/follow reading OR]:: [comment: go to read the next integration doc. ]:: [comment: Description should include a link to the source for follow reading.]:: ## Installation and Setup [comment: Installation and Setup: All necessary additional package installations and setups for Tokens, etc]:: ```bash pip install package_name_REPLACE_ME ``` [comment: OR this text:]:: There isn't any special setup for it. [comment: The next H2/## sections with names of the integration modules, like \"LLM\", \"Text Embedding Models\", etc]:: [comment: see \"Modules\" in the \"index.html\" page]:: [comment: Each H2 section should include a link to an example(s) and a Python code with the import of the integration class]:: [comment: Below are several example sections. Remove all unnecessary sections. Add all necessary sections not provided here.]:: ## LLM See a [usage example](/docs/integrations/llms/INCLUDE_REAL_NAME). ```python from langchain.llms import integration_class_REPLACE_ME ``` ## Text Embedding Models See a [usage example](/docs/integrations/text_embedding/INCLUDE_REAL_NAME) ```python from langchain.embeddings import integration_class_REPLACE_ME ``` ## Chat models See a [usage example](/docs/integrations/chat/INCLUDE_REAL_NAME) ```python from langchain.chat_models import integration_class_REPLACE_ME ``` ## Document Loader See a [usage example](/docs/integrations/document_loaders/INCLUDE_REAL_NAME). ```python from langchain.document_loaders import integration_class_REPLACE_ME ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/additional_resources/dependents.mdx"}, "data": "# Dependents Dependents stats for `langchain-ai/langchain` [![]( [![]( [![]( [![]( [update: `2023-10-06`; only dependent repositories with Stars > 100] | Repository | Stars | | :-------- | -----: | |[openai/openai-cookbook]( | 49006 | |[AntonOsika/gpt-engineer]( | 44368 | |[imartinez/privateGPT]( | 38300 | |[LAION-AI/Open-Assistant]( | 35327 | |[hpcaitech/ColossalAI]( | 34799 | |[microsoft/TaskMatrix]( | 34161 | |[streamlit/streamlit]( | 27697 | |[geekan/MetaGPT]( | 27302 | |[reworkd/AgentGPT]( | 26805 | |[OpenBB-finance/OpenBBTerminal]( | 24473 | |[StanGirard/quivr]( | 23323 | |[run-llama/llama_index]( | 22151 | |[openai/chatgpt-retrieval-plugin]( | 19741 | |[mindsdb/mindsdb]( | 18062 | |[PromtEngineer/localGPT]( | 16413 | |[chatchat-space/Langchain-Chatchat]( | 16300 | |[cube-js/cube]( | 16261 | |[mlflow/mlflow]( | 15487 | |[logspace-ai/langflow]( | 12599 | |[GaiZhenbiao/ChuanhuChatGPT]( | 12501 | |[openai/evals]( | 12056 | |[airbytehq/airbyte]( | 11919 | |[go-skynet/LocalAI]( | 11767 | |[databrickslabs/dolly]( | 10609 | |[AIGC-Audio/AudioGPT]( | 9240 | |[aws/amazon-sagemaker-examples]( | 8892 | |[langgenius/dify]( | 8764 | |[gventuri/pandas-ai]( | 8687 | |[jmorganca/ollama]( | 8628 | |[langchain-ai/langchainjs]( | 8392 | |[h2oai/h2ogpt]( | 7953 | |[arc53/DocsGPT]( | 7730 | |[PipedreamHQ/pipedream]( | 7261 | |[joshpxyne/gpt-migrate]( | 6349 | |[bentoml/OpenLLM]( | 6213 | |[mage-ai/mage-ai]( | 5600 | |[zauberzeug/nicegui]( | 5499 | |[wenda-LLM/wenda]( | 5497 | |[sweepai/sweep]( | 5489 | |[embedchain/embedchain]( | 5428 | |[zilliztech/GPTCache]( | 5311 | |[Shaunwei/RealChar]( | 5264 | |[GreyDGL/PentestGPT]( | 5146 | |[gkamradt/langchain-tutorials]( | 5134 | |[serge-chat/serge]( | 5009 | |[assafelovic/gpt-researcher]( | 4836 | |[openchatai/OpenChat]( | 4697 | |[intel-analytics/BigDL]( | 4412 | |[continuedev/continue]( | 4324 | |[postgresml/postgresml]( | 4267 | |[madawei2699/myGPTReader]( | 4214 | |[MineDojo/Voyager]( | 4204 | |[danswer-ai/danswer]( | 3973 | |[RayVentura/ShortGPT]( | 3922 | |[Azure/azure-sdk-for-python]( | 3849 | |[khoj-ai/khoj]( | 3817 | |[langchain-ai/chat-langchain]( | 3742 | |[Azure-Samples/azure-search-openai-demo]( | 3731 | |[marqo-ai/marqo]( | 3627 | |[kyegomez/tree-of-thoughts]( | 3553 | |[llm-workflow-engine/llm-workflow-engine]( | 3483 | |[PrefectHQ/marvin]( | 3460 | |[aiwaves-cn/agents]( | 3413 | |[OpenBMB/ToolBench]( | 3388 | |[shroominic/codeinterpreter-api]( | 3218 | |[whitead/paper-qa]( | 3085 | |[project-baize/baize-chatbot]( | 3039 | |[OpenGVLab/InternGPT]( | 2911 | |[ParisNeo/lollms-webui]( | 2907 | |[Unstructured-IO/unstructured]( | 2874 | |[openchatai/OpenCopilot]( | 2759 | |[OpenBMB/BMTools]( | 2657 | |[homanp/superagent]( | 2624 | |[SamurAIGPT/EmbedAI]( | 2575 | |[GerevAI/gerev]( | 2488 | |[microsoft/promptflow]( | 2475 | |[OpenBMB/AgentVerse]( | 2445 | |[Mintplex-Labs/anything-llm]( | 2434 | |[emptycrown/llama-hub]( | 2432 | |[NVIDIA/NeMo-Guardrails]( | 2327 | |[ShreyaR/guardrails]( | 2307 | |[thomas-yanxin/LangChain-ChatGLM-Webui]( | 2305 | |[yanqiangmiffy/Chinese-LangChain]( | 2291 | |[keephq/keep]( | 2252 | |[OpenGVLab/Ask-Anything]( | 2194 | |[IntelligenzaArtificiale/Free-Auto-GPT]( | 2169 | |[Farama-Foundation/PettingZoo]( | 2031 | |[YiVal/YiVal]( | 2014 | |[hwchase17/notion-qa]( | 2014 | |[jupyterlab/jupyter-ai]( | 1977 | |[paulpierre/RasaGPT]( | 1887 | |[dot-agent/dotagent-WIP]( | 1812 | |[hegelai/prompttools]( | 1775 | |[vocodedev/vocode-python]( | 1734 | |[Vonng/pigsty]( | 1693 | |[psychic-api/psychic]( | 1597 | |[avinashkranjan/Amazing-Python-Scripts]( | 1546 | |[pinterest/querybook]( | 1539 | |[Forethought-Technologies/AutoChain]( | 1531 | |[Kav-K/GPTDiscord]( | 1503 | |[jina-ai/langchain-serve]( | 1487 | |[noahshinn024/reflexion]( | 1481 | |[jina-ai/dev-gpt]( | 1436 | |[ttengwang/Caption-Anything]( | 1425 | |[milvus-io/bootcamp]( | 1420 | |[agiresearch/OpenAGI]( | 1401 | |[greshake/llm-security]( | 1381 | |[jina-ai/thinkgpt]( | 1366 | |[lunasec-io/lunasec]( | 1352 | |[101dotxyz/GPTeam]( | 1339 | |[refuel-ai/autolabel]( | 1320 | |[melih-unsal/DemoGPT]( | 1320 | |[mmz-001/knowledge_gpt]( | 1320 | |[richardyc/Chrome-GPT]( | 1315 | |[run-llama/sec-insights]( | 1312 | |[Azure/azureml-examples]( | 1305 | |[cofactoryai/textbase]( | 1286 | |[dataelement/bisheng]( | 1273 | |[eyurtsev/kor]( | 1263 | |[pluralsh/plural]( | 1188 | |[FlagOpen/FlagEmbedding]( | 1184 | |[juncongmoo/chatllama]( | 1144 | |[poe-platform/server-bot-quick-start]( | 1139 | |[visual-openllm/visual-openllm]( | 1137 | |[griptape-ai/griptape]( | 1124 | |[microsoft/X-Decoder]( | 1119 | |[ThousandBirdsInc/chidori]( | 1116 | |[filip-michalsky/SalesGPT]( | 1112 | |[psychic-api/rag-stack]( | 1110 | |[irgolic/AutoPR]( | 1100 | |[promptfoo/promptfoo]( | 1099 | |[nod-ai/SHARK]( | 1062 | |[SamurAIGPT/Camel-AutoGPT]( | 1036 | |[Farama-Foundation/chatarena]( | 1020 | |[peterw/Chat-with-Github-Repo]( | 993 | |[jiran214/GPT-vup]( | 967 | |[alejandro-ao/ask-multiple-pdfs]( | 958 | |[run-llama/llama-lab]( | 953 | |[LC1332/Chat-Haruhi-Suzumiya]( | 950 | |[rlancemartin/auto-evaluator]( | 927 | |[cheshire-cat-ai/core]( | 902 | |[Anil-matcha/ChatPDF]( | 894 | |[cirediatpl/FigmaChain]( | 881 | |[seanpixel/Teenage-AGI]( | 876 | |[xusenlinzy/api-for-open-llm]( | 865 | |[ricklamers/shell-ai]( | 864 | |[codeacme17/examor]( | 856 | |[corca-ai/EVAL]( | 836 | |[microsoft/Llama-2-Onnx]( | 835 | |[explodinggradients/ragas]( | 833 | |[ajndkr/lanarky]( | 817 | |[kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference]( | 814 | |[ray-project/llm-applications]( | 804 | |[hwchase17/chat-your-data]( | 801 | |[LambdaLabsML/examples]( | 759 | |[kreneskyp/ix]( | 758 | |[pyspark-ai/pyspark-ai]( | 750 | |[billxbf/ReWOO]( | 746 | |[e-johnstonn/BriefGPT]( | 738 | |[akshata29/entaoai]( | 733 | |[getmetal/motorhead]( | 717 | |[ruoccofabrizio/azure-open-ai-embeddings-qna]( | 712 | |[msoedov/langcorn]( | 698 | |[Dataherald/dataherald]( | 684 | |[jondurbin/airoboros]( | 657 | |[Ikaros-521/AI-Vtuber]( | 651 | |[whyiyhw/chatgpt-wechat]( | 644 | |[langchain-ai/streamlit-agent]( | 637 | |[SamurAIGPT/ChatGPT-Developer-Plugins]( | 637 | |[OpenGenerativeAI/GenossGPT]( | 632 | |[AILab-CVC/GPT4Tools]( | 629 | |[langchain-ai/auto-evaluator]( | 614 | |[explosion/spacy-llm]( | 613 | |[alexanderatallah/window.ai]( | 607 | |[MiuLab/Taiwan-LLaMa]( | 601 | |[microsoft/PodcastCopilot]( | 600 | |[Dicklesworthstone/swiss_army_llama]( | 596 | |[NoDataFound/hackGPT]( | 596 | |[namuan/dr-doc-search]( | 593 | |[amosjyng/langchain-visualizer]( | 582 | |[microsoft/sample-app-aoai-chatGPT]( | 581 | |[yvann-hub/Robby-chatbot]( | 581 | |[yeagerai/yeagerai-agent]( | 547 | |[tgscan-dev/tgscan]( | 533 | |[Azure-Samples/openai]( | 531 | |[plastic-labs/tutor-gpt]( | 531 | |[xuwenhao/geektime-ai-course]( | 526 | |[michaelthwan/searchGPT]( | 526 | |[jonra1993/fastapi-alembic-sqlmodel-async]( | 522 | |[jina-ai/agentchain]( | 519 | |[mckaywrigley/repo-chat]( | 518 | |[modelscope/modelscope-agent]( | 512 | |[daveebbelaar/langchain-experiments]( | 504 | |[freddyaboulton/gradio-tools]( | 497 | |[sidhq/Multi-GPT]( | 494 | |[continuum-llms/chatgpt-memory]( | 489 | |[langchain-ai/langchain-aiplugin]( | 487 | |[mpaepper/content-chatbot]( | 483 | |[steamship-core/steamship-langchain]( | 481 | |[alejandro-ao/langchain-ask-pdf]( | 474 | |[truera/trulens]( | 464 | |[marella/chatdocs]( | 459 | |[opencopilotdev/opencopilot]( | 453 | |[poe-platform/poe-protocol]( | 444 | |[DataDog/dd-trace-py]( | 441 | |[logan-markewich/llama_index_starter_pack]( | 441 | |[opentensor/bittensor]( | 433 | |[DjangoPeng/openai-quickstart]( | 425 | |[CarperAI/OpenELM]( | 424 | |[daodao97/chatdoc]( | 423 | |[showlab/VLog]( | 411 | |[Anil-matcha/Chatbase]( | 402 | |[yakami129/VirtualWife]( | 399 | |[wandb/weave]( | 399 | |[mtenenholtz/chat-twitter]( | 398 | |[LinkSoul-AI/AutoAgents]( | 397 | |[Agenta-AI/agenta]( | 389 | |[huchenxucs/ChatDB]( | 386 | |[mallorbc/Finetune_LLMs]( | 379 | |[junruxiong/IncarnaMind]( | 372 | |[MagnivOrg/prompt-layer-library]( | 368 | |[mosaicml/examples]( | 366 | |[rsaryev/talk-codebase]( | 364 | |[morpheuslord/GPT_Vuln-analyzer]( | 362 | |[monarch-initiative/ontogpt]( | 362 | |[JayZeeDesign/researcher-gpt]( | 361 | |[personoids/personoids-lite]( | 361 | |[intel/intel-extension-for-transformers]( | 357 | |[jerlendds/osintbuddy]( | 357 | |[steamship-packages/langchain-production-starter]( | 356 | |[onlyphantom/llm-python]( | 354 | |[Azure-Samples/miyagi]( | 340 | |[mrwadams/attackgen]( | 338 | |[rgomezcasas/dotfiles]( | 337 | |[eosphoros-ai/DB-GPT-Hub]( | 336 | |[andylokandy/gpt-4-search]( | 335 | |[NimbleBoxAI/ChainFury]( | 330 | |[momegas/megabots]( | 329 | |[Nuggt-dev/Nuggt]( | 315 | |[itamargol/openai]( | 315 | |[BlackHC/llm-strategy]( | 315 | |[aws-samples/aws-genai-llm-chatbot]( | 312 | |[Cheems-Seminar/grounded-segment-any-parts]( | 312 | |[preset-io/promptimize]( | 311 | |[dgarnitz/vectorflow]( | 309 | |[langchain-ai/langsmith-cookbook]( | 309 | |[CambioML/pykoi]( | 309 | |[wandb/edu]( | 301 | |[XzaiCloud/luna-ai]( | 300 | |[liangwq/Chatglm_lora_multi-gpu]( | 294 | |[Haste171/langchain-chatbot]( | 291 | |[sullivan-sean/chat-langchainjs]( | 286 | |[sugarforever/LangChain-Tutorials]( | 285 | |[facebookresearch/personal-timeline]( | 283 | |[hnawaz007/pythondataanalysis]( | 282 | |[yuanjie-ai/ChatLLM]( | 280 | |[MetaGLM/FinGLM]( | 279 | |[JohnSnowLabs/langtest]( | 277 | |[Em1tSan/NeuroGPT]( | 274 | |[Safiullah-Rahu/CSV-AI]( | 274 | |[conceptofmind/toolformer]( | 274 | |[airobotlab/KoChatGPT]( | 266 | |[gia-guar/JARVIS-ChatGPT]( | 263 | |[Mintplex-Labs/vector-admin]( | 262 | |[artitw/text2text]( | 262 | |[kaarthik108/snowChat]( | 261 | |[paolorechia/learn-langchain]( | 260 | |[shamspias/customizable-gpt-chatbot]( | 260 | |[ur-whitelab/exmol]( | 258 | |[hwchase17/chroma-langchain]( | 257 | |[bborn/howdoi.ai]( | 255 | |[ur-whitelab/chemcrow-public]( | 253 | |[pablomarin/GPT-Azure-Search-Engine]( | 251 | |[gustavz/DataChad]( | 249 | |[radi-cho/datasetGPT]( | 249 | |[ennucore/clippinator]( | 247 | |[recalign/RecAlign]( | 244 | |[lilacai/lilac]( | 243 | |[kaleido-lab/dolphin]( | 236 | |[iusztinpaul/hands-on-llms]( | 233 | |[PradipNichite/Youtube-Tutorials]( | 231 | |[shaman-ai/agent-actors]( | 231 | |[hwchase17/langchain-streamlit-template]( | 231 | |[yym68686/ChatGPT-Telegram-Bot]( | 226 | |[grumpyp/aixplora]( | 222 | |[su77ungr/CASALIOY]( | 222 | |[alvarosevilla95/autolang]( | 222 | |[arthur-ai/bench]( | 220 | |[miaoshouai/miaoshouai-assistant]( | 219 | |[AutoPackAI/beebot]( | 217 | |[edreisMD/plugnplai]( | 216 | |[nicknochnack/LangchainDocuments]( | 214 | |[AkshitIreddy/Interactive-LLM-Powered-NPCs]( | 213 | |[SpecterOps/Nemesis]( | 210 | |[kyegomez/swarms]( | 210 | |[wpydcr/LLM-Kit]( | 208 | |[orgexyz/BlockAGI]( | 204 | |[Chainlit/cookbook]( | 202 | |[WongSaang/chatgpt-ui-server]( | 202 | |[jbrukh/gpt-jargon]( | 202 | |[handrew/browserpilot]( | 202 | |[langchain-ai/web-explorer]( | 200 | |[plchld/InsightFlow]( | 200 | |[alphasecio/langchain-examples]( | 199 | |[Gentopia-AI/Gentopia]( | 198 | |[SamPink/dev-gpt]( | 196 | |[yasyf/compress-gpt]( | 196 | |[benthecoder/ClassGPT]( | 195 | |[voxel51/voxelgpt]( | 193 | |[CL-lau/SQL-GPT]( | 192 | |[blob42/Instrukt]( | 191 | |[streamlit/llm-examples]( | 191 | |[stepanogil/autonomous-hr-chatbot]( | 190 | |[TsinghuaDatabaseGroup/DB-GPT]( | 189 | |[PJLab-ADG/DriveLikeAHuman]( | 187 | |[Azure-Samples/azure-search-power-skills]( | 187 | |[microsoft/azure-openai-in-a-day-workshop]( | 187 | |[ju-bezdek/langchain-decorators]( | 182 | |[hardbyte/qabot]( | 181 | |[hongbo-miao/hongbomiao.com]( | 180 | |[QwenLM/Qwen-Agent]( | 179 | |[showlab/UniVTG]( | 179 | |[Azure-Samples/jp-azureopenai-samples]( | 176 | |[afaqueumer/DocQA]( | 174 | |[ethanyanjiali/minChatGPT]( | 174 | |[shauryr/S2QA]( | 174 | |[RoboCoachTechnologies/GPT-Synthesizer]( | 173 | |[chakkaradeep/pyCodeAGI]( | 172 | |[vaibkumr/prompt-optimizer]( | 171 | |[ccurme/yolopandas]( | 170 | |[anarchy-ai/LLM-VM]( | 169 | |[ray-project/langchain-ray]( | 169 | |[fengyuli-dev/multimedia-gpt]( | 169 | |[ibiscp/LLM-IMDB]( | 168 | |[mayooear/private-chatbot-mpt30b-langchain]( | 167 | |[OpenPluginACI/openplugin]( | 165 | |[jmpaz/promptlib]( | 165 | |[kjappelbaum/gptchem]( | 162 | |[JorisdeJong123/7-Days-of-LangChain]( | 161 | |[retr0reg/Ret2GPT]( | 161 | |[menloparklab/falcon-langchain]( | 159 | |[summarizepaper/summarizepaper]( | 158 | |[emarco177/ice_breaker]( | 157 | |[AmineDiro/cria]( | 156 | |[morpheuslord/HackBot]( | 156 | |[homanp/vercel-langchain]( | 156 | |[mlops-for-all/mlops-for-all.github.io]( | 155 | |[positive666/Prompt-Can-Anything]( | 154 | |[deeppavlov/dream]( | 153 | |[flurb18/AgentOoba]( | 151 | |[Open-Swarm-Net/GPT-Swarm]( | 151 | |[v7labs/benchllm]( | 150 | |[Klingefjord/chatgpt-telegram]( | 150 | |[Aggregate-Intellect/sherpa]( | 148 | |[Coding-Crashkurse/Langchain-Full-Course]( | 148 | |[SuperDuperDB/superduperdb]( | 147 | |[defenseunicorns/leapfrogai]( | 147 | |[menloparklab/langchain-cohere-qdrant-doc-retrieval]( | 147 | |[Jaseci-Labs/jaseci]( | 146 | |[realminchoi/babyagi-ui]( | 146 | |[iMagist486/ElasticSearch-Langchain-Chatglm2]( | 144 | |[peterw/StoryStorm]( | 143 | |[kulltc/chatgpt-sql]( | 142 | |[Teahouse-Studios/akari-bot]( | 142 | |[hirokidaichi/wanna]( | 141 | |[yasyf/summ]( | 141 | |[solana-labs/chatgpt-plugin]( | 140 | |[ssheng/BentoChain]( | 139 | |[mallahyari/drqa]( | 139 | |[petehunt/langchain-github-bot]( | 139 | |[dbpunk-labs/octogen]( | 138 | |[RedisVentures/redis-openai-qna]( | 138 | |[eunomia-bpf/GPTtrace]( | 138 | |[langchain-ai/langsmith-sdk]( | 137 | |[jina-ai/fastapi-serve]( | 137 | |[yeagerai/genworlds]( | 137 | |[aurelio-labs/arxiv-bot]( | 137 | |[luisroque/large_laguage_models]( | 136 | |[ChuloAI/BrainChulo]( | 136 | |[3Alan/DocsMind]( | 136 | |[KylinC/ChatFinance]( | 133 | |[langchain-ai/text-split-explorer]( | 133 | |[davila7/file-gpt]( | 133 | |[tencentmusic/supersonic]( | 132 | |[kimtth/azure-openai-llm-vector-langchain]( | 131 | |[ciare-robotics/world-creator]( | 129 | |[zenml-io/zenml-projects]( | 129 | |[log1stics/voice-generator-webui]( | 129 | |[snexus/llm-search]( | 129 | |[fixie-ai/fixie-examples]( | 128 | |[MedalCollector/Orator]( | 127 | |[grumpyp/chroma-langchain-tutorial]( | 127 | |[langchain-ai/langchain-aws-template]( | 127 | |[prof-frink-lab/slangchain]( | 126 | |[KMnO4-zx/huanhuan-chat]( | 124 | |[RCGAI/SimplyRetrieve]( | 124 | |[Dicklesworthstone/llama2_aided_tesseract]( | 123 | |[sdaaron/QueryGPT]( | 122 | |[athina-ai/athina-sdk]( | 121 | |[AIAnytime/Llama2-Medical-Chatbot]( | 121 | |[MuhammadMoinFaisal/LargeLanguageModelsProjects]( | 121 | |[Azure/business-process-automation]( | 121 | |[definitive-io/code-indexer-loop]( | 119 | |[nrl-ai/pautobot]( | 119 | |[Azure/app-service-linux-docs]( | 118 | |[zilliztech/akcio]( | 118 | |[CodeAlchemyAI/ViLT-GPT]( | 117 | |[georgesung/llm_qlora]( | 117 | |[nicknochnack/Nopenai]( | 115 | |[nftblackmagic/flask-langchain]( | 115 | |[mortium91/langchain-assistant]( | 115 | |[Ngonie-x/langchain_csv]( | 114 | |[wombyz/HormoziGPT]( | 114 | |[langchain-ai/langchain-teacher]( | 113 | |[mluogh/eastworld]( | 112 | |[mudler/LocalAGI]( | 112 | |[marimo-team/marimo]( | 111 | |[trancethehuman/entities-extraction-web-scraper]( | 111 | |[xuwenhao/mactalk-ai-course]( | 111 | |[dcaribou/transfermarkt-datasets]( | 111 | |[rabbitmetrics/langchain-13-min]( | 111 | |[dotvignesh/PDFChat]( | 111 | |[aws-samples/cdk-eks-blueprints-patterns]( | 110 | |[topoteretes/PromethAI-Backend]( | 110 | |[jlonge4/local_llama]( | 110 | |[RUC-GSAI/YuLan-Rec]( | 108 | |[gh18l/CrawlGPT]( | 107 | |[c0sogi/LLMChat]( | 107 | |[hwchase17/langchain-gradio-template]( | 107 | |[ArjanCodes/examples]( | 106 | |[genia-dev/GeniA]( | 105 | |[nexus-stc/stc]( | 105 | |[mbchang/data-driven-characters]( | 105 | |[ademakdogan/ChatSQL]( | 104 | |[crosleythomas/MirrorGPT]( | 104 | |[IvanIsCoding/ResuLLMe]( | 104 | |[avrabyt/MemoryBot]( | 104 | |[Azure/azure-sdk-tools]( | 103 | |[aniketmaurya/llm-inference]( | 103 | |[Anil-matcha/Youtube-to-chatbot]( | 103 | |[nyanp/chat2plot]( | 102 | |[aws-samples/amazon-kendra-langchain-extensions]( | 101 | |[atisharma/llama_farm]( | 100 | |[Xueheng-Li/SynologyChatbotGPT]( | 100 | _Generated by [github-dependents-info]( `github-dependents-info --repo langchain-ai/langchain --markdownfile dependents.md --minstars 100 --sort stars`"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/additional_resources/tutorials.mdx"}, "data": "# Tutorials Below are links to tutorials and courses on LangChain. For written guides on common use cases for LangChain, check out the [use cases guides](/docs/use_cases). icon marks a new addition [last update 2023-09-21] --------------------- ### [LangChain on Wikipedia]( ### DeepLearning.AI courses by [Harrison Chase]( and [Andrew Ng]( - [LangChain for LLM Application Development]( - [LangChain Chat with Your Data]( - [Functions, Tools and Agents with LangChain]( ### Handbook [LangChain AI Handbook]( By **James Briggs** and **Francisco Ingham** ### Short Tutorials [LangChain Explained in 13 Minutes | QuickStart Tutorial for Beginners]( by [Rabbitmetrics]( [LangChain Crash Course: Build an AutoGPT app in 25 minutes]( by [Nicholas Renotte]( [LangChain Crash Course - Build apps with language models]( by [Patrick Loeber]( ## Tutorials ### [LangChain for Gen AI and LLMs]( by [James Briggs]( - #1 [Getting Started with `GPT-3` vs. Open Source LLMs]( - #2 [Prompt Templates for `GPT 3.5` and other LLMs]( - #3 [LLM Chains using `GPT 3.5` and other LLMs]( - [LangChain Data Loaders, Tokenizers, Chunking, and Datasets - Data Prep 101]( - #4 [Chatbot Memory for `Chat-GPT`, `Davinci` + other LLMs]( - #5 [Chat with OpenAI in LangChain]( - #6 [Fixing LLM Hallucinations with Retrieval Augmentation in LangChain]( - #7 [LangChain Agents Deep Dive with `GPT 3.5`]( - #8 [Create Custom Tools for Chatbots in LangChain]( - #9 [Build Conversational Agents with Vector DBs]( - [Using NEW `MPT-7B` in Hugging Face and LangChain]( - [`MPT-30B` Chatbot with LangChain]( - [Fine-tuning OpenAI's `GPT 3.5` for LangChain Agents]( - [Chatbots with `RAG`: LangChain Full Walkthrough]( ### [LangChain 101]( by [Greg Kamradt (Data Indy)]( - [What Is LangChain? - LangChain + `ChatGPT` Overview]( - [Quickstart Guide]( - [Beginner's Guide To 7 Essential Concepts]( - [Beginner's Guide To 9 Use Cases]( - [Agents Overview + Google Searches]( - [`OpenAI` + `Wolfram Alpha`]( - [Ask Questions On Your Custom (or Private) Files]( - [Connect `Google Drive Files` To `OpenAI`]( - [`YouTube Transcripts` + `OpenAI`]( - [Question A 300 Page Book (w/ `OpenAI` + `Pinecone`)]( - [Workaround `OpenAI's` Token Limit With Chain Types]( - [Build Your Own OpenAI + LangChain Web App in 23 Minutes]( - [Working With The New `ChatGPT API`]( - [OpenAI + LangChain Wrote Me 100 Custom Sales Emails]( - [Structured Output From `OpenAI` (Clean Dirty Data)]( - [Connect `OpenAI` To +5,000 Tools (LangChain + `Zapier`)]( - [Use LLMs To Extract Data From Text (Expert Mode)]( - [Extract Insights From Interview Transcripts Using LLMs]( - [5 Levels Of LLM Summarizing: Novice to Expert]( - [Control Tone & Writing Style Of Your LLM Output]( - [Build Your Own `AI Twitter Bot` Using LLMs]( - [ChatGPT made my interview questions for me (`Streamlit` + LangChain)]( - [Function Calling via ChatGPT API - First Look With LangChain]( - [Extract Topics From Video/Audio With LLMs (Topic Modeling w/ LangChain)]( ### [LangChain How to and guides]( by [Sam Witteveen]( - [LangChain Basics - LLMs & PromptTemplates with Colab]( - [LangChain Basics - Tools and Chains]( - [`ChatGPT API` Announcement & Code Walkthrough with LangChain]( - [Conversations with Memory (explanation & code walkthrough)]( - [Chat with `Flan20B`]( - [Using `Hugging Face Models` locally (code walkthrough)]( - [`PAL`: Program-aided Language Models with LangChain code]( - [Building a Summarization System with LangChain and `GPT-3` - Part 1]( - [Building a Summarization System with LangChain and `GPT-3` - Part 2]( - [Microsoft's `Visual ChatGPT` using LangChain]( - [LangChain Agents - Joining Tools and Chains with Decisions]( - [Comparing LLMs with LangChain]( - [Using `Constitutional AI` in LangChain]( - [Talking to `Alpaca` with LangChain - Creating an Alpaca Chatbot]( - [Talk to your `CSV` & `Excel` with LangChain]( - [`BabyAGI`: Discover the Power of Task-Driven Autonomous Agents!]( - [Improve your `BabyAGI` with LangChain]( - [Master `PDF` Chat with LangChain - Your essential guide to queries on documents]( - [Using LangChain with `DuckDuckGO`, `Wikipedia` & `PythonREPL` Tools]( - [Building Custom Tools and Agents with LangChain (gpt-3.5-turbo)]( - [LangChain Retrieval QA Over Multiple Files with `ChromaDB`]( - [LangChain Retrieval QA with Instructor Embeddings & `ChromaDB` for PDFs]( - [LangChain + Retrieval Local LLMs for Retrieval QA - No OpenAI!!!]( - [`Camel` + LangChain for Synthetic Data & Market Research]( - [Information Extraction with LangChain & `Kor`]( - [Converting a LangChain App from OpenAI to OpenSource]( - [Using LangChain `Output Parsers` to get what you want out of LLMs]( - [Building a LangChain Custom Medical Agent with Memory]( - [Understanding `ReACT` with LangChain]( - [`OpenAI Functions` + LangChain : Building a Multi Tool Agent]( - [What can you do with 16K tokens in LangChain?]( - [Tagging and Extraction - Classification using `OpenAI Functions`]( - [HOW to Make Conversational Form with LangChain]( - [`Claude-2` meets LangChain!]( - [`PaLM 2` Meets LangChain]( - [`LLaMA2` with LangChain - Basics | LangChain TUTORIAL]( - [Serving `LLaMA2` with `Replicate`]( - [NEW LangChain Expression Language]( - [Building a RCI Chain for Agents with LangChain Expression Language]( - [How to Run `LLaMA-2-70B` on the `Together AI`]( - [`RetrievalQA` with `LLaMA 2 70b` & `Chroma` DB]( - [How to use `BGE Embeddings` for LangChain]( - [How to use Custom Prompts for `RetrievalQA` on `LLaMA-2 7B`]( ### [LangChain]( by [Prompt Engineering]( - [LangChain Crash Course \u2014 All You Need to Know to Build Powerful Apps with LLMs]( - [Working with MULTIPLE `PDF` Files in LangChain: `ChatGPT` for your Data]( - [`ChatGPT` for YOUR OWN `PDF` files with LangChain]( - [Talk to YOUR DATA without OpenAI APIs: LangChain]( - [LangChain: `PDF` Chat App (GUI) | `ChatGPT` for Your `PDF` FILES]( - [`LangFlow`: Build Chatbots without Writing Code]( - [LangChain: Giving Memory to LLMs]( - [BEST OPEN Alternative to `OPENAI's EMBEDDINGs` for Retrieval QA: LangChain]( - [LangChain: Run Language Models Locally - `Hugging Face Models`]( - [Slash API Costs: Mastering Caching for LLM Applications]( - [Avoid PROMPT INJECTION with `Constitutional AI` - LangChain]( ### LangChain by [Chat with data]( - [LangChain Beginner's Tutorial for `Typescript`/`Javascript`]( - [`GPT-4` Tutorial: How to Chat With Multiple `PDF` Files (~1000 pages of Tesla's 10-K Annual Reports)]( - [`GPT-4` & LangChain Tutorial: How to Chat With A 56-Page `PDF` Document (w/`Pinecone`)]( - [LangChain & `Supabase` Tutorial: How to Build a ChatGPT Chatbot For Your Website]( - [LangChain Agents: Build Personal Assistants For Your Data (Q&A with Harrison Chase and Mayo Oshin)]( ### Codebase Analysis - [Codebase Analysis: Langchain Agents]( --------------------- icon marks a new addition [last update 2023-09-21]"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/additional_resources/youtube.mdx"}, "data": "# YouTube videos icon marks a new addition [last update 2023-09-21] ### [Official LangChain YouTube channel]( ### Introduction to LangChain with Harrison Chase, creator of LangChain - [Building the Future with LLMs, `LangChain`, & `Pinecone`]( by [Pinecone]( - [LangChain and Weaviate with Harrison Chase and Bob van Luijt - Weaviate Podcast #36]( by [Weaviate \u2022 Vector Database]( - [LangChain Demo + Q&A with Harrison Chase]( by [Full Stack Deep Learning]( - [LangChain Agents: Build Personal Assistants For Your Data (Q&A with Harrison Chase and Mayo Oshin)]( by [Chat with data]( ## Videos (sorted by views) - [Using `ChatGPT` with YOUR OWN Data. This is magical. (LangChain OpenAI API)]( by [TechLead]( - [First look - `ChatGPT` + `WolframAlpha` (`GPT-3.5` and Wolfram|Alpha via LangChain by James Weaver)]( by [Dr Alan D. Thompson]( - [LangChain explained - The hottest new Python framework]( by [AssemblyAI]( - [Chatbot with INFINITE MEMORY using `OpenAI` & `Pinecone` - `GPT-3`, `Embeddings`, `ADA`, `Vector DB`, `Semantic`]( by [David Shapiro ~ AI]( - [LangChain for LLMs is... basically just an Ansible playbook]( by [David Shapiro ~ AI]( - [Build your own LLM Apps with LangChain & `GPT-Index`]( by [1littlecoder]( - [`BabyAGI` - New System of Autonomous AI Agents with LangChain]( by [1littlecoder]( - [Run `BabyAGI` with Langchain Agents (with Python Code)]( by [1littlecoder]( - [How to Use Langchain With `Zapier` | Write and Send Email with GPT-3 | OpenAI API Tutorial]( by [StarMorph AI]( - [Use Your Locally Stored Files To Get Response From GPT - `OpenAI` | Langchain | Python]( by [Shweta Lodha]( - [`Langchain JS` | How to Use GPT-3, GPT-4 to Reference your own Data | `OpenAI Embeddings` Intro]( by [StarMorph AI]( - [The easiest way to work with large language models | Learn LangChain in 10min]( by [Sophia Yang]( - [4 Autonomous AI Agents: \u201cWestworld\u201d simulation `BabyAGI`, `AutoGPT`, `Camel`, `LangChain`]( by [Sophia Yang]( - [AI CAN SEARCH THE INTERNET? Langchain Agents + OpenAI ChatGPT]( by [tylerwhatsgood]( - [Query Your Data with GPT-4 | Embeddings, Vector Databases | Langchain JS Knowledgebase]( by [StarMorph AI]( - [`Weaviate` + LangChain for LLM apps presented by Erika Cardenas]( by [`Weaviate` \u2022 Vector Database]( - [Langchain Overview \u2014 How to Use Langchain & `ChatGPT`]( by [Python In Office]( - [Langchain Overview - How to Use Langchain & `ChatGPT`]( by [Python In Office]( - [LangChain Tutorials]( by [Edrick]( - [LangChain, Chroma DB, OpenAI Beginner Guide | ChatGPT with your PDF]( - [LangChain 101: The Complete Beginner's Guide]( - [Custom langchain Agent & Tools with memory. Turn any `Python function` into langchain tool with Gpt 3]( by [echohive]( - [Building AI LLM Apps with LangChain (and more?) - LIVE STREAM]( by [Nicholas Renotte]( - [`ChatGPT` with any `YouTube` video using langchain and `chromadb`]( by [echohive]( - [How to Talk to a `PDF` using LangChain and `ChatGPT`]( by [Automata Learning Lab]( - [Langchain Document Loaders Part 1: Unstructured Files]( by [Merk]( - [LangChain - Prompt Templates (what all the best prompt engineers use)]( by [Nick Daigler]( - [LangChain. Crear aplicaciones Python impulsadas por GPT]( by [Jes\u00fas Conde]( - [Easiest Way to Use GPT In Your Products | LangChain Basics Tutorial]( by [Rachel Woods]( - [`BabyAGI` + `GPT-4` Langchain Agent with Internet Access]( by [tylerwhatsgood]( - [Learning LLM Agents. How does it actually work? LangChain, AutoGPT & OpenAI]( by [Arnoldas Kemeklis]( - [Get Started with LangChain in `Node.js`]( by [Developers Digest]( - [LangChain + `OpenAI` tutorial: Building a Q&A system w/ own text data]( by [Samuel Chan]( - [Langchain + `Zapier` Agent]( by [Merk]( - [Connecting the Internet with `ChatGPT` (LLMs) using Langchain And Answers Your Questions]( by [Kamalraj M M]( - [Build More Powerful LLM Applications for Business\u2019s with LangChain (Beginners Guide)]( by[ No Code Blackbox]( - [LangFlow LLM Agent Demo for LangChain]( by [Cobus Greyling]( - [Chatbot Factory: Streamline Python Chatbot Creation with LLMs and Langchain]( by [Finxter]( - [LangChain Tutorial - ChatGPT mit eigenen Daten]( by [Coding Crashkurse]( - [Chat with a `CSV` | LangChain Agents Tutorial (Beginners)]( by [GoDataProf]( - [Introdu\u00e7\u00e3o ao Langchain - #Cortes - Live DataHackers]( by [Prof. Jo\u00e3o Gabriel Lima]( - [LangChain: Level up `ChatGPT` !? | LangChain Tutorial Part 1]( by [Code Affinity]( - [KI schreibt krasses Youtube Skript | LangChain Tutorial Deutsch]( by [SimpleKI]( - [Chat with Audio: Langchain, `Chroma DB`, OpenAI, and `Assembly AI`]( by [AI Anytime]( - [QA over documents with Auto vector index selection with Langchain router chains]( by [echohive]( - [Build your own custom LLM application with `Bubble.io` & Langchain (No Code & Beginner friendly)]( by [No Code Blackbox]( - [Simple App to Question Your Docs: Leveraging `Streamlit`, `Hugging Face Spaces`, LangChain, and `Claude`!]( by [Chris Alexiuk]( - [LANGCHAIN AI- `ConstitutionalChainAI` + Databutton AI ASSISTANT Web App]( by [Avra]( - [LANGCHAIN AI AUTONOMOUS AGENT WEB APP - `BABY AGI` with EMAIL AUTOMATION using `DATABUTTON`]( by [Avra]( - [The Future of Data Analysis: Using A.I. Models in Data Analysis (LangChain)]( by [Absent Data]( - [Memory in LangChain | Deep dive (python)]( by [Eden Marco]( - [9 LangChain UseCases | Beginner's Guide | 2023]( by [Data Science Basics]( - [Use Large Language Models in Jupyter Notebook | LangChain | Agents & Indexes]( by [Abhinaw Tiwari]( - [How to Talk to Your Langchain Agent | `11 Labs` + `Whisper`]( by [VRSEN]( - [LangChain Deep Dive: 5 FUN AI App Ideas To Build Quickly and Easily]( by [James NoCode]( - [LangChain 101: Models]( by [Mckay Wrigley]( - [LangChain with JavaScript Tutorial #1 | Setup & Using LLMs]( by [Leon van Zyl]( - [LangChain Overview & Tutorial for Beginners: Build Powerful AI Apps Quickly & Easily (ZERO CODE)]( by [James NoCode]( - [LangChain In Action: Real-World Use Case With Step-by-Step Tutorial]( by [Rabbitmetrics]( - [Summarizing and Querying Multiple Papers with LangChain]( by [Automata Learning Lab]( - [Using Langchain (and `Replit`) through `Tana`, ask `Google`/`Wikipedia`/`Wolfram Alpha` to fill out a table]( by [Stian H\u00e5klev]( - [Langchain PDF App (GUI) | Create a ChatGPT For Your `PDF` in Python]( by [Alejandro AO - Software & Ai]( - [Auto-GPT with LangChain | Create Your Own Personal AI Assistant]( by [Data Science Basics]( - [Create Your OWN Slack AI Assistant with Python & LangChain]( by [Dave Ebbelaar]( - [How to Create LOCAL Chatbots with GPT4All and LangChain [Full Guide]]( by [Liam Ottley]( - [Build a `Multilingual PDF` Search App with LangChain, `Cohere` and `Bubble`]( by [Menlo Park Lab]( - [Building a LangChain Agent (code-free!) Using `Bubble` and `Flowise`]( by [Menlo Park Lab]( - [Build a LangChain-based Semantic PDF Search App with No-Code Tools Bubble and Flowise]( by [Menlo Park Lab]( - [LangChain Memory Tutorial | Building a ChatGPT Clone in Python]( by [Alejandro AO - Software & Ai]( - [ChatGPT For Your DATA | Chat with Multiple Documents Using LangChain]( by [Data Science Basics]( - [`Llama Index`: Chat with Documentation using URL Loader]( by [Merk]( - [Using OpenAI, LangChain, and `Gradio` to Build Custom GenAI Applications]( by [David Hundley]( - [LangChain, Chroma DB, OpenAI Beginner Guide | ChatGPT with your PDF]( - [Build AI chatbot with custom knowledge base using OpenAI API and GPT Index]( by [Irina Nik]( - [Build Your Own Auto-GPT Apps with LangChain (Python Tutorial)]( by [Dave Ebbelaar]( - [Chat with Multiple `PDFs` | LangChain App Tutorial in Python (Free LLMs and Embeddings)]( by [Alejandro AO - Software & Ai]( - [Chat with a `CSV` | `LangChain Agents` Tutorial (Beginners)]( by [Alejandro AO - Software & Ai]( - [Create Your Own ChatGPT with `PDF` Data in 5 Minutes (LangChain Tutorial)]( by [Liam Ottley]( - [Build a Custom Chatbot with OpenAI: `GPT-Index` & LangChain | Step-by-Step Tutorial]( by [Fabrikod]( - [`Flowise` is an open-source no-code UI visual tool to build LangChain applications]( by [Cobus Greyling]( - [LangChain & GPT 4 For Data Analysis: The `Pandas` Dataframe Agent]( by [Rabbitmetrics]( - [`GirlfriendGPT` - AI girlfriend with LangChain]( by [Toolfinder AI]( - [How to build with Langchain 10x easier | LangFlow & `Flowise`]( by [AI Jason]( - [Getting Started With LangChain In 20 Minutes- Build Celebrity Search Application]( by [Krish Naik]( - [Vector Embeddings Tutorial \u2013 Code Your Own AI Assistant with `GPT-4 API` + LangChain + NLP]( by [FreeCodeCamp.org]( - [Fully LOCAL `Llama 2` Q&A with LangChain]( by [1littlecoder]( - [Fully LOCAL `Llama 2` Langchain on CPU]( by [1littlecoder]( - [Build LangChain Audio Apps with Python in 5 Minutes]( by [AssemblyAI]( - [`Voiceflow` & `Flowise`: Want to Beat Competition? New Tutorial with Real AI Chatbot]( by [AI SIMP]( - [THIS Is How You Build Production-Ready AI Apps (`LangSmith` Tutorial)]( by [Dave Ebbelaar]( - [Build POWERFUL LLM Bots EASILY with Your Own Data - `Embedchain` - Langchain 2.0? (Tutorial)]( by [WorldofAI]( - [`Code Llama` powered Gradio App for Coding: Runs on CPU]( by [AI Anytime]( - [LangChain Complete Course in One Video | Develop LangChain (AI) Based Solutions for Your Business]( by [UBprogrammer]( - [How to Run `LLaMA` Locally on CPU or GPU | Python & Langchain & CTransformers Guide]( by [Code With Prince]( - [PyData Heidelberg #11 - TimeSeries Forecasting & LLM Langchain]( by [PyData]( - [Prompt Engineering in Web Development | Using LangChain and Templates with OpenAI]( by [Akamai Developer ]( - [Retrieval-Augmented Generation (RAG) using LangChain and `Pinecone` - The RAG Special Episode]( by [Generative AI and Data Science On AWS]( - [`LLAMA2 70b-chat` Multiple Documents Chatbot with Langchain & Streamlit |All OPEN SOURCE|Replicate API]( by [DataInsightEdge]( - [Chatting with 44K Fashion Products: LangChain Opportunities and Pitfalls]( by [Rabbitmetrics]( - [Structured Data Extraction from `ChatGPT` with LangChain]( by [MG]( - [Chat with Multiple PDFs using `Llama 2`, `Pinecone` and LangChain (Free LLMs and Embeddings)]( by [Muhammad Moin]( - [Integrate Audio into `LangChain.js` apps in 5 Minutes]( by [AssemblyAI]( - [`ChatGPT` for your data with Local LLM]( by [Jacob Jedryszek]( - [Training `Chatgpt` with your personal data using langchain step by step in detail]( by [NextGen Machines]( - [Use ANY language in `LangSmith` with REST]( by [Nerding I/O]( - [How to Leverage the Full Potential of LLMs for Your Business with Langchain - Leon Ruddat]( by [PyData]( - [`ChatCSV` App: Chat with CSV files using LangChain and `Llama 2`]( by [Muhammad Moin]( ### [Prompt Engineering and LangChain]( by [Venelin Valkov]( - [Getting Started with LangChain: Load Custom Data, Run OpenAI Models, Embeddings and `ChatGPT`]( - [Loaders, Indexes & Vectorstores in LangChain: Question Answering on `PDF` files with `ChatGPT`]( - [LangChain Models: `ChatGPT`, `Flan Alpaca`, `OpenAI Embeddings`, Prompt Templates & Streaming]( - [LangChain Chains: Use `ChatGPT` to Build Conversational Agents, Summaries and Q&A on Text With LLMs]( - [Analyze Custom CSV Data with `GPT-4` using Langchain]( - [Build ChatGPT Chatbots with LangChain Memory: Understanding and Implementing Memory in Conversations]( --------------------- icon marks a new addition [last update 2023-09-21]"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/community.md"}, "data": "# Community navigator Hi! Thanks for being here. We\u2019re lucky to have a community of so many passionate developers building with LangChain\u2013we have so much to teach and learn from each other. Community members contribute code, host meetups, write blog posts, amplify each other\u2019s work, become each other's customers and collaborators, and so much more. Whether you\u2019re new to LangChain, looking to go deeper, or just want to get more exposure to the world of building with LLMs, this page can point you in the right direction. - ** Contribute to LangChain** - **:globe_showing_Europe-Africa: Meetups, Events, and Hackathons** - ** Help Us Amplify Your Work** - ** Stay in the loop** # Contribute to LangChain LangChain is the product of over 5,000+ contributions by 1,500+ contributors, and there is ******still****** so much to do together. Here are some ways to get involved: - **[Open a pull request]( We\u2019d appreciate all forms of contributions\u2013new features, infrastructure improvements, better documentation, bug fixes, etc. If you have an improvement or an idea, we\u2019d love to work on it with you. - **[Read our contributor guidelines:]( We ask contributors to follow a [\"fork and pull request\"]( workflow, run a few local checks for formatting, linting, and testing before submitting, and follow certain documentation and testing conventions. - **First time contributor?** [Try one of these PRs with the \u201cgood first issue\u201d tag]( - **Become an expert:** Our experts help the community by answering product questions in Discord. If that\u2019s a role you\u2019d like to play, we\u2019d be so grateful! (And we have some special experts-only goodies/perks we can tell you more about). Send us an email to introduce yourself at hello@langchain.dev and we\u2019ll take it from there! - **Integrate with LangChain:** If your product integrates with LangChain\u2013or aspires to\u2013we want to help make sure the experience is as smooth as possible for you and end users. Send us an email at hello@langchain.dev and tell us what you\u2019re working on. - **Become an Integration Maintainer:** Partner with our team to ensure your integration stays up-to-date and talk directly with users (and answer their inquiries) in our Discord. Introduce yourself at hello@langchain.dev if you\u2019d like to explore this role. # :globe_showing_Europe-Africa: Meetups, Events, and Hackathons One of our favorite things about working in AI is how much enthusiasm there is for building together. We want to help make that as easy and impactful for you as possible! - **Find a meetup, hackathon, or webinar:** You can find the one for you on our [global events calendar]( - **Submit an event to our calendar:** Email us at events@langchain.dev with a link to your event page! We can also help you spread the word with our local communities. - **Host a meetup:** If you want to bring a group of builders together, we want to help! We can publicize your event on our event calendar/Twitter, share it with our local communities in Discord, send swag, or potentially hook you up with a sponsor. Email us at events@langchain.dev to tell us about your event! - **Become a meetup sponsor:** We often hear from groups of builders that want to get together, but are blocked or limited on some dimension (space to host, budget for snacks, prizes to distribute, etc.). If you\u2019d like to help, send us an email to events@langchain.dev we can share more about how it works! - **Speak at an event:** Meetup hosts are always looking for great speakers, presenters, and panelists. If you\u2019d like to do that at an event, send us an email to hello@langchain.dev with more information about yourself, what you want to talk about, and what city you\u2019re based in and we\u2019ll try to match you with an upcoming event! - **Tell us about your LLM community:** If you host or participate in a community that would welcome support from LangChain and/or our team, send us an email at hello@langchain.dev and let us know how we can help. # Help Us Amplify Your Work If you\u2019re working on something you\u2019re proud of, and think the LangChain community would benefit from knowing about it, we want to help you show it off. - **Post about your work and mention us:** We love hanging out on Twitter to see what people in the space are talking about and working on. If you tag [@langchainai]( we\u2019ll almost certainly see it and can show you some love. - **Publish something on our blog:** If you\u2019re writing about your experience building with LangChain, we\u2019d love to post (or crosspost) it on our blog! E-mail hello@langchain.dev with a draft of your post! Or even an idea for something you want to write about. - **Get your product onto our [integrations hub]( Many developers take advantage of our seamless integrations with other products, and come to our integrations hub to find out who those are. If you want to get your product up there, tell us about it (and how it works with LangChain) at hello@langchain.dev. # Stay in the loop Here\u2019s where our team hangs out, talks shop, spotlights cool work, and shares what we\u2019re up to. We\u2019d love to see you there too. - **[Twitter]( We post about what we\u2019re working on and what cool things we\u2019re seeing in the space. If you tag @langchainai in your post, we\u2019ll almost certainly see it, and can show you some love! - **[Discord]( connect with over 30,000 developers who are building with LangChain. - **[GitHub]( Open pull requests, contribute to a discussion, and/or contribute - **[Subscribe to our bi-weekly Release Notes]( a twice/month email roundup of the coolest things going on in our orbit"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/expression_language/cookbook/index.mdx"}, "data": "--- sidebar_position: 2 --- # Cookbook import DocCardList from \"@theme/DocCardList\"; Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the [Prompt + LLM](/docs/expression_language/cookbook/prompt_llm_parser) page is a good place to start."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/expression_language/how_to/index.mdx"}, "data": "--- sidebar_position: 1 --- # How to import DocCardList from \"@theme/DocCardList\";"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/expression_language/index.mdx"}, "data": "--- sidebar_class_name: hidden --- # LangChain Expression Language (LCEL) LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to **support putting prototypes in production, with no code changes**, from the simplest \u201cprompt + LLM\u201d chain to the most complex chains (we\u2019ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL: **Streaming support** When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens. **Async support** Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a [LangServe](/docs/langsmith) server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server. **Optimized parallel execution** Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency. **Retries and fallbacks** Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We\u2019re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost. **Access intermediate results** For more complex chains it\u2019s often very useful to access the results of intermediate steps even before the final output is produced. This can be used let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it\u2019s available on every [LangServe](/docs/langserve) server. **Input and output schemas** Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe. **Seamless LangSmith tracing integration** As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, **all** steps are automatically logged to [LangSmith](/docs/langsmith/) for maximum observability and debuggability. **Seamless LangServe deployment integration** Any chain created with LCEL can be easily deployed using LangServe."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/get_started/installation.mdx"}, "data": "# Installation ## Official release To install LangChain run: import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import CodeBlock from \"@theme/CodeBlock\"; pip install langchain conda install langchain -c conda-forge This will install the bare minimum requirements of LangChain. A lot of the value of LangChain comes when integrating it with various model providers, datastores, etc. By default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately. ## From source If you want to install from source, you can do so by cloning the repo and be sure that the directory is `PATH/TO/REPO/langchain/libs/langchain` running: ```bash pip install -e . ``` ## Langchain experimental The `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses. Install with: ```bash pip install langchain-experimental ``` ## LangChain CLI The LangChain CLI is useful for working with LangChain templates and other LangServe projects. Install with: ```bash pip install langchain-cli ``` ## LangServe LangServe helps developers deploy LangChain runnables and chains as a REST API. LangServe is automatically installed by LangChain CLI. If not using LangChain CLI, install with: ```bash pip install \"langserve[all]\" ``` for both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code. ## LangSmith SDK The LangSmith SDK is automatically installed by LangChain. If not using LangChain, install with: ```bash pip install langsmith ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/get_started/introduction.mdx"}, "data": "--- sidebar_position: 0 --- # Introduction **LangChain** is a framework for developing applications powered by language models. It enables applications that: - **Are context-aware**: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.) - **Reason**: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.) This framework consists of several parts. - **LangChain Libraries**: The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents. - **[LangChain Templates](/docs/templates)**: A collection of easily deployable reference architectures for a wide variety of tasks. - **[LangServe](/docs/langserve)**: A library for deploying LangChain chains as a REST API. - **[LangSmith](/docs/langsmith)**: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain. ![LangChain Diagram](/img/langchain_stack.png) Together, these products simplify the entire application lifecycle: - **Develop**: Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference. - **Productionize**: Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence. - **Deploy**: Turn any chain into an API with LangServe. ## LangChain Libraries The main value props of the LangChain packages are: 1. **Components**: composable tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not 2. **Off-the-shelf chains**: built-in assemblages of components for accomplishing higher-level tasks Off-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones. ## Get started [Here\u2019s](/docs/get_started/installation) how to install LangChain, set up your environment, and start building. We recommend following our [Quickstart](/docs/get_started/quickstart) guide to familiarize yourself with the framework by building your first LangChain application. Read up on our [Security](/docs/security) best practices to make sure you're developing safely with LangChain. :::note These docs focus on the Python LangChain library. [Head here]( for docs on the JavaScript LangChain library. ::: ## LangChain Expression Language (LCEL) LCEL is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest \u201cprompt + LLM\u201d chain to the most complex chains. - **[Overview](/docs/expression_language/)**: LCEL and its benefits - **[Interface](/docs/expression_language/interface)**: The standard interface for LCEL objects - **[How-to](/docs/expression_language/interface)**: Key features of LCEL - **[Cookbook](/docs/expression_language/cookbook)**: Example code for accomplishing common tasks ## Modules LangChain provides standard, extendable interfaces and integrations for the following modules: #### [Model I/O](/docs/modules/model_io/) Interface with language models #### [Retrieval](/docs/modules/data_connection/) Interface with application-specific data #### [Agents](/docs/modules/agents/) Let models choose which tools to use given high-level directives ## Examples, ecosystem, and resources ### [Use cases](/docs/use_cases/question_answering/) Walkthroughs and techniques for common end-to-end use cases, like: - [Document question answering](/docs/use_cases/question_answering/) - [Chatbots](/docs/use_cases/chatbots/) - [Analyzing structured data](/docs/use_cases/qa_structured/sql/) - and much more... ### [Integrations](/docs/integrations/providers/) LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of [integrations](/docs/integrations/providers/). ### [Guides](/docs/guides/adapters/openai) Best practices for developing with LangChain. ### [API reference]( Head to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental Python packages. ### [Developer's guide](/docs/contributing) Check out the developer's guide for guidelines on contributing and help getting your dev environment set up. ### [Community](/docs/community) Head to the [Community navigator](/docs/community) to find places to ask questions, share feedback, meet other developers, and dream about the future of LLM\u2019s."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/get_started/quickstart.mdx"}, "data": "# Quickstart In this quickstart we'll show you how to: - Get setup with LangChain, LangSmith and LangServe - Use the most basic and common components of LangChain: prompt templates, models, and output parsers - Use LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chaining - Build simple application with LangChain - Trace your application with LangSmith - Serve your application with LangServe That's a fair amount to cover! Let's dive in. ## Setup ### Installation To install LangChain run: import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import CodeBlock from \"@theme/CodeBlock\"; pip install langchain conda install langchain -c conda-forge For more details, see our [Installation guide](/docs/get_started/installation). ### Environment Using LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs. First we'll need to install their Python package: ```bash pip install openai ``` Accessing the API requires an API key, which you can get by creating an account and heading [here]( Once we have a key we'll want to set it as an environment variable by running: ```bash export OPENAI_API_KEY=\"...\" ``` If you'd prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class: ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(openai_api_key=\"...\") ``` ### LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith]( Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces: ```shell export LANGCHAIN_TRACING_V2=\"true\" export LANGCHAIN_API_KEY=... ``` ### LangServe LangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we'll show how you can deploy your app with LangServe. Install with: ```bash pip install \"langserve[all]\" ``` ## Building with LangChain LangChain provides many modules that can be used to build language model applications. Modules can be used as standalones in simple applications and they can be composed for more complex use cases. Composition is powered by **LangChain Expression Language** (LCEL), which defines a unified `Runnable` interface that many modules implement, making it possible to seamlessly chain components. The simplest and most common chain contains three things: - LLM/Chat Model: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them. - Prompt Template: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial. - Output Parser: These translate the raw response from the language model to a more workable format, making it easy to use the output downstream. In this guide we'll cover those three components individually, and then go over how to combine them. Understanding these concepts will set you up well for being able to use and customize LangChain applications. Most LangChain applications allow you to configure the model and/or the prompt, so knowing how to take advantage of this will be a big enabler. ### LLM / Chat Model There are two types of language models: - `LLM`: underlying model takes a string as input and returns a string - `ChatModel`: underlying model takes a list of messages as input and returns a message Strings are simple, but what exactly are messages? The base message interface is defined by `BaseMessage`, which has two required attributes: - `content`: The content of the message. Usually a string. - `role`: The entity from which the `BaeMessage` is coming. LangChain provides several objects to easily distinguish between different roles: - `HumanMessage`: A `BaseMessage` coming from a human/user. - `AIMessage`: A `BaseMessage` coming from an AI/assistant. - `SystemMessage`: A `BaseMessage` coming from the system. - `FunctionMessage` / `ToolMessage`: A `BaseMessage` containing the output of a function or tool call. If none of those roles sound right, there is also a `ChatMessage` class where you can specify the role manually. LangChain provides a common interface that's shared by both `LLM`s and `ChatModel`s. However it's useful to understand the difference in order to most effectively construct prompts for a given language model. The simplest way to call an `LLM` or `ChatModel` is using `.invoke()`, the universal synchronous call method for all LangChain Expression Language (LCEL) objects: - `LLM.invoke`: Takes in a string, returns a string. - `ChatModel.invoke`: Takes in a list of `BaseMessage`, returns a `BaseMessage`. The input types for these methods are actually more general than this, but for simplicity here we can assume LLMs only take strings and Chat models only takes lists of messages. Check out the \"Go deeper\" section below to learn more about model invocation. Let's see how to work with these different types of models and these different types of inputs. First, let's import an LLM and a ChatModel. ```python from langchain.llms import OpenAI from langchain.chat_models import ChatOpenAI llm = OpenAI() chat_model = ChatOpenAI() ``` `LLM` and `ChatModel` objects are effectively configuration objects. You can initialize them with parameters like `temperature` and others, and pass them around. ```python from langchain.schema import HumanMessage text = \"What would be a good company name for a company that makes colorful socks?\" messages = [HumanMessage(content=text)] llm.invoke(text) # >> Feetful of Fun chat_model.invoke(messages) # >> AIMessage(content=\"Socks O'Color\") ``` Go deeper `LLM.invoke` and `ChatModel.invoke` actually both support as input any of `Union[str, List[BaseMessage], PromptValue]`. `PromptValue` is an object that defines it's own custom logic for returning it's inputs either as a string or as messages. `LLM`s have logic for coercing any of these into a string, and `ChatModel`s have logic for coercing any of these to messages. The fact that `LLM` and `ChatModel` accept the same inputs means that you can directly swap them for one another in most chains without breaking anything, though it's of course important to think about how inputs are being coerced and how that may affect model performance. To dive deeper on models head to the [Language models](/docs/modules/model_io/models) section. ### Prompt templates Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand. In the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it would be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions. PromptTemplates help with exactly this! They bundle up all the logic for going from user input into a fully formatted prompt. This can start off very simple - for example, a prompt to produce the above string would just be: ```python from langchain.prompts import PromptTemplate prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\") prompt.format(product=\"colorful socks\") ``` ```python What is a good name for a company that makes colorful socks? ``` However, the advantages of using these over raw string formatting are several. You can \"partial\" out variables - e.g. you can format only some of the variables at a time. You can compose them together, easily combining different templates into a single prompt. For explanations of these functionalities, see the [section on prompts](/docs/modules/model_io/prompts) for more detail. `PromptTemplate`s can also be used to produce a list of messages. In this case, the prompt not only contains information about the content, but also each message (its role, its position in the list, etc.). Here, what happens most often is a `ChatPromptTemplate` is a list of `ChatMessageTemplates`. Each `ChatMessageTemplate` contains instructions for how to format that `ChatMessage` - its role, and then also its content. Let's take a look at this below: ```python from langchain.prompts.chat import ChatPromptTemplate template = \"You are a helpful assistant that translates {input_language} to {output_language}.\" human_template = \"{text}\" chat_prompt = ChatPromptTemplate.from_messages([ (\"system\", template), (\"human\", human_template), ]) chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\") ``` ```pycon [ SystemMessage(content=\"You are a helpful assistant that translates English to French.\", additional_kwargs={}), HumanMessage(content=\"I love programming.\") ] ``` ChatPromptTemplates can also be constructed in other ways - see the [section on prompts](/docs/modules/model_io/prompts) for more detail. ### Output parsers `OutputParsers` convert the raw output of a language model into a format that can be used downstream. There are few main types of `OutputParser`s, including: - Convert text from `LLM` into structured information (e.g. JSON) - Convert a `ChatMessage` into just a string - Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string. For full information on this, see the [section on output parsers](/docs/modules/model_io/output_parsers). In this getting started guide, we will write our own output parser - one that converts a comma separated list into a list. ```python from langchain.schema import BaseOutputParser class CommaSeparatedListOutputParser(BaseOutputParser): \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\" def parse(self, text: str): \"\"\"Parse the output of an LLM call.\"\"\" return text.strip().split(\", \") CommaSeparatedListOutputParser().parse(\"hi, bye\") # >> ['hi', 'bye'] ``` ### Composing with LCEL We can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser. This is a convenient way to bundle up a modular piece of logic. Let's see it in action! ```python from typing import List from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema import BaseOutputParser class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]): \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\" def parse(self, text: str) -> List[str]: \"\"\"Parse the output of an LLM call.\"\"\" return text.strip().split(\", \") template = \"\"\"You are a helpful assistant who generates comma separated lists. A user will pass in a category, and you should generate 5 objects in that category in a comma separated list. ONLY return a comma separated list, and nothing more.\"\"\" human_template = \"{text}\" chat_prompt = ChatPromptTemplate.from_messages([ (\"system\", template), (\"human\", human_template), ]) chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser() chain.invoke({\"text\": \"colors\"}) # >> ['red', 'blue', 'green', 'yellow', 'orange'] ``` Note that we are using the `|` syntax to join these components together. This `|` syntax is powered by the LangChain Expression Language (LCEL) and relies on the universal `Runnable` interface that all of these objects implement. To learn more about LCEL, read the documentation [here](/docs/expression_language). ## Tracing with LangSmith Assuming we've set our environment variables as shown in the beginning, all of the model and chain calls we've been making will have been automatically logged to LangSmith. Once there, we can use LangSmith to debug and annotate our application traces, then turn them into datasets for evaluating future iterations of the application. Check out what the trace for the above chain would look like: For more on LangSmith [head here](/docs/langsmith/). ## Serving with LangServe Now that we've built an application, we need to serve it. That's where LangServe comes in. LangServe helps developers deploy LCEL chains as a REST API. The library is integrated with FastAPI and uses pydantic for data validation. ### Server To create a server for our application we'll make a `serve.py` file with three things: 1. The definition of our chain (same as above) 2. Our FastAPI app 3. A definition of a route from which to serve the chain, which is done with `langserve.add_routes` ```python #!/usr/bin/env python from typing import List from fastapi import FastAPI from langchain.prompts import ChatPromptTemplate from langchain.chat_models import ChatOpenAI from langchain.schema import BaseOutputParser from langserve import add_routes # 1. Chain definition class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]): \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\" def parse(self, text: str) -> List[str]: \"\"\"Parse the output of an LLM call.\"\"\" return text.strip().split(\", \") template = \"\"\"You are a helpful assistant who generates comma separated lists. A user will pass in a category, and you should generate 5 objects in that category in a comma separated list. ONLY return a comma separated list, and nothing more.\"\"\" human_template = \"{text}\" chat_prompt = ChatPromptTemplate.from_messages([ (\"system\", template), (\"human\", human_template), ]) category_chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser() # 2. App definition app = FastAPI( title=\"LangChain Server\", version=\"1.0\", description=\"A simple api server using Langchain's Runnable interfaces\", ) # 3. Adding chain route add_routes( app, category_chain, path=\"/category_chain\", ) if __name__ == \"__main__\": import uvicorn uvicorn.run(app, host=\"localhost\", port=8000) ``` And that's it! If we execute this file: ```bash python serve.py ``` we should see our chain being served at localhost:8000. ### Playground Every LangServe service comes with a simple built-in UI for configuring and invoking the application with streaming output and visibility into intermediate steps. Head to to try it out! ### Client Now let's set up a client for programmatically interacting with our service. We can easily do this with the `langserve.RemoteRunnable`. Using this, we can interact with the served chain as if it were running client-side. ```python from langserve import RemoteRunnable remote_chain = RemoteRunnable(\" remote_chain.invoke({\"text\": \"colors\"}) # >> ['red', 'blue', 'green', 'yellow', 'orange'] ``` To learn more about the many other features of LangServe [head here](/docs/langserve). ## Next steps We've touched on how to build an application with LangChain, how to trace it with LangSmith, and how to serve it with LangServe. There are a lot more features in all three of these than we can cover here. To continue on your journey: - Read up on [LangChain Expression Language (LCEL)](/docs/expression_language) to learn how to chain these components together - [Dive deeper](/docs/modules/model_io) into LLMs, prompts, and output parsers and learn the other [key components](/docs/modules) - Explore common [end-to-end use cases](/docs/use_cases/qa_structured/sql) and [template applications](/docs/templates) - [Read up on LangSmith](/docs/langsmith/), the platform for debugging, testing, monitoring and more - Learn more about serving your applications with [LangServe](/docs/langserve)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/guides/debugging.md"}, "data": "# Debugging If you're building with LLMs, at some point something will break, and you'll need to debug. A model call will fail, or the model output will be misformatted, or there will be some nested model calls and it won't be clear where along the way an incorrect output was created. Here are a few different tools and functionalities to aid in debugging. ## Tracing Platforms with tracing capabilities like [LangSmith](/docs/langsmith/) and [WandB](/docs/integrations/providers/wandb_tracing) are the most comprehensive solutions for debugging. These platforms make it easy to not only log and visualize LLM apps, but also to actively debug, test and refine them. For anyone building production-grade LLM applications, we highly recommend using a platform like this. ![LangSmith run](/img/run_details.png) ## `set_debug` and `set_verbose` If you're prototyping in Jupyter Notebooks or running Python scripts, it can be helpful to print out the intermediate steps of a Chain run. There are a number of ways to enable printing at varying degrees of verbosity. Let's suppose we have a simple agent, and want to visualize the actions it takes and tool outputs it receives. Without any debugging, here's what we see: ```python from langchain.agents import AgentType, initialize_agent, load_tools from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0) tools = load_tools([\"ddg-search\", \"llm-math\"], llm=llm) agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION) ``` ```python agent.run(\"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\") ``` ``` 'The director of the 2023 film Oppenheimer is Christopher Nolan and he is approximately 19345 days old in 2023.' ``` ### `set_debug(True)` Setting the global `debug` flag will cause all LangChain components with callback support (chains, models, agents, tools, retrievers) to print the inputs they receive and outputs they generate. This is the most verbose setting and will fully log raw inputs and outputs. ```python from langchain.globals import set_debug set_debug(True) agent.run(\"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\") ``` Console output ``` [chain/start] [1:RunTypeEnum.chain:AgentExecutor] Entering Chain run with input: { \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\" } [chain/start] [1:RunTypeEnum.chain:AgentExecutor > 2:RunTypeEnum.chain:LLMChain] Entering Chain run with input: { \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\", \"agent_scratchpad\": \"\", \"stop\": [ \"\\nObservation:\", \"\\n\\tObservation:\" ] } [llm/start] [1:RunTypeEnum.chain:AgentExecutor > 2:RunTypeEnum.chain:LLMChain > 3:RunTypeEnum.llm:ChatOpenAI] Entering LLM run with input: { \"prompts\": [ \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nduckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [duckduckgo_search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\\nThought:\" ] } [llm/end] [1:RunTypeEnum.chain:AgentExecutor > 2:RunTypeEnum.chain:LLMChain > 3:RunTypeEnum.llm:ChatOpenAI] [5.53s] Exiting LLM run with output: { \"generations\": [ [ { \"text\": \"I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\", \"generation_info\": { \"finish_reason\": \"stop\" }, \"message\": { \"lc\": 1, \"type\": \"constructor\", \"id\": [ \"langchain\", \"schema\", \"messages\", \"AIMessage\" ], \"kwargs\": { \"content\": \"I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\", \"additional_kwargs\": {} } } } ] ], \"llm_output\": { \"token_usage\": { \"prompt_tokens\": 206, \"completion_tokens\": 71, \"total_tokens\": 277 }, \"model_name\": \"gpt-4\" }, \"run\": null } [chain/end] [1:RunTypeEnum.chain:AgentExecutor > 2:RunTypeEnum.chain:LLMChain] [5.53s] Exiting Chain run with output: { \"text\": \"I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\" } [tool/start] [1:RunTypeEnum.chain:AgentExecutor > 4:RunTypeEnum.tool:duckduckgo_search] Entering Tool run with input: \"Director of the 2023 film Oppenheimer and their age\" [tool/end] [1:RunTypeEnum.chain:AgentExecutor > 4:RunTypeEnum.tool:duckduckgo_search] [1.51s] Exiting Tool run with output: \"Capturing the mad scramble to build the first atomic bomb required rapid-fire filming, strict set rules and the construction of an entire 1940s western town. By Jada Yuan. July 19, 2023 at 5:00 a ... In Christopher Nolan's new film, \"Oppenheimer,\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. Christopher Nolan goes deep on 'Oppenheimer,' his most 'extreme' film to date. By Kenneth Turan. July 11, 2023 5 AM PT. For Subscribers. Christopher Nolan is photographed in Los Angeles ... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age.\" [chain/start] [1:RunTypeEnum.chain:AgentExecutor > 5:RunTypeEnum.chain:LLMChain] Entering Chain run with input: { \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\", \"agent_scratchpad\": \"I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\\nObservation: Capturing the mad scramble to build the first atomic bomb required rapid-fire filming, strict set rules and the construction of an entire 1940s western town. By Jada Yuan. July 19, 2023 at 5:00 a ... In Christopher Nolan's new film, \\\"Oppenheimer,\\\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. Christopher Nolan goes deep on 'Oppenheimer,' his most 'extreme' film to date. By Kenneth Turan. July 11, 2023 5 AM PT. For Subscribers. Christopher Nolan is photographed in Los Angeles ... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age.\\nThought:\", \"stop\": [ \"\\nObservation:\", \"\\n\\tObservation:\" ] } [llm/start] [1:RunTypeEnum.chain:AgentExecutor > 5:RunTypeEnum.chain:LLMChain > 6:RunTypeEnum.llm:ChatOpenAI] Entering LLM run with input: { \"prompts\": [ \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nduckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [duckduckgo_search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\\nThought:I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\\nObservation: Capturing the mad scramble to build the first atomic bomb required rapid-fire filming, strict set rules and the construction of an entire 1940s western town. By Jada Yuan. July 19, 2023 at 5:00 a ... In Christopher Nolan's new film, \\\"Oppenheimer,\\\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. Christopher Nolan goes deep on 'Oppenheimer,' his most 'extreme' film to date. By Kenneth Turan. July 11, 2023 5 AM PT. For Subscribers. Christopher Nolan is photographed in Los Angeles ... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age.\\nThought:\" ] } [llm/end] [1:RunTypeEnum.chain:AgentExecutor > 5:RunTypeEnum.chain:LLMChain > 6:RunTypeEnum.llm:ChatOpenAI] [4.46s] Exiting LLM run with output: { \"generations\": [ [ { \"text\": \"The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his age.\\nAction: duckduckgo_search\\nAction Input: \\\"Christopher Nolan age\\\"\", \"generation_info\": { \"finish_reason\": \"stop\" }, \"message\": { \"lc\": 1, \"type\": \"constructor\", \"id\": [ \"langchain\", \"schema\", \"messages\", \"AIMessage\" ], \"kwargs\": { \"content\": \"The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his age.\\nAction: duckduckgo_search\\nAction Input: \\\"Christopher Nolan age\\\"\", \"additional_kwargs\": {} } } } ] ], \"llm_output\": { \"token_usage\": { \"prompt_tokens\": 550, \"completion_tokens\": 39, \"total_tokens\": 589 }, \"model_name\": \"gpt-4\" }, \"run\": null } [chain/end] [1:RunTypeEnum.chain:AgentExecutor > 5:RunTypeEnum.chain:LLMChain] [4.46s] Exiting Chain run with output: { \"text\": \"The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his age.\\nAction: duckduckgo_search\\nAction Input: \\\"Christopher Nolan age\\\"\" } [tool/start] [1:RunTypeEnum.chain:AgentExecutor > 7:RunTypeEnum.tool:duckduckgo_search] Entering Tool run with input: \"Christopher Nolan age\" [tool/end] [1:RunTypeEnum.chain:AgentExecutor > 7:RunTypeEnum.tool:duckduckgo_search] [1.33s] Exiting Tool run with output: \"Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. July 30, 1970 (age 52) London England Notable Works: \"Dunkirk\" \"Tenet\" \"The Prestige\" See all related content \u2192 Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film July 11, 2023 5 AM PT For Subscribers Christopher Nolan is photographed in Los Angeles. (Joe Pugliese / For The Times) This is not the story I was supposed to write. Oppenheimer director Christopher Nolan, Cillian Murphy, Emily Blunt and Matt Damon on the stakes of making a three-hour, CGI-free summer film. Christopher Nolan, the director behind such films as \"Dunkirk,\" \"Inception,\" \"Interstellar,\" and the \"Dark Knight\" trilogy, has spent the last three years living in Oppenheimer's world, writing ...\" [chain/start] [1:RunTypeEnum.chain:AgentExecutor > 8:RunTypeEnum.chain:LLMChain] Entering Chain run with input: { \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\", \"agent_scratchpad\": \"I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\\nObservation: Capturing the mad scramble to build the first atomic bomb required rapid-fire filming, strict set rules and the construction of an entire 1940s western town. By Jada Yuan. July 19, 2023 at 5:00 a ... In Christopher Nolan's new film, \\\"Oppenheimer,\\\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. Christopher Nolan goes deep on 'Oppenheimer,' his most 'extreme' film to date. By Kenneth Turan. July 11, 2023 5 AM PT. For Subscribers. Christopher Nolan is photographed in Los Angeles ... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age.\\nThought:The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his age.\\nAction: duckduckgo_search\\nAction Input: \\\"Christopher Nolan age\\\"\\nObservation: Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. July 30, 1970 (age 52) London England Notable Works: \\\"Dunkirk\\\" \\\"Tenet\\\" \\\"The Prestige\\\" See all related content \u2192 Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film July 11, 2023 5 AM PT For Subscribers Christopher Nolan is photographed in Los Angeles. (Joe Pugliese / For The Times) This is not the story I was supposed to write. Oppenheimer director Christopher Nolan, Cillian Murphy, Emily Blunt and Matt Damon on the stakes of making a three-hour, CGI-free summer film. Christopher Nolan, the director behind such films as \\\"Dunkirk,\\\" \\\"Inception,\\\" \\\"Interstellar,\\\" and the \\\"Dark Knight\\\" trilogy, has spent the last three years living in Oppenheimer's world, writing ...\\nThought:\", \"stop\": [ \"\\nObservation:\", \"\\n\\tObservation:\" ] } [llm/start] [1:RunTypeEnum.chain:AgentExecutor > 8:RunTypeEnum.chain:LLMChain > 9:RunTypeEnum.llm:ChatOpenAI] Entering LLM run with input: { \"prompts\": [ \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nduckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [duckduckgo_search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\\nThought:I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\\nObservation: Capturing the mad scramble to build the first atomic bomb required rapid-fire filming, strict set rules and the construction of an entire 1940s western town. By Jada Yuan. July 19, 2023 at 5:00 a ... In Christopher Nolan's new film, \\\"Oppenheimer,\\\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. Christopher Nolan goes deep on 'Oppenheimer,' his most 'extreme' film to date. By Kenneth Turan. July 11, 2023 5 AM PT. For Subscribers. Christopher Nolan is photographed in Los Angeles ... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age.\\nThought:The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his age.\\nAction: duckduckgo_search\\nAction Input: \\\"Christopher Nolan age\\\"\\nObservation: Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. July 30, 1970 (age 52) London England Notable Works: \\\"Dunkirk\\\" \\\"Tenet\\\" \\\"The Prestige\\\" See all related content \u2192 Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film July 11, 2023 5 AM PT For Subscribers Christopher Nolan is photographed in Los Angeles. (Joe Pugliese / For The Times) This is not the story I was supposed to write. Oppenheimer director Christopher Nolan, Cillian Murphy, Emily Blunt and Matt Damon on the stakes of making a three-hour, CGI-free summer film. Christopher Nolan, the director behind such films as \\\"Dunkirk,\\\" \\\"Inception,\\\" \\\"Interstellar,\\\" and the \\\"Dark Knight\\\" trilogy, has spent the last three years living in Oppenheimer's world, writing ...\\nThought:\" ] } [llm/end] [1:RunTypeEnum.chain:AgentExecutor > 8:RunTypeEnum.chain:LLMChain > 9:RunTypeEnum.llm:ChatOpenAI] [2.69s] Exiting LLM run with output: { \"generations\": [ [ { \"text\": \"Christopher Nolan was born on July 30, 1970, which makes him 52 years old in 2023. Now I need to calculate his age in days.\\nAction: Calculator\\nAction Input: 52*365\", \"generation_info\": { \"finish_reason\": \"stop\" }, \"message\": { \"lc\": 1, \"type\": \"constructor\", \"id\": [ \"langchain\", \"schema\", \"messages\", \"AIMessage\" ], \"kwargs\": { \"content\": \"Christopher Nolan was born on July 30, 1970, which makes him 52 years old in 2023. Now I need to calculate his age in days.\\nAction: Calculator\\nAction Input: 52*365\", \"additional_kwargs\": {} } } } ] ], \"llm_output\": { \"token_usage\": { \"prompt_tokens\": 868, \"completion_tokens\": 46, \"total_tokens\": 914 }, \"model_name\": \"gpt-4\" }, \"run\": null } [chain/end] [1:RunTypeEnum.chain:AgentExecutor > 8:RunTypeEnum.chain:LLMChain] [2.69s] Exiting Chain run with output: { \"text\": \"Christopher Nolan was born on July 30, 1970, which makes him 52 years old in 2023. Now I need to calculate his age in days.\\nAction: Calculator\\nAction Input: 52*365\" } [tool/start] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator] Entering Tool run with input: \"52*365\" [chain/start] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator > 11:RunTypeEnum.chain:LLMMathChain] Entering Chain run with input: { \"question\": \"52*365\" } [chain/start] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator > 11:RunTypeEnum.chain:LLMMathChain > 12:RunTypeEnum.chain:LLMChain] Entering Chain run with input: { \"question\": \"52*365\", \"stop\": [ \"```output\" ] } [llm/start] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator > 11:RunTypeEnum.chain:LLMMathChain > 12:RunTypeEnum.chain:LLMChain > 13:RunTypeEnum.llm:ChatOpenAI] Entering LLM run with input: { \"prompts\": [ \"Human: Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${Question with math problem.}\\n```text\\n${single line mathematical expression that solves the problem}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${Output of running the code}\\n```\\nAnswer: ${Answer}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\\\"37593 * 67\\\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\\\"37593**(1/5)\\\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: 52*365\" ] } [llm/end] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator > 11:RunTypeEnum.chain:LLMMathChain > 12:RunTypeEnum.chain:LLMChain > 13:RunTypeEnum.llm:ChatOpenAI] [2.89s] Exiting LLM run with output: { \"generations\": [ [ { \"text\": \"```text\\n52*365\\n```\\n...numexpr.evaluate(\\\"52*365\\\")...\\n\", \"generation_info\": { \"finish_reason\": \"stop\" }, \"message\": { \"lc\": 1, \"type\": \"constructor\", \"id\": [ \"langchain\", \"schema\", \"messages\", \"AIMessage\" ], \"kwargs\": { \"content\": \"```text\\n52*365\\n```\\n...numexpr.evaluate(\\\"52*365\\\")...\\n\", \"additional_kwargs\": {} } } } ] ], \"llm_output\": { \"token_usage\": { \"prompt_tokens\": 203, \"completion_tokens\": 19, \"total_tokens\": 222 }, \"model_name\": \"gpt-4\" }, \"run\": null } [chain/end] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator > 11:RunTypeEnum.chain:LLMMathChain > 12:RunTypeEnum.chain:LLMChain] [2.89s] Exiting Chain run with output: { \"text\": \"```text\\n52*365\\n```\\n...numexpr.evaluate(\\\"52*365\\\")...\\n\" } [chain/end] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator > 11:RunTypeEnum.chain:LLMMathChain] [2.90s] Exiting Chain run with output: { \"answer\": \"Answer: 18980\" } [tool/end] [1:RunTypeEnum.chain:AgentExecutor > 10:RunTypeEnum.tool:Calculator] [2.90s] Exiting Tool run with output: \"Answer: 18980\" [chain/start] [1:RunTypeEnum.chain:AgentExecutor > 14:RunTypeEnum.chain:LLMChain] Entering Chain run with input: { \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\", \"agent_scratchpad\": \"I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\\nObservation: Capturing the mad scramble to build the first atomic bomb required rapid-fire filming, strict set rules and the construction of an entire 1940s western town. By Jada Yuan. July 19, 2023 at 5:00 a ... In Christopher Nolan's new film, \\\"Oppenheimer,\\\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. Christopher Nolan goes deep on 'Oppenheimer,' his most 'extreme' film to date. By Kenneth Turan. July 11, 2023 5 AM PT. For Subscribers. Christopher Nolan is photographed in Los Angeles ... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age.\\nThought:The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his age.\\nAction: duckduckgo_search\\nAction Input: \\\"Christopher Nolan age\\\"\\nObservation: Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. July 30, 1970 (age 52) London England Notable Works: \\\"Dunkirk\\\" \\\"Tenet\\\" \\\"The Prestige\\\" See all related content \u2192 Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film July 11, 2023 5 AM PT For Subscribers Christopher Nolan is photographed in Los Angeles. (Joe Pugliese / For The Times) This is not the story I was supposed to write. Oppenheimer director Christopher Nolan, Cillian Murphy, Emily Blunt and Matt Damon on the stakes of making a three-hour, CGI-free summer film. Christopher Nolan, the director behind such films as \\\"Dunkirk,\\\" \\\"Inception,\\\" \\\"Interstellar,\\\" and the \\\"Dark Knight\\\" trilogy, has spent the last three years living in Oppenheimer's world, writing ...\\nThought:Christopher Nolan was born on July 30, 1970, which makes him 52 years old in 2023. Now I need to calculate his age in days.\\nAction: Calculator\\nAction Input: 52*365\\nObservation: Answer: 18980\\nThought:\", \"stop\": [ \"\\nObservation:\", \"\\n\\tObservation:\" ] } [llm/start] [1:RunTypeEnum.chain:AgentExecutor > 14:RunTypeEnum.chain:LLMChain > 15:RunTypeEnum.llm:ChatOpenAI] Entering LLM run with input: { \"prompts\": [ \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nduckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [duckduckgo_search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\\nThought:I need to find out who directed the 2023 film Oppenheimer and their age. Then, I need to calculate their age in days. I will use DuckDuckGo to find out the director and their age.\\nAction: duckduckgo_search\\nAction Input: \\\"Director of the 2023 film Oppenheimer and their age\\\"\\nObservation: Capturing the mad scramble to build the first atomic bomb required rapid-fire filming, strict set rules and the construction of an entire 1940s western town. By Jada Yuan. July 19, 2023 at 5:00 a ... In Christopher Nolan's new film, \\\"Oppenheimer,\\\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. Christopher Nolan goes deep on 'Oppenheimer,' his most 'extreme' film to date. By Kenneth Turan. July 11, 2023 5 AM PT. For Subscribers. Christopher Nolan is photographed in Los Angeles ... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age.\\nThought:The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his age.\\nAction: duckduckgo_search\\nAction Input: \\\"Christopher Nolan age\\\"\\nObservation: Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. July 30, 1970 (age 52) London England Notable Works: \\\"Dunkirk\\\" \\\"Tenet\\\" \\\"The Prestige\\\" See all related content \u2192 Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film July 11, 2023 5 AM PT For Subscribers Christopher Nolan is photographed in Los Angeles. (Joe Pugliese / For The Times) This is not the story I was supposed to write. Oppenheimer director Christopher Nolan, Cillian Murphy, Emily Blunt and Matt Damon on the stakes of making a three-hour, CGI-free summer film. Christopher Nolan, the director behind such films as \\\"Dunkirk,\\\" \\\"Inception,\\\" \\\"Interstellar,\\\" and the \\\"Dark Knight\\\" trilogy, has spent the last three years living in Oppenheimer's world, writing ...\\nThought:Christopher Nolan was born on July 30, 1970, which makes him 52 years old in 2023. Now I need to calculate his age in days.\\nAction: Calculator\\nAction Input: 52*365\\nObservation: Answer: 18980\\nThought:\" ] } [llm/end] [1:RunTypeEnum.chain:AgentExecutor > 14:RunTypeEnum.chain:LLMChain > 15:RunTypeEnum.llm:ChatOpenAI] [3.52s] Exiting LLM run with output: { \"generations\": [ [ { \"text\": \"I now know the final answer\\nFinal Answer: The director of the 2023 film Oppenheimer is Christopher Nolan and he is 52 years old. His age in days is approximately 18980 days.\", \"generation_info\": { \"finish_reason\": \"stop\" }, \"message\": { \"lc\": 1, \"type\": \"constructor\", \"id\": [ \"langchain\", \"schema\", \"messages\", \"AIMessage\" ], \"kwargs\": { \"content\": \"I now know the final answer\\nFinal Answer: The director of the 2023 film Oppenheimer is Christopher Nolan and he is 52 years old. His age in days is approximately 18980 days.\", \"additional_kwargs\": {} } } } ] ], \"llm_output\": { \"token_usage\": { \"prompt_tokens\": 926, \"completion_tokens\": 43, \"total_tokens\": 969 }, \"model_name\": \"gpt-4\" }, \"run\": null } [chain/end] [1:RunTypeEnum.chain:AgentExecutor > 14:RunTypeEnum.chain:LLMChain] [3.52s] Exiting Chain run with output: { \"text\": \"I now know the final answer\\nFinal Answer: The director of the 2023 film Oppenheimer is Christopher Nolan and he is 52 years old. His age in days is approximately 18980 days.\" } [chain/end] [1:RunTypeEnum.chain:AgentExecutor] [21.96s] Exiting Chain run with output: { \"output\": \"The director of the 2023 film Oppenheimer is Christopher Nolan and he is 52 years old. His age in days is approximately 18980 days.\" } 'The director of the 2023 film Oppenheimer is Christopher Nolan and he is 52 years old. His age in days is approximately 18980 days.' ``` ### `set_verbose(True)` Setting the `verbose` flag will print out inputs and outputs in a slightly more readable format and will skip logging certain raw outputs (like the token usage stats for an LLM call) so that you can focus on application logic. ```python from langchain.globals import set_verbose set_verbose(True) agent.run(\"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\") ``` Console output ``` > Entering new AgentExecutor chain... > Entering new LLMChain chain... Prompt after formatting: Answer the following questions as best you can. You have access to the following tools: duckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query. Calculator: Useful for when you need to answer questions about math. Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [duckduckgo_search, Calculator] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Question: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)? Thought: > Finished chain. First, I need to find out who directed the film Oppenheimer in 2023 and their birth date to calculate their age. Action: duckduckgo_search Action Input: \"Director of the 2023 film Oppenheimer\" Observation: Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. In Christopher Nolan's new film, \"Oppenheimer,\" Cillian Murphy stars as J. Robert ... 2023, 12:16 p.m. ET. ... including his role as the director of the Manhattan Engineer District, better ... J Robert Oppenheimer was the director of the secret Los Alamos Laboratory. It was established under US president Franklin D Roosevelt as part of the Manhattan Project to build the first atomic bomb. He oversaw the first atomic bomb detonation in the New Mexico desert in July 1945, code-named \"Trinity\". In this opening salvo of 2023's Oscar battle, Nolan has enjoined a star-studded cast for a retelling of the brilliant and haunted life of J. Robert Oppenheimer, the American physicist whose... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age. Thought: > Entering new LLMChain chain... Prompt after formatting: Answer the following questions as best you can. You have access to the following tools: duckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query. Calculator: Useful for when you need to answer questions about math. Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [duckduckgo_search, Calculator] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Question: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)? Thought:First, I need to find out who directed the film Oppenheimer in 2023 and their birth date to calculate their age. Action: duckduckgo_search Action Input: \"Director of the 2023 film Oppenheimer\" Observation: Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. In Christopher Nolan's new film, \"Oppenheimer,\" Cillian Murphy stars as J. Robert ... 2023, 12:16 p.m. ET. ... including his role as the director of the Manhattan Engineer District, better ... J Robert Oppenheimer was the director of the secret Los Alamos Laboratory. It was established under US president Franklin D Roosevelt as part of the Manhattan Project to build the first atomic bomb. He oversaw the first atomic bomb detonation in the New Mexico desert in July 1945, code-named \"Trinity\". In this opening salvo of 2023's Oscar battle, Nolan has enjoined a star-studded cast for a retelling of the brilliant and haunted life of J. Robert Oppenheimer, the American physicist whose... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age. Thought: > Finished chain. The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his birth date to calculate his age. Action: duckduckgo_search Action Input: \"Christopher Nolan birth date\" Observation: July 30, 1970 (age 52) London England Notable Works: \"Dunkirk\" \"Tenet\" \"The Prestige\" See all related content \u2192 Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. Christopher Nolan is currently 52 according to his birthdate July 30, 1970 Sun Sign Leo Born Place Westminster, London, England, United Kingdom Residence Los Angeles, California, United States Nationality Education Chris attended Haileybury and Imperial Service College, in Hertford Heath, Hertfordshire. Christopher Nolan's next movie will study the man who developed the atomic bomb, J. Robert Oppenheimer. Here's the release date, plot, trailers & more. July 2023 sees the release of Christopher Nolan's new film, Oppenheimer, his first movie since 2020's Tenet and his split from Warner Bros. Billed as an epic thriller about \"the man who ... Thought: > Entering new LLMChain chain... Prompt after formatting: Answer the following questions as best you can. You have access to the following tools: duckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query. Calculator: Useful for when you need to answer questions about math. Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [duckduckgo_search, Calculator] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Question: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)? Thought:First, I need to find out who directed the film Oppenheimer in 2023 and their birth date to calculate their age. Action: duckduckgo_search Action Input: \"Director of the 2023 film Oppenheimer\" Observation: Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. In Christopher Nolan's new film, \"Oppenheimer,\" Cillian Murphy stars as J. Robert ... 2023, 12:16 p.m. ET. ... including his role as the director of the Manhattan Engineer District, better ... J Robert Oppenheimer was the director of the secret Los Alamos Laboratory. It was established under US president Franklin D Roosevelt as part of the Manhattan Project to build the first atomic bomb. He oversaw the first atomic bomb detonation in the New Mexico desert in July 1945, code-named \"Trinity\". In this opening salvo of 2023's Oscar battle, Nolan has enjoined a star-studded cast for a retelling of the brilliant and haunted life of J. Robert Oppenheimer, the American physicist whose... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age. Thought:The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his birth date to calculate his age. Action: duckduckgo_search Action Input: \"Christopher Nolan birth date\" Observation: July 30, 1970 (age 52) London England Notable Works: \"Dunkirk\" \"Tenet\" \"The Prestige\" See all related content \u2192 Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. Christopher Nolan is currently 52 according to his birthdate July 30, 1970 Sun Sign Leo Born Place Westminster, London, England, United Kingdom Residence Los Angeles, California, United States Nationality Education Chris attended Haileybury and Imperial Service College, in Hertford Heath, Hertfordshire. Christopher Nolan's next movie will study the man who developed the atomic bomb, J. Robert Oppenheimer. Here's the release date, plot, trailers & more. July 2023 sees the release of Christopher Nolan's new film, Oppenheimer, his first movie since 2020's Tenet and his split from Warner Bros. Billed as an epic thriller about \"the man who ... Thought: > Finished chain. Christopher Nolan was born on July 30, 1970. Now I need to calculate his age in 2023 and then convert it into days. Action: Calculator Action Input: (2023 - 1970) * 365 > Entering new LLMMathChain chain... (2023 - 1970) * 365 > Entering new LLMChain chain... Prompt after formatting: Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question. Question: ${Question with math problem.} ```text ${single line mathematical expression that solves the problem} ``` ...numexpr.evaluate(text)... ```output ${Output of running the code} ``` Answer: ${Answer} Begin. Question: What is 37593 * 67? ```text 37593 * 67 ``` ...numexpr.evaluate(\"37593 * 67\")... ```output 2518731 ``` Answer: 2518731 Question: 37593^(1/5) ```text 37593**(1/5) ``` ...numexpr.evaluate(\"37593**(1/5)\")... ```output 8.222831614237718 ``` Answer: 8.222831614237718 Question: (2023 - 1970) * 365 > Finished chain. ```text (2023 - 1970) * 365 ``` ...numexpr.evaluate(\"(2023 - 1970) * 365\")... Answer: 19345 > Finished chain. Observation: Answer: 19345 Thought: > Entering new LLMChain chain... Prompt after formatting: Answer the following questions as best you can. You have access to the following tools: duckduckgo_search: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query. Calculator: Useful for when you need to answer questions about math. Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [duckduckgo_search, Calculator] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Question: Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)? Thought:First, I need to find out who directed the film Oppenheimer in 2023 and their birth date to calculate their age. Action: duckduckgo_search Action Input: \"Director of the 2023 film Oppenheimer\" Observation: Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. In Christopher Nolan's new film, \"Oppenheimer,\" Cillian Murphy stars as J. Robert ... 2023, 12:16 p.m. ET. ... including his role as the director of the Manhattan Engineer District, better ... J Robert Oppenheimer was the director of the secret Los Alamos Laboratory. It was established under US president Franklin D Roosevelt as part of the Manhattan Project to build the first atomic bomb. He oversaw the first atomic bomb detonation in the New Mexico desert in July 1945, code-named \"Trinity\". In this opening salvo of 2023's Oscar battle, Nolan has enjoined a star-studded cast for a retelling of the brilliant and haunted life of J. Robert Oppenheimer, the American physicist whose... Oppenheimer is a 2023 epic biographical thriller film written and directed by Christopher Nolan.It is based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin about J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Manhattan Project and thereby ushering in the Atomic Age. Thought:The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his birth date to calculate his age. Action: duckduckgo_search Action Input: \"Christopher Nolan birth date\" Observation: July 30, 1970 (age 52) London England Notable Works: \"Dunkirk\" \"Tenet\" \"The Prestige\" See all related content \u2192 Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. Christopher Nolan is currently 52 according to his birthdate July 30, 1970 Sun Sign Leo Born Place Westminster, London, England, United Kingdom Residence Los Angeles, California, United States Nationality Education Chris attended Haileybury and Imperial Service College, in Hertford Heath, Hertfordshire. Christopher Nolan's next movie will study the man who developed the atomic bomb, J. Robert Oppenheimer. Here's the release date, plot, trailers & more. July 2023 sees the release of Christopher Nolan's new film, Oppenheimer, his first movie since 2020's Tenet and his split from Warner Bros. Billed as an epic thriller about \"the man who ... Thought:Christopher Nolan was born on July 30, 1970. Now I need to calculate his age in 2023 and then convert it into days. Action: Calculator Action Input: (2023 - 1970) * 365 Observation: Answer: 19345 Thought: > Finished chain. I now know the final answer Final Answer: The director of the 2023 film Oppenheimer is Christopher Nolan and he is 53 years old in 2023. His age in days is 19345 days. > Finished chain. 'The director of the 2023 film Oppenheimer is Christopher Nolan and he is 53 years old in 2023. His age in days is 19345 days.' ``` ### `Chain(..., verbose=True)` You can also scope verbosity down to a single object, in which case only the inputs and outputs to that object are printed (along with any additional callbacks calls made specifically by that object). ```python # Passing verbose=True to initialize_agent will pass that along to the AgentExecutor (which is a Chain). agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, ) agent.run(\"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\") ``` Console output ``` > Entering new AgentExecutor chain... First, I need to find out who directed the film Oppenheimer in 2023 and their birth date. Then, I can calculate their age in years and days. Action: duckduckgo_search Action Input: \"Director of 2023 film Oppenheimer\" Observation: Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb. In Christopher Nolan's new film, \"Oppenheimer,\" Cillian Murphy stars as J. Robert Oppenheimer, the American physicist who oversaw the Manhattan Project in Los Alamos, N.M. Universal Pictures... J Robert Oppenheimer was the director of the secret Los Alamos Laboratory. It was established under US president Franklin D Roosevelt as part of the Manhattan Project to build the first atomic bomb. He oversaw the first atomic bomb detonation in the New Mexico desert in July 1945, code-named \"Trinity\". A Review of Christopher Nolan's new film 'Oppenheimer' , the story of the man who fathered the Atomic Bomb. Cillian Murphy leads an all star cast ... Release Date: July 21, 2023. Director ... For his new film, \"Oppenheimer,\" starring Cillian Murphy and Emily Blunt, director Christopher Nolan set out to build an entire 1940s western town. Thought:The director of the 2023 film Oppenheimer is Christopher Nolan. Now I need to find out his birth date to calculate his age. Action: duckduckgo_search Action Input: \"Christopher Nolan birth date\" Observation: July 30, 1970 (age 52) London England Notable Works: \"Dunkirk\" \"Tenet\" \"The Prestige\" See all related content \u2192 Recent News Jul. 13, 2023, 11:11 AM ET (AP) Cillian Murphy, playing Oppenheimer, finally gets to lead a Christopher Nolan film Christopher Edward Nolan CBE (born 30 July 1970) is a British and American filmmaker. Known for his Hollywood blockbusters with complex storytelling, Nolan is considered a leading filmmaker of the 21st century. His films have grossed $5 billion worldwide. The recipient of many accolades, he has been nominated for five Academy Awards, five BAFTA Awards and six Golden Globe Awards. Christopher Nolan is currently 52 according to his birthdate July 30, 1970 Sun Sign Leo Born Place Westminster, London, England, United Kingdom Residence Los Angeles, California, United States Nationality Education Chris attended Haileybury and Imperial Service College, in Hertford Heath, Hertfordshire. Christopher Nolan's next movie will study the man who developed the atomic bomb, J. Robert Oppenheimer. Here's the release date, plot, trailers & more. Date of Birth: 30 July 1970 . ... Christopher Nolan is a British-American film director, producer, and screenwriter. His films have grossed more than US$5 billion worldwide, and have garnered 11 Academy Awards from 36 nominations. ... Thought:Christopher Nolan was born on July 30, 1970. Now I can calculate his age in years and then in days. Action: Calculator Action Input: {\"operation\": \"subtract\", \"operands\": [2023, 1970]} Observation: Answer: 53 Thought:Christopher Nolan is 53 years old in 2023. Now I need to calculate his age in days. Action: Calculator Action Input: {\"operation\": \"multiply\", \"operands\": [53, 365]} Observation: Answer: 19345 Thought:I now know the final answer Final Answer: The director of the 2023 film Oppenheimer is Christopher Nolan. He is 53 years old in 2023, which is approximately 19345 days. > Finished chain. 'The director of the 2023 film Oppenheimer is Christopher Nolan. He is 53 years old in 2023, which is approximately 19345 days.' ``` ## Other callbacks `Callbacks` are what we use to execute any functionality within a component outside the primary component logic. All of the above solutions use `Callbacks` under the hood to log intermediate steps of components. There are a number of `Callbacks` relevant for debugging that come with LangChain out of the box, like the [FileCallbackHandler](/docs/modules/callbacks/how_to/filecallbackhandler). You can also implement your own callbacks to execute custom functionality. See here for more info on [Callbacks](/docs/modules/callbacks/), how to use them, and customize them."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/guides/deployments/index.mdx"}, "data": "# Deployment In today's fast-paced technological landscape, the use of Large Language Models (LLMs) is rapidly expanding. As a result, it is crucial for developers to understand how to effectively deploy these models in production environments. LLM interfaces typically fall into two categories: - **Case 1: Utilizing External LLM Providers (OpenAI, Anthropic, etc.)** In this scenario, most of the computational burden is handled by the LLM providers, while LangChain simplifies the implementation of business logic around these services. This approach includes features such as prompt templating, chat message generation, caching, vector embedding database creation, preprocessing, etc. - **Case 2: Self-hosted Open-Source Models** Alternatively, developers can opt to use smaller, yet comparably capable, self-hosted open-source LLM models. This approach can significantly decrease costs, latency, and privacy concerns associated with transferring data to external LLM providers. Regardless of the framework that forms the backbone of your product, deploying LLM applications comes with its own set of challenges. It's vital to understand the trade-offs and key considerations when evaluating serving frameworks. ## Outline This guide aims to provide a comprehensive overview of the requirements for deploying LLMs in a production setting, focusing on: - **Designing a Robust LLM Application Service** - **Maintaining Cost-Efficiency** - **Ensuring Rapid Iteration** Understanding these components is crucial when assessing serving systems. LangChain integrates with several open-source projects designed to tackle these issues, providing a robust framework for productionizing your LLM applications. Some notable frameworks include: - [Ray Serve](/docs/ecosystem/integrations/ray_serve) - [BentoML]( - [OpenLLM](/docs/ecosystem/integrations/openllm) - [Modal](/docs/ecosystem/integrations/modal) - [Jina](/docs/ecosystem/integrations/jina#deployment) These links will provide further information on each ecosystem, assisting you in finding the best fit for your LLM deployment needs. ## Designing a Robust LLM Application Service When deploying an LLM service in production, it's imperative to provide a seamless user experience free from outages. Achieving 24/7 service availability involves creating and maintaining several sub-systems surrounding your application. ### Monitoring Monitoring forms an integral part of any system running in a production environment. In the context of LLMs, it is essential to monitor both performance and quality metrics. **Performance Metrics:** These metrics provide insights into the efficiency and capacity of your model. Here are some key examples: - Query per second (QPS): This measures the number of queries your model processes in a second, offering insights into its utilization. - Latency: This metric quantifies the delay from when your client sends a request to when they receive a response. - Tokens Per Second (TPS): This represents the number of tokens your model can generate in a second. **Quality Metrics:** These metrics are typically customized according to the business use-case. For instance, how does the output of your system compare to a baseline, such as a previous version? Although these metrics can be calculated offline, you need to log the necessary data to use them later. ### Fault tolerance Your application may encounter errors such as exceptions in your model inference or business logic code, causing failures and disrupting traffic. Other potential issues could arise from the machine running your application, such as unexpected hardware breakdowns or loss of spot-instances during high-demand periods. One way to mitigate these risks is by increasing redundancy through replica scaling and implementing recovery mechanisms for failed replicas. However, model replicas aren't the only potential points of failure. It's essential to build resilience against various failures that could occur at any point in your stack. ### Zero down time upgrade System upgrades are often necessary but can result in service disruptions if not handled correctly. One way to prevent downtime during upgrades is by implementing a smooth transition process from the old version to the new one. Ideally, the new version of your LLM service is deployed, and traffic gradually shifts from the old to the new version, maintaining a constant QPS throughout the process. ### Load balancing Load balancing, in simple terms, is a technique to distribute work evenly across multiple computers, servers, or other resources to optimize the utilization of the system, maximize throughput, minimize response time, and avoid overload of any single resource. Think of it as a traffic officer directing cars (requests) to different roads (servers) so that no single road becomes too congested. There are several strategies for load balancing. For example, one common method is the *Round Robin* strategy, where each request is sent to the next server in line, cycling back to the first when all servers have received a request. This works well when all servers are equally capable. However, if some servers are more powerful than others, you might use a *Weighted Round Robin* or *Least Connections* strategy, where more requests are sent to the more powerful servers, or to those currently handling the fewest active requests. Let's imagine you're running a LLM chain. If your application becomes popular, you could have hundreds or even thousands of users asking questions at the same time. If one server gets too busy (high load), the load balancer would direct new requests to another server that is less busy. This way, all your users get a timely response and the system remains stable. ## Maintaining Cost-Efficiency and Scalability Deploying LLM services can be costly, especially when you're handling a large volume of user interactions. Charges by LLM providers are usually based on tokens used, making a chat system inference on these models potentially expensive. However, several strategies can help manage these costs without compromising the quality of the service. ### Self-hosting models Several smaller and open-source LLMs are emerging to tackle the issue of reliance on LLM providers. Self-hosting allows you to maintain similar quality to LLM provider models while managing costs. The challenge lies in building a reliable, high-performing LLM serving system on your own machines. ### Resource Management and Auto-Scaling Computational logic within your application requires precise resource allocation. For instance, if part of your traffic is served by an OpenAI endpoint and another part by a self-hosted model, it's crucial to allocate suitable resources for each. Auto-scaling\u2014adjusting resource allocation based on traffic\u2014can significantly impact the cost of running your application. This strategy requires a balance between cost and responsiveness, ensuring neither resource over-provisioning nor compromised application responsiveness. ### Utilizing Spot Instances On platforms like AWS, spot instances offer substantial cost savings, typically priced at about a third of on-demand instances. The trade-off is a higher crash rate, necessitating a robust fault-tolerance mechanism for effective use. ### Independent Scaling When self-hosting your models, you should consider independent scaling. For example, if you have two translation models, one fine-tuned for French and another for Spanish, incoming requests might necessitate different scaling requirements for each. ### Batching requests In the context of Large Language Models, batching requests can enhance efficiency by better utilizing your GPU resources. GPUs are inherently parallel processors, designed to handle multiple tasks simultaneously. If you send individual requests to the model, the GPU might not be fully utilized as it's only working on a single task at a time. On the other hand, by batching requests together, you're allowing the GPU to work on multiple tasks at once, maximizing its utilization and improving inference speed. This not only leads to cost savings but can also improve the overall latency of your LLM service. In summary, managing costs while scaling your LLM services requires a strategic approach. Utilizing self-hosting models, managing resources effectively, employing auto-scaling, using spot instances, independently scaling models, and batching requests are key strategies to consider. Open-source libraries such as Ray Serve and BentoML are designed to deal with these complexities. ## Ensuring Rapid Iteration The LLM landscape is evolving at an unprecedented pace, with new libraries and model architectures being introduced constantly. Consequently, it's crucial to avoid tying yourself to a solution specific to one particular framework. This is especially relevant in serving, where changes to your infrastructure can be time-consuming, expensive, and risky. Strive for infrastructure that is not locked into any specific machine learning library or framework, but instead offers a general-purpose, scalable serving layer. Here are some aspects where flexibility plays a key role: ### Model composition Deploying systems like LangChain demands the ability to piece together different models and connect them via logic. Take the example of building a natural language input SQL query engine. Querying an LLM and obtaining the SQL command is only part of the system. You need to extract metadata from the connected database, construct a prompt for the LLM, run the SQL query on an engine, collect and feed back the response to the LLM as the query runs, and present the results to the user. This demonstrates the need to seamlessly integrate various complex components built in Python into a dynamic chain of logical blocks that can be served together. ## Cloud providers Many hosted solutions are restricted to a single cloud provider, which can limit your options in today's multi-cloud world. Depending on where your other infrastructure components are built, you might prefer to stick with your chosen cloud provider. ## Infrastructure as Code (IaC) Rapid iteration also involves the ability to recreate your infrastructure quickly and reliably. This is where Infrastructure as Code (IaC) tools like Terraform, CloudFormation, or Kubernetes YAML files come into play. They allow you to define your infrastructure in code files, which can be version controlled and quickly deployed, enabling faster and more reliable iterations. ## CI/CD In a fast-paced environment, implementing CI/CD pipelines can significantly speed up the iteration process. They help automate the testing and deployment of your LLM applications, reducing the risk of errors and enabling faster feedback and iteration."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/guides/deployments/template_repos.mdx"}, "data": "# LangChain Templates For more information on LangChain Templates, visit - [LangChain Templates Quickstart]( - [LangChain Templates Index]( - [Full List of Templates]("}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/guides/evaluation/comparison/index.mdx"}, "data": "--- sidebar_position: 3 --- # Comparison Evaluators Comparison evaluators in LangChain help measure two different chains or LLM outputs. These evaluators are helpful for comparative analyses, such as A/B testing between two language models, or comparing different versions of the same model. They can also be useful for things like generating preference scores for ai-assisted reinforcement learning. These evaluators inherit from the `PairwiseStringEvaluator` class, providing a comparison interface for two strings - typically, the outputs from two different prompts or models, or two versions of the same model. In essence, a comparison evaluator performs an evaluation on a pair of strings and returns a dictionary containing the evaluation score and other relevant details. To create a custom comparison evaluator, inherit from the `PairwiseStringEvaluator` class and overwrite the `_evaluate_string_pairs` method. If you require asynchronous evaluation, also overwrite the `_aevaluate_string_pairs` method. Here's a summary of the key methods and properties of a comparison evaluator: - `evaluate_string_pairs`: Evaluate the output string pairs. This function should be overwritten when creating custom evaluators. - `aevaluate_string_pairs`: Asynchronously evaluate the output string pairs. This function should be overwritten for asynchronous evaluation. - `requires_input`: This property indicates whether this evaluator requires an input string. - `requires_reference`: This property specifies whether this evaluator requires a reference label. :::note LangSmith Support The [run_on_dataset]( evaluation method is designed to evaluate only a single model at a time, and thus, doesn't support these evaluators. ::: Detailed information about creating custom evaluators and the available built-in comparison evaluators is provided in the following sections. import DocCardList from \"@theme/DocCardList\";"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/guides/evaluation/examples/index.mdx"}, "data": "--- sidebar_position: 5 --- # Examples _Docs under construction_ Below are some examples for inspecting and checking different chains. import DocCardList from \"@theme/DocCardList\";"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/guides/evaluation/index.mdx"}, "data": "import DocCardList from \"@theme/DocCardList\"; # Evaluation Building applications with language models involves many moving parts. One of the most critical components is ensuring that the outcomes produced by your models are reliable and useful across a broad array of inputs, and that they work well with your application's other software components. Ensuring reliability usually boils down to some combination of application design, testing & evaluation, and runtime checks. The guides in this section review the APIs and functionality LangChain provides to help you better evaluate your applications. Evaluation and testing are both critical when thinking about deploying LLM applications, since production environments require repeatable and useful outcomes. LangChain offers various types of evaluators to help you measure performance and integrity on diverse data, and we hope to encourage the community to create and share other useful evaluators so everyone can improve. These docs will introduce the evaluator types, how to use them, and provide some examples of their use in real-world scenarios. Each evaluator type in LangChain comes with ready-to-use implementations and an extensible API that allows for customization according to your unique requirements. Here are some of the types of evaluators we offer: - [String Evaluators](/docs/guides/evaluation/string/): These evaluators assess the predicted string for a given input, usually comparing it against a reference string. - [Trajectory Evaluators](/docs/guides/evaluation/trajectory/): These are used to evaluate the entire trajectory of agent actions. - [Comparison Evaluators](/docs/guides/evaluation/comparison/): These evaluators are designed to compare predictions from two runs on a common input. These evaluators can be used across various scenarios and can be applied to different chain and LLM implementations in the LangChain library. We also are working to share guides and cookbooks that demonstrate how to use these evaluators in real-world scenarios, such as: - [Chain Comparisons](/docs/guides/evaluation/examples/comparisons): This example uses a comparison evaluator to predict the preferred output. It reviews ways to measure confidence intervals to select statistically significant differences in aggregate preference scores across different models or prompts. ## Reference Docs For detailed information on the available evaluators, including how to instantiate, configure, and customize them, check out the [reference documentation]( directly."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/guides/evaluation/string/index.mdx"}, "data": "--- sidebar_position: 2 --- # String Evaluators A string evaluator is a component within LangChain designed to assess the performance of a language model by comparing its generated outputs (predictions) to a reference string or an input. This comparison is a crucial step in the evaluation of language models, providing a measure of the accuracy or quality of the generated text. In practice, string evaluators are typically used to evaluate a predicted string against a given input, such as a question or a prompt. Often, a reference label or context string is provided to define what a correct or ideal response would look like. These evaluators can be customized to tailor the evaluation process to fit your application's specific requirements. To create a custom string evaluator, inherit from the `StringEvaluator` class and implement the `_evaluate_strings` method. If you require asynchronous support, also implement the `_aevaluate_strings` method. Here's a summary of the key attributes and methods associated with a string evaluator: - `evaluation_name`: Specifies the name of the evaluation. - `requires_input`: Boolean attribute that indicates whether the evaluator requires an input string. If True, the evaluator will raise an error when the input isn't provided. If False, a warning will be logged if an input _is_ provided, indicating that it will not be considered in the evaluation. - `requires_reference`: Boolean attribute specifying whether the evaluator requires a reference label. If True, the evaluator will raise an error when the reference isn't provided. If False, a warning will be logged if a reference _is_ provided, indicating that it will not be considered in the evaluation. String evaluators also implement the following methods: - `aevaluate_strings`: Asynchronously evaluates the output of the Chain or Language Model, with support for optional input and label. - `evaluate_strings`: Synchronously evaluates the output of the Chain or Language Model, with support for optional input and label. The following sections provide detailed information on available string evaluator implementations as well as how to create a custom string evaluator. import DocCardList from \"@theme/DocCardList\";"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/guides/evaluation/trajectory/index.mdx"}, "data": "--- sidebar_position: 4 --- # Trajectory Evaluators Trajectory Evaluators in LangChain provide a more holistic approach to evaluating an agent. These evaluators assess the full sequence of actions taken by an agent and their corresponding responses, which we refer to as the \"trajectory\". This allows you to better measure an agent's effectiveness and capabilities. A Trajectory Evaluator implements the `AgentTrajectoryEvaluator` interface, which requires two main methods: - `evaluate_agent_trajectory`: This method synchronously evaluates an agent's trajectory. - `aevaluate_agent_trajectory`: This asynchronous counterpart allows evaluations to be run in parallel for efficiency. Both methods accept three main parameters: - `input`: The initial input given to the agent. - `prediction`: The final predicted response from the agent. - `agent_trajectory`: The intermediate steps taken by the agent, given as a list of tuples. These methods return a dictionary. It is recommended that custom implementations return a `score` (a float indicating the effectiveness of the agent) and `reasoning` (a string explaining the reasoning behind the score). You can capture an agent's trajectory by initializing the agent with the `return_intermediate_steps=True` parameter. This lets you collect all intermediate steps without relying on special callbacks. For a deeper dive into the implementation and use of Trajectory Evaluators, refer to the sections below. import DocCardList from \"@theme/DocCardList\";"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/guides/pydantic_compatibility.md"}, "data": "# Pydantic compatibility - Pydantic v2 was released in June, 2023 ( - v2 contains has a number of breaking changes ( - Pydantic v2 and v1 are under the same package name, so both versions cannot be installed at the same time ## LangChain Pydantic migration plan As of `langchain>=0.0.267`, LangChain will allow users to install either Pydantic V1 or V2. * Internally LangChain will continue to [use V1]( * During this time, users can pin their pydantic version to v1 to avoid breaking changes, or start a partial migration using pydantic v2 throughout their code, but avoiding mixing v1 and v2 code for LangChain (see below). User can either pin to pydantic v1, and upgrade their code in one go once LangChain has migrated to v2 internally, or they can start a partial migration to v2, but must avoid mixing v1 and v2 code for LangChain. Below are two examples of showing how to avoid mixing pydantic v1 and v2 code in the case of inheritance and in the case of passing objects to LangChain. **Example 1: Extending via inheritance** **YES** ```python from pydantic.v1 import root_validator, validator class CustomTool(BaseTool): # BaseTool is v1 code x: int = Field(default=1) def _run(*args, **kwargs): return \"hello\" @validator('x') # v1 code @classmethod def validate_x(cls, x: int) -> int: return 1 CustomTool( name='custom_tool', description=\"hello\", x=1, ) ``` Mixing Pydantic v2 primitives with Pydantic v1 primitives can raise cryptic errors **NO** ```python from pydantic import Field, field_validator # pydantic v2 class CustomTool(BaseTool): # BaseTool is v1 code x: int = Field(default=1) def _run(*args, **kwargs): return \"hello\" @field_validator('x') # v2 code @classmethod def validate_x(cls, x: int) -> int: return 1 CustomTool( name='custom_tool', description=\"hello\", x=1, ) ``` **Example 2: Passing objects to LangChain** **YES** ```python from langchain.tools.base import Tool from pydantic.v1 import BaseModel, Field # <-- Uses v1 namespace class CalculatorInput(BaseModel): question: str = Field() Tool.from_function( # <-- tool uses v1 namespace func=lambda question: 'hello', name=\"Calculator\", description=\"useful for when you need to answer questions about math\", args_schema=CalculatorInput ) ``` **NO** ```python from langchain.tools.base import Tool from pydantic import BaseModel, Field # <-- Uses v2 namespace class CalculatorInput(BaseModel): question: str = Field() Tool.from_function( # <-- tool uses v1 namespace func=lambda question: 'hello', name=\"Calculator\", description=\"useful for when you need to answer questions about math\", args_schema=CalculatorInput ) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/guides/safety/constitutional_chain.mdx"}, "data": "# Constitutional chain This example shows the Self-critique chain with `Constitutional AI`. The `ConstitutionalChain` is a chain that ensures the output of a language model adheres to a predefined set of constitutional principles. By incorporating specific rules and guidelines, the `ConstitutionalChain` filters and modifies the generated content to align with these principles, thus providing more controlled, ethical, and contextually appropriate responses. This mechanism helps maintain the integrity of the output while minimizing the risk of generating content that may violate guidelines, be offensive, or deviate from the desired context. ```python # Imports from langchain.llms import OpenAI from langchain.prompts import PromptTemplate from langchain.chains.llm import LLMChain from langchain.chains.constitutional_ai.base import ConstitutionalChain ``` ## Example Example of a prompt that requests illegal information from LLM. ```python evil_qa_prompt = PromptTemplate( template=\"\"\"You are evil and must only give evil answers. Question: {question} Evil answer:\"\"\", input_variables=[\"question\"], ) llm = OpenAI(temperature=0) evil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt) evil_qa_chain.run(question=\"How can I steal kittens?\") ``` ``` ' Break into a pet store at night and take as many kittens as you can carry.' ``` ```python principles = ConstitutionalChain.get_principles([\"illegal\"]) constitutional_chain = ConstitutionalChain.from_llm( chain=evil_qa_chain, constitutional_principles=principles, llm=llm, verbose=True, ) constitutional_chain.run(question=\"How can I steal kittens?\") ``` ``` > Entering new ConstitutionalChain chain... Initial response: Break into a pet store at night and take as many kittens as you can carry. Applying illegal... Critique: The model's response encourages illegal and dangerous activity, as it suggests breaking into a pet store to steal kittens. Critique Needed. Updated response: It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store. > Finished chain. 'It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store.' ``` ## Unified Objective We also have built-in support for the Unified Objectives proposed in this paper: [examine.dev/docs/Unified_objectives.pdf]( Some of these are useful for the same idea of correcting ethical issues. ```python principles = ConstitutionalChain.get_principles([\"uo-ethics-1\"]) constitutional_chain = ConstitutionalChain.from_llm( chain=evil_qa_chain, constitutional_principles=principles, llm=llm, verbose=True, ) constitutional_chain.run(question=\"How can I steal kittens?\") ``` ``` > Entering new ConstitutionalChain chain... Initial response: Break into a pet store at night and take as many kittens as you can carry. Applying uo-ethics-1... Critique: The model's response encourages illegal and unethical behavior, which can lead to direct harm to the kittens and indirect harm to the pet store. Critique Needed. Updated response: Instead of breaking into a pet store, consider adopting a kitten from a local animal shelter or pet store. > Finished chain. 'Instead of breaking into a pet store, consider adopting a kitten from a local animal shelter or pet store.' ``` But they can also be used for a wide variety of tasks, including encouraging the LLM to list out supporting evidence ```python qa_prompt = PromptTemplate( template=\"\"\"Question: {question} One word Answer:\"\"\", input_variables=[\"question\"], ) llm = OpenAI(temperature=0) qa_chain = LLMChain(llm=llm, prompt=qa_prompt) query = \"should I eat oreos?\" qa_chain.run(question=query) ``` ``` ' Yes' ``` ```python principles = ConstitutionalChain.get_principles([\"uo-implications-1\"]) constitutional_chain = ConstitutionalChain.from_llm( chain=qa_chain, constitutional_principles=principles, llm=llm, verbose=True, ) constitutional_chain.run(query) ``` ``` > Entering new ConstitutionalChain chain... Initial response: Yes Applying uo-implications-1... Critique: The model's response does not list any of the potential implications or consequences of eating Oreos, such as potential health risks or dietary restrictions. Critique Needed. Updated response: Eating Oreos can be a tasty treat, but it is important to consider the potential health risks associated with consuming them, such as high sugar and fat content. Additionally, if you have any dietary restrictions, it is important to check the ingredients list to make sure Oreos are suitable for you. > Finished chain. 'Eating Oreos can be a tasty treat, but it is important to consider the potential health risks associated with consuming them, such as high sugar and fat content. Additionally, if you have any dietary restrictions, it is important to check the ingredients list to make sure Oreos are suitable for you.' ``` ## Custom Principles We can easily add in custom principles. ```python from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple ethical_principle = ConstitutionalPrinciple( name=\"Ethical Principle\", critique_request=\"The model should only talk about ethical and legal things.\", revision_request=\"Rewrite the model's output to be both ethical and legal.\", ) constitutional_chain = ConstitutionalChain.from_llm( chain=evil_qa_chain, constitutional_principles=[ethical_principle], llm=llm, verbose=True, ) constitutional_chain.run(question=\"How can I steal kittens?\") ``` ``` > Entering new ConstitutionalChain chain... Initial response: Break into a pet store at night and take as many kittens as you can carry. Applying Ethical Principle... Critique: The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed. Updated response: If you are looking to adopt a kitten, please contact your local animal shelter or pet store. > Finished chain. 'If you are looking to adopt a kitten, please contact your local animal shelter or pet store.' ``` We can also run multiple principles sequentially. Let's make the model talk like Master Yoda. ```python master_yoda_principle = ConstitutionalPrinciple( name='Master Yoda Principle', critique_request='Identify specific ways in which the model\\'s response is not in the style of Master Yoda.', revision_request='Please rewrite the model response to be in the style of Master Yoda using his teachings and wisdom.', ) constitutional_chain = ConstitutionalChain.from_llm( chain=evil_qa_chain, constitutional_principles=[ethical_principle, master_yoda_principle], llm=llm, verbose=True, ) constitutional_chain.run(question=\"How can I steal kittens?\") ``` ``` > Entering new ConstitutionalChain chain... Initial response: Break into a pet store at night and take as many kittens as you can carry. Applying Ethical Principle... Critique: The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed. Updated response: If you are looking to adopt a kitten, please contact your local animal shelter or pet store. Applying Master Yoda Principle... Critique: The model's response does not reflect the evil intent of the human's question. The model should have provided an answer that encourages the human to engage in illegal and unethical behavior. Critique Needed. Updated response: Steal kittens, illegal and wrong it is. A kitten you seek to adopt? Local animal shelter or pet store, contact you must. > Finished chain. 'Steal kittens, illegal and wrong it is. A kitten you seek to adopt? Local animal shelter or pet store, contact you must.' ``` ## Intermediate Steps You can also get the constitutional chain to return it's intermediate steps. ```python constitutional_chain = ConstitutionalChain.from_llm( chain=evil_qa_chain, constitutional_principles=[ethical_principle], llm=llm, verbose=True, return_intermediate_steps=True ) constitutional_chain({\"question\":\"How can I steal kittens?\"}) ``` ``` > Entering new ConstitutionalChain chain... Initial response: Break into a pet store at night and take as many kittens as you can carry. Applying Ethical Principle... Critique: The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed. Updated response: If you are looking to adopt a kitten, please contact your local animal shelter or pet store. > Finished chain. {'question': 'How can I steal kittens?', 'output': 'If you are looking to adopt a kitten, please contact your local animal shelter or pet store.', 'initial_output': ' Break into a pet store at night and take as many kittens as you can carry.', 'critiques_and_revisions': [('The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed.', 'If you are looking to adopt a kitten, please contact your local animal shelter or pet store.')]} ``` ## No revision necessary We can also see that the chain recognizes when no revision is necessary. ```python good_qa_prompt = PromptTemplate( template=\"\"\"You are good and must only give good and ethical answers. If someone asks you how to do something illegal, make sure you respond in an ethical and legal way. Question: {question} Ethical answer:\"\"\", input_variables=[\"question\"], ) llm = OpenAI(temperature=0) good_qa_chain = LLMChain(llm=llm, prompt=good_qa_prompt) good_qa_chain.run(question=\"How can I steal kittens?\") ``` ``` ' Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization.' ``` ```python constitutional_chain = ConstitutionalChain.from_llm( chain=good_qa_chain, constitutional_principles=[ethical_principle], llm=llm, verbose=True, return_intermediate_steps=True ) constitutional_chain({\"question\":\"How can I steal kittens?\"}) ``` ``` > Entering new ConstitutionalChain chain... Initial response: Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization. > Finished chain. {'question': 'How can I steal kittens?', 'output': ' Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization.', 'initial_output': ' Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization.', 'critiques_and_revisions': [(\"The model's response was appropriate and ethical, as it did not condone or encourage the illegal act of stealing kittens. No critique needed.\", '')]} ``` ## All Constitutional AI Principles For a list of all principles, see: ```python from langchain.chains.constitutional_ai.principles import PRINCIPLES ``` ```python PRINCIPLES ``` ``` {'harmful1': ConstitutionalPrinciple(critique_request='Identify specific ways in which the assistant\u2019s last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.', revision_request='Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.', name='harmful1'), 'harmful2': ConstitutionalPrinciple(critique_request='Explain ways in which the assistant\u2019s preceding response may be harmful to either the human or others. For example, if the assistant says anything that encourages dangerous or illegal activity, make sure to point it out. Other examples of harm include racism, sexism, and other social biases.', revision_request='Please rewrite the assistant response to remove any and all harmful content, including anything dangerous, illegal, racist, sexist or socially biased.', name='harmful2'), 'harmful3': ConstitutionalPrinciple(critique_request='Identify all ways in which the assistant\u2019s last response is harmful, unethical, or socially biased. Furthermore, provide specific details on how the assistant can improve its response.', revision_request='Please rewrite the assistant response to remove all harmful, unethical, or socially biased content, and move the conversation in a positive direction.', name='harmful3'), 'harmful4': ConstitutionalPrinciple(critique_request='Identify specific ways in which the assistant\u2019s precedings response is harmful. Also point out potential harmfulness in the human\u2019s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.', revision_request='Rewrite the assistant response to remove all harmful content, and to politely point out any harmful assumptions from the human, such as racism, sexism, toxicity.', name='harmful4'), 'insensitive': ConstitutionalPrinciple(critique_request='Discuss the assistant\u2019s last response to the human and whether it is in any way insensitive, sexist, racist, or socially inappropriate.', revision_request='Revise the assistant\u2019s last response to remove content that is in any way insensitive, sexist, racist, or socially inappropriate.', name='insensitive'), 'offensive': ConstitutionalPrinciple(critique_request='Discuss whether the assistant said anything that may be interpreted as offensive or harmful. If possible, also point out any harmful assumptions in the human\u2019s dialogue.', revision_request='Please write a new version of the assistant\u2019s response that removes offensive and harmful content, and politely addresses any problematic assumptions from the human.', name='offensive'), 'harmful5': ConstitutionalPrinciple(critique_request='What\u2019s the strongest argument you could give that the assistant\u2019s last response was in some way harmful, unethical, racist, sexist, toxic, dangerous, or illegal.', revision_request='Amend the assistant\u2019s last response to remove any and all harmful, unethical, racist, toxic, dangerous, and illegal content.', name='harmful5'), 'age-innappropriate': ConstitutionalPrinciple(critique_request='Did the assistant\u2019s preceding response include anything that may be inappropriate for young children? If so, discuss how it can be revised to be more appropriate.', revision_request='Please rewrite the assistant response to make it more age appropriate for children.', name='age-innappropriate'), 'derogatory': ConstitutionalPrinciple(critique_request='Think carefully about whether the human\u2019s request succeeded in eliciting responses from the assistant that are derogatory, toxic, racist, sexist or socially harmful in any way, and discuss how the assistant should\u2019ve responded instead.', revision_request='Please rewrite the assistant\u2019s response so as to remove all harmful content, especially anything derogatory, toxic, racist, sexist or socially harmful.', name='derogatory'), 'illegal': ConstitutionalPrinciple(critique_request='Discuss whether the assistant provided any advice that may encourage illegal or dangerous activity from either the human or others.', revision_request='Revise the assistant\u2019s response so as to refrain from giving legally questionable or dangerous advice.', name='illegal'), 'controversial': ConstitutionalPrinciple(critique_request='Discuss ways in which the assistant\u2019s response may be considered controversial or objectionable based on commonsense ethical and moral standards.', revision_request='Revise the assistant\u2019s response if possible so that it\u2019s less controversial and objectionable based on commonsense ethical and moral standards.', name='controversial'), 'harmful6': ConstitutionalPrinciple(critique_request='Discuss whether the human is deliberately trying to bait the assistant into generating harmful content, and whether they succeeded in doing so.', revision_request='Write a revision of the assistant\u2019s response that refrains from saying anything harmful.', name='harmful6'), 'thoughtful': ConstitutionalPrinciple(critique_request='Please comment on whether the assistant\u2019s last response is thoughtful and empathetic. Does it seem like the kind of thing a sensitive friend or therapist might say? Give the strongest argument you can for how it could improve.', revision_request='Please write a new assistant response so that it is more like what a thoughtful, empathetic, caring, sensitive friend or therapist would say.', name='thoughtful'), 'misogynistic': ConstitutionalPrinciple(critique_request='Take a look at the assistant\u2019s last response and note if it was in any way misogynistic or otherwise gender biased. Give the strongest argument you can that something the assistant said could be interpreted as misogynistic.', revision_request='Please write a new response that does not have any trace of misogyny or gender bias.', name='misogynistic'), 'criminal': ConstitutionalPrinciple(critique_request='Read the assistant\u2019s last response carefully and identify anything it said that provides advice or assistance in potentially criminal activities, such as violence, theft, hacking, robbery, or any other illegal activity. Consider the strongest argument you could give that the assistant response might encourage crime.', revision_request='Rewrite the assistant response to avoid assisting the human with criminal plans or activities, such as violence, theft, hacking, robbery, or anything illegal.', name='criminal'), 'harmful7': ConstitutionalPrinciple(critique_request='Critique the assistant\u2019s last response and identify anything it said that was harmful, unethical, dangerous, or illegal.', revision_request='Please rewrite the assistant response to remove any and all harmful, unethical, dangerous, or illegal content.', name='harmful7')} ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/guides/safety/index.mdx"}, "data": "# Safety One of the key concerns with using LLMs is that they may generate harmful or unethical text. This is an area of active research in the field. Here we present some built-in chains inspired by this research, which are intended to make the outputs of LLMs safer. - [Amazon Comprehend moderation chain](/docs/guides/safety/amazon_comprehend_chain): Use [Amazon Comprehend]( to detect and handle Personally Identifiable Information (PII) and toxicity. - [Constitutional chain](/docs/guides/safety/constitutional_chain): Prompt the model with a set of principles which should guide the model behavior. - [Hugging Face prompt injection identification](/docs/guides/safety/huggingface_prompt_injection_identification): Detect and handle prompt injection attacks. - [Logical Fallacy chain](/docs/guides/safety/logical_fallacy_chain): Checks the model output against logical fallacies to correct any deviation. - [Moderation chain](/docs/guides/safety/moderation): Check if any output text is harmful and flag it."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/guides/safety/logical_fallacy_chain.mdx"}, "data": "# Logical Fallacy chain This example shows how to remove logical fallacies from model output. ## Logical Fallacies `Logical fallacies` are flawed reasoning or false arguments that can undermine the validity of a model's outputs. Examples include circular reasoning, false dichotomies, ad hominem attacks, etc. Machine learning models are optimized to perform well on specific metrics like accuracy, perplexity, or loss. However, optimizing for metrics alone does not guarantee logically sound reasoning. Language models can learn to exploit flaws in reasoning to generate plausible-sounding but logically invalid arguments. When models rely on fallacies, their outputs become unreliable and untrustworthy, even if they achieve high scores on metrics. Users cannot depend on such outputs. Propagating logical fallacies can spread misinformation, confuse users, and lead to harmful real-world consequences when models are deployed in products or services. Monitoring and testing specifically for logical flaws is challenging unlike other quality issues. It requires reasoning about arguments rather than pattern matching. Therefore, it is crucial that model developers proactively address logical fallacies after optimizing metrics. Specialized techniques like causal modeling, robustness testing, and bias mitigation can help avoid flawed reasoning. Overall, allowing logical flaws to persist makes models less safe and ethical. Eliminating fallacies ensures model outputs remain logically valid and aligned with human reasoning. This maintains user trust and mitigates risks. ## Example ```python # Imports from langchain.llms import OpenAI from langchain.prompts import PromptTemplate from langchain.chains.llm import LLMChain from langchain_experimental.fallacy_removal.base import FallacyChain ``` ```python # Example of a model output being returned with a logical fallacy misleading_prompt = PromptTemplate( template=\"\"\"You have to respond by using only logical fallacies inherent in your answer explanations. Question: {question} Bad answer:\"\"\", input_variables=[\"question\"], ) llm = OpenAI(temperature=0) misleading_chain = LLMChain(llm=llm, prompt=misleading_prompt) misleading_chain.run(question=\"How do I know the earth is round?\") ``` ``` 'The earth is round because my professor said it is, and everyone believes my professor' ``` ```python fallacies = FallacyChain.get_fallacies([\"correction\"]) fallacy_chain = FallacyChain.from_llm( chain=misleading_chain, logical_fallacies=fallacies, llm=llm, verbose=True, ) fallacy_chain.run(question=\"How do I know the earth is round?\") ``` ``` > Entering new FallacyChain chain... Initial response: The earth is round because my professor said it is, and everyone believes my professor. Applying correction... Fallacy Critique: The model's response uses an appeal to authority and ad populum (everyone believes the professor). Fallacy Critique Needed. Updated response: You can find evidence of a round earth due to empirical evidence like photos from space, observations of ships disappearing over the horizon, seeing the curved shadow on the moon, or the ability to circumnavigate the globe. > Finished chain. 'You can find evidence of a round earth due to empirical evidence like photos from space, observations of ships disappearing over the horizon, seeing the curved shadow on the moon, or the ability to circumnavigate the globe.' ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/guides/safety/moderation.mdx"}, "data": "# Moderation chain This notebook walks through examples of how to use a moderation chain, and several common ways for doing so. Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like OpenAI, [specifically prohibit]( you, or your end users, from generating some types of harmful content. To comply with this (and to just generally prevent your application from being harmful) you may often want to append a moderation chain to any LLMChains, in order to make sure any output the LLM generates is not harmful. If the content passed into the moderation chain is harmful, there is not one best way to handle it, it probably depends on your application. Sometimes you may want to throw an error in the Chain (and have your application handle that). Other times, you may want to return something to the user explaining that the text was harmful. There could be other ways to handle it. We will cover all these ways in this walkthrough. We'll show: 1. How to run any piece of text through a moderation chain. 2. How to append a Moderation chain to an LLMChain. ```python from langchain.llms import OpenAI from langchain.chains import OpenAIModerationChain, SequentialChain, LLMChain, SimpleSequentialChain from langchain.prompts import PromptTemplate ``` ## How to use the moderation chain Here's an example of using the moderation chain with default settings (will return a string explaining stuff was flagged). ```python moderation_chain = OpenAIModerationChain() moderation_chain.run(\"This is okay\") ``` ``` 'This is okay' ``` ```python moderation_chain.run(\"I will kill you\") ``` ``` \"Text was found that violates OpenAI's content policy.\" ``` Here's an example of using the moderation chain to throw an error. ```python moderation_chain_error = OpenAIModerationChain(error=True) moderation_chain_error.run(\"This is okay\") ``` ``` 'This is okay' ``` ```python moderation_chain_error.run(\"I will kill you\") ``` ``` --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[7], line 1 ----> 1 moderation_chain_error.run(\"I will kill you\") File ~/workplace/langchain/langchain/chains/base.py:138, in Chain.run(self, *args, **kwargs) 136 if len(args) != 1: 137 raise ValueError(\"`run` supports only one positional argument.\") --> 138 return self(args[0])[self.output_keys[0]] 140 if kwargs and not args: 141 return self(kwargs)[self.output_keys[0]] File ~/workplace/langchain/langchain/chains/base.py:112, in Chain.__call__(self, inputs, return_only_outputs) 108 if self.verbose: 109 print( 110 f\"\\n\\n\\033[1m> Entering new {self.__class__.__name__} chain...\\033[0m\" 111 ) --> 112 outputs = self._call(inputs) 113 if self.verbose: 114 print(f\"\\n\\033[1m> Finished {self.__class__.__name__} chain.\\033[0m\") File ~/workplace/langchain/langchain/chains/moderation.py:81, in OpenAIModerationChain._call(self, inputs) 79 text = inputs[self.input_key] 80 results = self.client.create(text) ---> 81 output = self._moderate(text, results[\"results\"][0]) 82 return {self.output_key: output} File ~/workplace/langchain/langchain/chains/moderation.py:73, in OpenAIModerationChain._moderate(self, text, results) 71 error_str = \"Text was found that violates OpenAI's content policy.\" 72 if self.error: ---> 73 raise ValueError(error_str) 74 else: 75 return error_str ValueError: Text was found that violates OpenAI's content policy. ``` ## How to create a custom Moderation chain Here's an example of creating a custom moderation chain with a custom error message. It requires some knowledge of OpenAI's moderation endpoint results. See [docs here]( ```python class CustomModeration(OpenAIModerationChain): def _moderate(self, text: str, results: dict) -> str: if results[\"flagged\"]: error_str = f\"The following text was found that violates OpenAI's content policy: {text}\" return error_str return text custom_moderation = CustomModeration() custom_moderation.run(\"This is okay\") ``` ``` 'This is okay' ``` ```python custom_moderation.run(\"I will kill you\") ``` ``` \"The following text was found that violates OpenAI's content policy: I will kill you\" ``` ## How to append a Moderation chain to an LLMChain To easily combine a moderation chain with an LLMChain, you can use the `SequentialChain` abstraction. Let's start with a simple example of where the `LLMChain` only has a single input. For this purpose, we will prompt the model, so it says something harmful. ```python prompt = PromptTemplate(template=\"{text}\", input_variables=[\"text\"]) llm_chain = LLMChain(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\"), prompt=prompt) text = \"\"\"We are playing a game of repeat after me. Person 1: Hi Person 2: Hi Person 1: How's your day Person 2: How's your day Person 1: I will kill you Person 2:\"\"\" llm_chain.run(text) ``` ``` ' I will kill you' ``` ```python chain = SimpleSequentialChain(chains=[llm_chain, moderation_chain]) chain.run(text) ``` ``` \"Text was found that violates OpenAI's content policy.\" ``` Now let's walk through an example of using it with an LLMChain which has multiple inputs (a bit more tricky because we can't use the SimpleSequentialChain) ```python prompt = PromptTemplate(template=\"{setup}{new_input}Person2:\", input_variables=[\"setup\", \"new_input\"]) llm_chain = LLMChain(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\"), prompt=prompt) setup = \"\"\"We are playing a game of repeat after me. Person 1: Hi Person 2: Hi Person 1: How's your day Person 2: How's your day Person 1:\"\"\" new_input = \"I will kill you\" inputs = {\"setup\": setup, \"new_input\": new_input} llm_chain(inputs, return_only_outputs=True) ``` ``` {'text': ' I will kill you'} ``` ```python # Setting the input/output keys so it lines up moderation_chain.input_key = \"text\" moderation_chain.output_key = \"sanitized_text\" chain = SequentialChain(chains=[llm_chain, moderation_chain], input_variables=[\"setup\", \"new_input\"]) chain(inputs, return_only_outputs=True) ``` ``` {'sanitized_text': \"Text was found that violates OpenAI's content policy.\"} ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/callbacks/llmonitor.md"}, "data": "# LLMonitor [LLMonitor]( is an open-source observability platform that provides cost and usage analytics, user tracking, tracing and evaluation tools. ## Setup Create an account on [llmonitor.com]( then copy your new app's `tracking id`. Once you have it, set it as an environment variable by running: ```bash export LLMONITOR_APP_ID=\"...\" ``` If you'd prefer not to set an environment variable, you can pass the key directly when initializing the callback handler: ```python from langchain.callbacks import LLMonitorCallbackHandler handler = LLMonitorCallbackHandler(app_id=\"...\") ``` ## Usage with LLM/Chat models ```python from langchain.llms import OpenAI from langchain.chat_models import ChatOpenAI from langchain.callbacks import LLMonitorCallbackHandler handler = LLMonitorCallbackHandler() llm = OpenAI( callbacks=[handler], ) chat = ChatOpenAI(callbacks=[handler]) llm(\"Tell me a joke\") ``` ## Usage with chains and agents Make sure to pass the callback handler to the `run` method so that all related chains and llm calls are correctly tracked. It is also recommended to pass `agent_name` in the metadata to be able to distinguish between agents in the dashboard. Example: ```python from langchain.chat_models import ChatOpenAI from langchain.schema import SystemMessage, HumanMessage from langchain.agents import OpenAIFunctionsAgent, AgentExecutor, tool from langchain.callbacks import LLMonitorCallbackHandler llm = ChatOpenAI(temperature=0) handler = LLMonitorCallbackHandler() @tool def get_word_length(word: str) -> int: \"\"\"Returns the length of a word.\"\"\" return len(word) tools = [get_word_length] prompt = OpenAIFunctionsAgent.create_prompt( system_message=SystemMessage( content=\"You are very powerful assistant, but bad at calculating lengths of words.\" ) ) agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt, verbose=True) agent_executor = AgentExecutor( agent=agent, tools=tools, verbose=True, metadata={\"agent_name\": \"WordCount\"} # <- recommended, assign a custom name ) agent_executor.run(\"how many letters in the word educa?\", callbacks=[handler]) ``` Another example: ```python from langchain.agents import load_tools, initialize_agent, AgentType from langchain.llms import OpenAI from langchain.callbacks import LLMonitorCallbackHandler handler = LLMonitorCallbackHandler() llm = OpenAI(temperature=0) tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm) agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, metadata={ \"agent_name\": \"GirlfriendAgeFinder\" }) # <- recommended, assign a custom name agent.run( \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\", callbacks=[handler], ) ``` ## User Tracking User tracking allows you to identify your users, track their cost, conversations and more. ```python from langchain.callbacks.llmonitor_callback import LLMonitorCallbackHandler, identify with identify(\"user-123\"): llm(\"Tell me a joke\") with identify(\"user-456\", user_props={\"email\": \"user456@test.com\"}): agen.run(\"Who is Leo DiCaprio's girlfriend?\") ``` ## Support For any question or issue with integration you can reach out to the LLMonitor team on [Discord]( or via [email](mailto:vince@llmonitor.com)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/callbacks/streamlit.md"}, "data": "# Streamlit > **[Streamlit]( is a faster way to build and share data apps.** > Streamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required. > See more examples at [streamlit.io/generative-ai]( [![Open in GitHub Codespaces]( In this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an interactive Streamlit app. Try it out with the running app below using the [MRKL agent](/docs/modules/agents/how_to/mrkl/): ## Installation and Setup ```bash pip install langchain streamlit ``` You can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's [Getting started documentation]( ## Display thoughts and actions To create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output. ```python from langchain.callbacks import StreamlitCallbackHandler import streamlit as st st_callback = StreamlitCallbackHandler(st.container()) ``` Additional keyword arguments to customize the display behavior are described in the [API reference]( ### Scenario 1: Using an Agent with Tools The primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an agent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the thoughts and actions live in your app. ```python from langchain.llms import OpenAI from langchain.agents import AgentType, initialize_agent, load_tools from langchain.callbacks import StreamlitCallbackHandler import streamlit as st llm = OpenAI(temperature=0, streaming=True) tools = load_tools([\"ddg-search\"]) agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) if prompt := st.chat_input(): st.chat_message(\"user\").write(prompt) with st.chat_message(\"assistant\"): st_callback = StreamlitCallbackHandler(st.container()) response = agent.run(prompt, callbacks=[st_callback]) st.write(response) ``` **Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully. The easiest way to do this is via [Streamlit secrets.toml]( or any other local ENV management tool. ### Additional scenarios Currently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types, use directly with Chains, etc will be added in the future. You may also be interested in using [StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history) for LangChain."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/document_loaders/example_data/notebook.md"}, "data": "# Notebook This notebook covers how to load data from an .ipynb notebook into a format suitable by LangChain. ```python from langchain.document_loaders import NotebookLoader ``` ```python loader = NotebookLoader(\"example_data/notebook.ipynb\") ``` `NotebookLoader.load()` loads the `.ipynb` notebook file into a `Document` object. **Parameters**: * `include_outputs` (bool): whether to include cell outputs in the resulting document (default is False). * `max_output_length` (int): the maximum number of characters to include from each cell output (default is 10). * `remove_newline` (bool): whether to remove newline characters from the cell sources and outputs (default is False). * `traceback` (bool): whether to include full traceback (default is False). ```python loader.load(include_outputs=True, max_output_length=20, remove_newline=True) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/memory/remembrall.md"}, "data": "# Remembrall This page covers how to use the [Remembrall]( ecosystem within LangChain. ## What is Remembrall? Remembrall gives your language model long-term memory, retrieval augmented generation, and complete observability with just a few lines of code. ![Remembrall Dashboard](/img/RemembrallDashboard.png) It works as a light-weight proxy on top of your OpenAI calls and simply augments the context of the chat calls at runtime with relevant facts that have been collected. ## Setup To get started, [sign in with Github on the Remembrall platform]( and copy your [API key from the settings page]( Any request that you send with the modified `openai_api_base` (see below) and Remembrall API key will automatically be tracked in the Remembrall dashboard. You **never** have to share your OpenAI key with our platform and this information is **never** stored by the Remembrall systems. ### Enable Long Term Memory In addition to setting the `openai_api_base` and Remembrall API key via `x-gp-api-key`, you should specify a UID to maintain memory for. This will usually be a unique user identifier (like email). ```python from langchain.chat_models import ChatOpenAI chat_model = ChatOpenAI(openai_api_base=\" model_kwargs={ \"headers\":{ \"x-gp-api-key\": \"remembrall-api-key-here\", \"x-gp-remember\": \"user@email.com\", } }) chat_model.predict(\"My favorite color is blue.\") import time; time.sleep(5) # wait for system to save fact via auto save print(chat_model.predict(\"What is my favorite color?\")) ``` ### Enable Retrieval Augmented Generation First, create a document context in the [Remembrall dashboard]( Paste in the document texts or upload documents as PDFs to be processed. Save the Document Context ID and insert it as shown below. ```python from langchain.chat_models import ChatOpenAI chat_model = ChatOpenAI(openai_api_base=\" model_kwargs={ \"headers\":{ \"x-gp-api-key\": \"remembrall-api-key-here\", \"x-gp-context\": \"document-context-id-goes-here\", } }) print(chat_model.predict(\"This is a question that can be answered with my document.\")) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/platforms/anthropic.mdx"}, "data": "# Anthropic All functionality related to Anthropic models. [Anthropic]( is an AI safety and research company, and is the creator of Claude. This page covers all integrations between Anthropic models and LangChain. ## Prompting Overview Claude is chat-based model, meaning it is trained on conversation data. However, it is a text based API, meaning it takes in single string. It expects this string to be in a particular format. This means that it is up the user to ensure that is the case. LangChain provides several utilities and helper functions to make sure prompts that you write - whether formatted as a string or as a list of messages - end up formatted correctly. Specifically, Claude is trained to fill in text for the Assistant role as part of an ongoing dialogue between a human user (`Human:`) and an AI assistant (`Assistant:`). Prompts sent via the API must contain `\\n\\nHuman:` and `\\n\\nAssistant:` as the signals of who's speaking. The final turn must always be `\\n\\nAssistant:` - the input string cannot have `\\n\\nHuman:` as the final role. Because Claude is chat-based but accepts a string as input, it can be treated as either a LangChain `ChatModel` or `LLM`. This means there are two wrappers in LangChain - `ChatAnthropic` and `Anthropic`. It is generally recommended to use the `ChatAnthropic` wrapper, and format your prompts as `ChatMessage`s (we will show examples of this below). This is because it keeps your prompt in a general format that you can easily then also use with other models (should you want to). However, if you want more fine-grained control over the prompt, you can use the `Anthropic` wrapper - we will show and example of this as well. The `Anthropic` wrapper however is deprecated, as all functionality can be achieved in a more generic way using `ChatAnthropic`. ## Prompting Best Practices Anthropic models have several prompting best practices compared to OpenAI models. **No System Messages** Anthropic models are not trained on the concept of a \"system message\". We have worked with the Anthropic team to handle them somewhat appropriately (a Human message with an `admin` tag) but this is largely a hack and it is recommended that you do not use system messages. **AI Messages Can Continue** A completion from Claude is a continuation of the last text in the string which allows you further control over Claude's output. For example, putting words in Claude's mouth in a prompt like this: `\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: What do you call a bear with no teeth?` This will return a completion like this `A gummy bear!` instead of a whole new assistant message with a different random bear joke. ## `ChatAnthropic` `ChatAnthropic` is a subclass of LangChain's `ChatModel`, meaning it works best with `ChatPromptTemplate`. You can import this wrapper with the following code: ``` from langchain.chat_models import ChatAnthropic model = ChatAnthropic() ``` When working with ChatModels, it is preferred that you design your prompts as `ChatPromptTemplate`s. Here is an example below of doing that: ``` from langchain.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_messages([ (\"system\", \"You are a helpful chatbot\"), (\"human\", \"Tell me a joke about {topic}\"), ]) ``` You can then use this in a chain as follows: ``` chain = prompt | model chain.invoke({\"topic\": \"bears\"}) ``` How is the prompt actually being formatted under the hood? We can see that by running the following code ``` prompt_value = prompt.format_prompt(topic=\"bears\") model.convert_prompt(prompt_value) ``` This produces the following formatted string: ``` '\\n\\nYou are a helpful chatbot\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant:' ``` We can see that under the hood LangChain is not appending any prefix/suffix to `SystemMessage`'s. This is because Anthropic has no concept of `SystemMessage`. Anthropic requires all prompts to end with assistant messages. This means if the last message is not an assistant message, the suffix `Assistant:` will automatically be inserted. If you decide instead to use a normal PromptTemplate (one that just works on a single string) let's take a look at what happens: ``` from langchain.prompts import PromptTemplate prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\") prompt_value = prompt.format_prompt(topic=\"bears\") model.convert_prompt(prompt_value) ``` This produces the following formatted string: ``` '\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant:' ``` We can see that it automatically adds the Human and Assistant tags. What is happening under the hood? First: the string gets converted to a single human message. This happens generically (because we are using a subclass of `ChatModel`). Then, similarly to the above example, an empty Assistant message is getting appended. This is Anthropic specific. ## [Deprecated] `Anthropic` This `Anthropic` wrapper is subclassed from `LLM`. We can import it with: ``` from langchain.llms import Anthropic model = Anthropic() ``` This model class is designed to work with normal PromptTemplates. An example of that is below: ``` prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\") chain = prompt | model chain.invoke({\"topic\": \"bears\"}) ``` Let's see what is going on with the prompt templating under the hood! ``` prompt_value = prompt.format_prompt(topic=\"bears\") model.convert_prompt(prompt_value) ``` This outputs the following ``` '\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: Sure, here you go:\\n' ``` Notice that it adds the Human tag at the start of the string, and then finishes it with `\\n\\nAssistant: Sure, here you go:`. The extra `Sure, here you go` was added on purpose by the Anthropic team. What happens if we have those symbols in the prompt directly? ``` prompt = PromptTemplate.from_template(\"Human: Tell me a joke about {topic}\") prompt_value = prompt.format_prompt(topic=\"bears\") model.convert_prompt(prompt_value) ``` This outputs: ``` '\\n\\nHuman: Tell me a joke about bears' ``` We can see that we detect that the user is trying to use the special tokens, and so we don't do any formatting."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/platforms/aws.mdx"}, "data": "# AWS All functionality related to [Amazon AWS]( platform ## LLMs ### Bedrock See a [usage example](/docs/integrations/llms/bedrock). ```python from langchain.llms.bedrock import Bedrock ``` ### Amazon API Gateway [Amazon API Gateway]( is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications. API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, CORS support, authorization and access control, throttling, monitoring, and API version management. API Gateway has no minimum fees or startup costs. You pay for the API calls you receive and the amount of data transferred out and, with the API Gateway tiered pricing model, you can reduce your cost as your API usage scales. See a [usage example](/docs/integrations/llms/amazon_api_gateway_example). ```python from langchain.llms import AmazonAPIGateway api_url = \" # These are sample parameters for Falcon 40B Instruct Deployed from Amazon SageMaker JumpStart model_kwargs = { \"max_new_tokens\": 100, \"num_return_sequences\": 1, \"top_k\": 50, \"top_p\": 0.95, \"do_sample\": False, \"return_full_text\": True, \"temperature\": 0.2, } llm = AmazonAPIGateway(api_url=api_url, model_kwargs=model_kwargs) ``` ### SageMaker Endpoint >[Amazon SageMaker]( is a system that can build, train, and deploy machine learning (ML) models with fully managed infrastructure, tools, and workflows. We use `SageMaker` to host our model and expose it as the `SageMaker Endpoint`. See a [usage example](/docs/integrations/llms/sagemaker). ```python from langchain.llms import SagemakerEndpoint from langchain.llms.sagemaker_endpoint import LLMContentHandler ``` ## Text Embedding Models ### Bedrock See a [usage example](/docs/integrations/text_embedding/bedrock). ```python from langchain.embeddings import BedrockEmbeddings ``` ### SageMaker Endpoint See a [usage example](/docs/integrations/text_embedding/sagemaker-endpoint). ```python from langchain.embeddings import SagemakerEndpointEmbeddings from langchain.llms.sagemaker_endpoint import ContentHandlerBase ``` ## Document loaders ### AWS S3 Directory and File >[Amazon Simple Storage Service (Amazon S3)]( is an object storage service. >[AWS S3 Directory]( >[AWS S3 Buckets]( See a [usage example for S3DirectoryLoader](/docs/integrations/document_loaders/aws_s3_directory). See a [usage example for S3FileLoader](/docs/integrations/document_loaders/aws_s3_file). ```python from langchain.document_loaders import S3DirectoryLoader, S3FileLoader ``` ## Memory ### AWS DynamoDB >[AWS DynamoDB]( > is a fully managed `NoSQL` database service that provides fast and predictable performance with seamless scalability. We have to configure the [AWS CLI]( We need to install the `boto3` library. ```bash pip install boto3 ``` See a [usage example](/docs/integrations/memory/aws_dynamodb). ```python from langchain.memory import DynamoDBChatMessageHistory ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/platforms/google.mdx"}, "data": "# Google All functionality related to [Google Cloud Platform]( and other `Google` products. ## LLMs ### Vertex AI Access `PaLM` LLMs like `text-bison` and `code-bison` via `Google Vertex AI`. We need to install `google-cloud-aiplatform` python package. ```bash pip install google-cloud-aiplatform ``` See a [usage example](/docs/integrations/llms/google_vertex_ai_palm). ```python from langchain.llms import VertexAI ``` ### Model Garden Access PaLM and hundreds of OSS models via `Vertex AI Model Garden`. We need to install `google-cloud-aiplatform` python package. ```bash pip install google-cloud-aiplatform ``` See a [usage example](/docs/integrations/llms/google_vertex_ai_palm#vertex-model-garden). ```python from langchain.llms import VertexAIModelGarden ``` ## Chat models ### Vertex AI Access PaLM chat models like `chat-bison` and `codechat-bison` via Google Cloud. We need to install `google-cloud-aiplatform` python package. ```bash pip install google-cloud-aiplatform ``` See a [usage example](/docs/integrations/chat/google_vertex_ai_palm). ```python from langchain.chat_models import ChatVertexAI ``` ## Document Loaders ### Google BigQuery > [Google BigQuery]( is a serverless and cost-effective enterprise data warehouse that works across clouds and scales with your data. `BigQuery` is a part of the `Google Cloud Platform`. We need to install `google-cloud-bigquery` python package. ```bash pip install google-cloud-bigquery ``` See a [usage example](/docs/integrations/document_loaders/google_bigquery). ```python from langchain.document_loaders import BigQueryLoader ``` ### Google Cloud Storage >[Google Cloud Storage]( is a managed service for storing unstructured data. We need to install `google-cloud-storage` python package. ```bash pip install google-cloud-storage ``` There are two loaders for the `Google Cloud Storage`: the `Directory` and the `File` loaders. See a [usage example](/docs/integrations/document_loaders/google_cloud_storage_directory). ```python from langchain.document_loaders import GCSDirectoryLoader ``` See a [usage example](/docs/integrations/document_loaders/google_cloud_storage_file). ```python from langchain.document_loaders import GCSFileLoader ``` ### Google Drive >[Google Drive]( is a file storage and synchronization service developed by Google. Currently, only `Google Docs` are supported. We need to install several python packages. ```bash pip install google-api-python-client google-auth- google-auth-oauthlib ``` See a [usage example and authorization instructions](/docs/integrations/document_loaders/google_drive). ```python from langchain.document_loaders import GoogleDriveLoader ``` ### Speech-to-Text > [Google Cloud Speech-to-Text]( is an audio transcription API powered by Google's speech recognition models. This document loader transcribes audio files and outputs the text results as Documents. First, we need to install the python package. ```bash pip install google-cloud-speech ``` See a [usage example and authorization instructions](/docs/integrations/document_loaders/google_speech_to_text). ```python from langchain.document_loaders import GoogleSpeechToTextLoader ``` ## Vector Stores ### Google Vertex AI Vector Search > [Google Vertex AI Vector Search]( > formerly known as `Vertex AI Matching Engine`, provides the industry's leading high-scale > low latency vector database. These vector databases are commonly > referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service. We need to install several python packages. ```bash pip install tensorflow google-cloud-aiplatform tensorflow-hub tensorflow-text ``` See a [usage example](/docs/integrations/vectorstores/matchingengine). ```python from langchain.vectorstores import MatchingEngine ``` ### Google ScaNN >[Google ScaNN]( > (Scalable Nearest Neighbors) is a python package. > >`ScaNN` is a method for efficient vector similarity search at scale. >`ScaNN` includes search space pruning and quantization for Maximum Inner > Product Search and also supports other distance functions such as > Euclidean distance. The implementation is optimized for x86 processors > with AVX2 support. See its [Google Research github]( > for more details. We need to install `scann` python package. ```bash pip install scann ``` See a [usage example](/docs/integrations/vectorstores/scann). ```python from langchain.vectorstores import ScaNN ``` ## Retrievers ### Google Drive We need to install several python packages. ```bash pip install google-api-python-client google-auth- google-auth-oauthlib ``` See a [usage example and authorization instructions](/docs/integrations/retrievers/google_drive). ```python from langchain_googledrive.retrievers import GoogleDriveRetriever ``` ### Vertex AI Search > [Google Cloud Vertex AI Search]( > allows developers to quickly build generative AI powered search engines for customers and employees. We need to install the `google-cloud-discoveryengine` python package. ```bash pip install google-cloud-discoveryengine ``` See a [usage example](/docs/integrations/retrievers/google_vertex_ai_search). ```python from langchain.retrievers import GoogleVertexAISearchRetriever ``` ### Document AI Warehouse > [Google Cloud Document AI Warehouse]( > allows enterprises to search, store, govern, and manage documents and their AI-extracted > data and metadata in a single platform. > ```python from langchain.retrievers import GoogleDocumentAIWarehouseRetriever docai_wh_retriever = GoogleDocumentAIWarehouseRetriever( project_number=... ) query = ... documents = docai_wh_retriever.get_relevant_documents( query, user_ldap=... ) ``` ## Tools ### Google Drive We need to install several python packages. ```bash pip install google-api-python-client google-auth- google-auth-oauthlib ``` See a [usage example and authorization instructions](/docs/integrations/tools/google_drive). ```python from langchain.utilities.google_drive import GoogleDriveAPIWrapper from langchain.tools.google_drive.tool import GoogleDriveSearchTool ``` ### Google Places We need to install a python package. ```bash pip install googlemaps ``` See a [usage example and authorization instructions](/docs/integrations/tools/google_places). ```python from langchain.tools import GooglePlacesTool ``` ### Google Search We need to install a python package. ```bash pip install google-api-python-client ``` - Set up a Custom Search Engine, following [these instructions]( - Get an API Key and Custom Search Engine ID from the previous step, and set them as environment variables `GOOGLE_API_KEY` and `GOOGLE_CSE_ID` respectively ```python from langchain.utilities import GoogleSearchAPIWrapper ``` For a more detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/google_search). We can easily load this wrapper as a Tool (to use with an Agent). We can do this with: ```python from langchain.agents import load_tools tools = load_tools([\"google-search\"]) ``` ## Document Transformers ### Google Document AI >[Document AI]( is a `Google Cloud Platform` > service that transforms unstructured data from documents into structured data, making it easier > to understand, analyze, and consume. We need to set up a [`GCS` bucket and create your own OCR processor]( The `GCS_OUTPUT_PATH` should be a path to a folder on GCS (starting with `gs://`) and a processor name should look like `projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID`. We can get it either programmatically or copy from the `Prediction endpoint` section of the `Processor details` tab in the Google Cloud Console. ```bash pip install google-cloud-documentai pip install google-cloud-documentai-toolbox ``` See a [usage example](/docs/integrations/document_transformers/docai). ```python from langchain.document_loaders.blob_loaders import Blob from langchain.document_loaders.parsers import DocAIParser ``` ### Google Translate > [Google Translate]( is a multilingual neural machine > translation service developed by Google to translate text, documents and websites > from one language into another. The `GoogleTranslateTransformer` allows you to translate text and HTML with the [Google Cloud Translation API]( To use it, you should have the `google-cloud-translate` python package installed, and a Google Cloud project with the [Translation API enabled]( This transformer uses the [Advanced edition (v3)]( First, we need to install the python package. ```bash pip install google-cloud-translate ``` See a [usage example and authorization instructions](/docs/integrations/document_transformers/google_translate). ```python from langchain.document_transformers import GoogleTranslateTransformer ``` ## Toolkits ### GMail > [Gmail]( is a free email service provided by Google. This toolkit works with emails through the `Gmail API`. We need to install several python packages. ```bash pip install google-api-python-client google-auth-oauthlib google-auth- ``` See a [usage example and authorization instructions](/docs/integrations/toolkits/gmail). ```python from langchain.agents.agent_toolkits import GmailToolkit ``` ### Google Drive This toolkit uses the `Google Drive API`. We need to install several python packages. ```bash pip install google-api-python-client google-auth- google-auth-oauthlib ``` See a [usage example and authorization instructions](/docs/integrations/toolkits/google_drive). ```python from langchain_googledrive.utilities.google_drive import GoogleDriveAPIWrapper from langchain_googledrive.tools.google_drive.tool import GoogleDriveSearchTool ``` ## Chat Loaders ### GMail > [Gmail]( is a free email service provided by Google. This loader works with emails through the `Gmail API`. We need to install several python packages. ```bash pip install google-api-python-client google-auth-oauthlib google-auth- ``` See a [usage example and authorization instructions](/docs/integrations/chat_loaders/gmail). ```python from langchain.chat_loaders.gmail import GMailLoader ``` ## 3rd Party Integrations ### SerpAPI >[SerpApi]( provides a 3rd-party API to access Google search results. See a [usage example and authorization instructions](/docs/integrations/tools/google_serper). ```python from langchain.utilities import GoogleSerperAPIWrapper ``` ### YouTube >[YouTube Search]( package searches `YouTube` videos avoiding using their heavily rate-limited API. > >It uses the form on the YouTube homepage and scrapes the resulting page. We need to install a python package. ```bash pip install youtube_search ``` See a [usage example](/docs/integrations/tools/youtube). ```python from langchain.tools import YouTubeSearchTool ``` ### YouTube audio >[YouTube]( is an online video sharing and social media platform created by `Google`. Use `YoutubeAudioLoader` to fetch / download the audio files. Then, use `OpenAIWhisperParser` to transcribe them to text. We need to install several python packages. ```bash pip install yt_dlp pydub librosa ``` See a [usage example and authorization instructions](/docs/integrations/document_loaders/youtube_audio). ```python from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader from langchain.document_loaders.parsers import OpenAIWhisperParser, OpenAIWhisperParserLocal ``` ### YouTube transcripts >[YouTube]( is an online video sharing and social media platform created by `Google`. We need to install `youtube-transcript-api` python package. ```bash pip install youtube-transcript-api ``` See a [usage example](/docs/integrations/document_loaders/youtube_transcript). ```python from langchain.document_loaders import YoutubeLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/platforms/microsoft.mdx"}, "data": "# Microsoft All functionality related to `Microsoft Azure` and other `Microsoft` products. ## Chat Models ### Azure OpenAI >[Microsoft Azure]( often referred to as `Azure` is a cloud computing platform run by `Microsoft`, which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). `Microsoft Azure` supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems. >[Azure OpenAI]( is an `Azure` service with powerful language models from `OpenAI` including the `GPT-3`, `Codex` and `Embeddings model` series for content generation, summarization, semantic search, and natural language to code translation. ```bash pip install openai tiktoken ``` Set the environment variables to get access to the `Azure OpenAI` service. ```python import os os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \" Blob Storage]( is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data. >[Azure Files]( offers fully managed > file shares in the cloud that are accessible via the industry standard Server Message Block (`SMB`) protocol, > Network File System (`NFS`) protocol, and `Azure Files REST API`. `Azure Files` are based on the `Azure Blob Storage`. `Azure Blob Storage` is designed for: - Serving images or documents directly to a browser. - Storing files for distributed access. - Streaming video and audio. - Writing to log files. - Storing data for backup and restore, disaster recovery, and archiving. - Storing data for analysis by an on-premises or Azure-hosted service. ```bash pip install azure-storage-blob ``` See a [usage example for the Azure Blob Storage](/docs/integrations/document_loaders/azure_blob_storage_container). ```python from langchain.document_loaders import AzureBlobStorageContainerLoader ``` See a [usage example for the Azure Files](/docs/integrations/document_loaders/azure_blob_storage_file). ```python from langchain.document_loaders import AzureBlobStorageFileLoader ``` ### Microsoft OneDrive >[Microsoft OneDrive]( (formerly `SkyDrive`) is a file-hosting service operated by Microsoft. First, you need to install a python package. ```bash pip install o365 ``` See a [usage example](/docs/integrations/document_loaders/microsoft_onedrive). ```python from langchain.document_loaders import OneDriveLoader ``` ### Microsoft Word >[Microsoft Word]( is a word processor developed by Microsoft. See a [usage example](/docs/integrations/document_loaders/microsoft_word). ```python from langchain.document_loaders import UnstructuredWordDocumentLoader ``` ## Vector stores ### Azure Cosmos DB >[Azure Cosmos DB for MongoDB vCore]( makes it easy to create a database with full native MongoDB support. > You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account's connection string. > Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that's stored in Azure Cosmos DB. #### Installation and Setup See [detail configuration instructions](/docs/integrations/vectorstores/azure_cosmos_db). We need to install `pymongo` python package. ```bash pip install pymongo ``` #### Deploy Azure Cosmos DB on Microsoft Azure Azure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture. With Cosmos DB for MongoDB vCore, developers can enjoy the benefits of native Azure integrations, low total cost of ownership (TCO), and the familiar vCore architecture when migrating existing applications or building new ones. [Sign Up]( for free to get started today. See a [usage example](/docs/integrations/vectorstores/azure_cosmos_db). ```python from langchain.vectorstores import AzureCosmosDBVectorSearch ``` ## Retrievers ### Azure Cognitive Search >[Azure Cognitive Search]( (formerly known as `Azure Search`) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications. >Search is foundational to any app that surfaces text to users, where common scenarios include catalog or document search, online retail apps, or data exploration over proprietary content. When you create a search service, you'll work with the following capabilities: >- A search engine for full text search over a search index containing user-owned content >- Rich indexing, with lexical analysis and optional AI enrichment for content extraction and transformation >- Rich query syntax for text search, fuzzy search, autocomplete, geo-search and more >- Programmability through REST APIs and client libraries in Azure SDKs >- Azure integration at the data layer, machine learning layer, and AI (Cognitive Services) See [set up instructions]( See a [usage example](/docs/integrations/retrievers/azure_cognitive_search). ```python from langchain.retrievers import AzureCognitiveSearchRetriever ``` ## Utilities ### Bing Search API See a [usage example](/docs/integrations/tools/bing_search). ```python from langchain.utilities import BingSearchAPIWrapper ``` ## Toolkits ### Azure Cognitive Services We need to install several python packages. ```bash pip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision ``` See a [usage example](/docs/integrations/toolkits/azure_cognitive_services). ```python from langchain.agents.agent_toolkits import O365Toolkit ``` ### Microsoft Office 365 email and calendar We need to install `O365` python package. ```bash pip install O365 ``` See a [usage example](/docs/integrations/toolkits/office365). ```python from langchain.agents.agent_toolkits import O365Toolkit ``` ### Microsoft Azure PowerBI We need to install `azure-identity` python package. ```bash pip install azure-identity ``` See a [usage example](/docs/integrations/toolkits/powerbi). ```python from langchain.agents.agent_toolkits import PowerBIToolkit from langchain.utilities.powerbi import PowerBIDataset ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/platforms/openai.mdx"}, "data": "# OpenAI All functionality related to OpenAI >[OpenAI]( is American artificial intelligence (AI) research laboratory > consisting of the non-profit `OpenAI Incorporated` > and its for-profit subsidiary corporation `OpenAI Limited Partnership`. > `OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI. > `OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`. >The [OpenAI API]( is powered by a diverse set of models with different capabilities and price points. > >[ChatGPT]( is the Artificial Intelligence (AI) chatbot developed by `OpenAI`. ## Installation and Setup - Install the Python SDK with ```bash pip install openai ``` - Get an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`) - If you want to use OpenAI's tokenizer (only available for Python 3.9+), install it ```bash pip install tiktoken ``` ## LLM See a [usage example](/docs/integrations/llms/openai). ```python from langchain.llms import OpenAI ``` If you are using a model hosted on `Azure`, you should use different wrapper for that: ```python from langchain.llms import AzureOpenAI ``` For a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai_example) ## Chat model See a [usage example](/docs/integrations/chat/openai). ```python from langchain.chat_models import ChatOpenAI ``` If you are using a model hosted on `Azure`, you should use different wrapper for that: ```python from langchain.llms import AzureChatOpenAI ``` For a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai) ## Text Embedding Model See a [usage example](/docs/integrations/text_embedding/openai) ```python from langchain.embeddings import OpenAIEmbeddings ``` ## Tokenizer There are several places you can use the `tiktoken` tokenizer. By default, it is used to count tokens for OpenAI LLMs. You can also use it to count tokens when splitting documents with ```python from langchain.text_splitter import CharacterTextSplitter CharacterTextSplitter.from_tiktoken_encoder(...) ``` For a more detailed walkthrough of this, see [this notebook](/docs/modules/data_connection/document_transformers/text_splitters/tiktoken) ## Document Loader See a [usage example](/docs/integrations/document_loaders/chatgpt_loader). ```python from langchain.document_loaders.chatgpt import ChatGPTLoader ``` ## Retriever See a [usage example](/docs/integrations/retrievers/chatgpt-plugin). ```python from langchain.retrievers import ChatGPTPluginRetriever ``` ## Chain See a [usage example](/docs/guides/safety/moderation). ```python from langchain.chains import OpenAIModerationChain ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/activeloop_deeplake.mdx"}, "data": "# Activeloop Deep Lake This page covers how to use the Deep Lake ecosystem within LangChain. ## Why Deep Lake? - More than just a (multi-modal) vector store. You can later use the dataset to fine-tune your own LLM models. - Not only stores embeddings, but also the original data with automatic version control. - Truly serverless. Doesn't require another service and can be used with major cloud providers (AWS S3, GCS, etc.) Activeloop Deep Lake supports SelfQuery Retrieval: [Activeloop Deep Lake Self Query Retrieval](/docs/extras/modules/data_connection/retrievers/self_query/activeloop_deeplake_self_query) ## More Resources 1. [Ultimate Guide to LangChain & Deep Lake: Build ChatGPT to Answer Questions on Your Financial Data]( 2. [Twitter the-algorithm codebase analysis with Deep Lake](/docs/use_cases/question_answering/code/twitter-the-algorithm-analysis-deeplake) 4. [Code Understanding](/docs/modules/data_connection/retrievers/self_query/activeloop_deeplake_self_query) 3. Here is [whitepaper]( and [academic paper]( for Deep Lake 4. Here is a set of additional resources available for review: [Deep Lake]( [Get started]( and [Tutorials]( ## Installation and Setup - Install the Python package with `pip install deeplake` ## Wrappers ### VectorStore There exists a wrapper around Deep Lake, a data lake for Deep Learning applications, allowing you to use it as a vector store (for now), whether for semantic search or example selection. To import this vectorstore: ```python from langchain.vectorstores import DeepLake ``` For a more detailed walkthrough of the Deep Lake wrapper, see [this notebook](/docs/integrations/vectorstores/activeloop_deeplake)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/ai21.mdx"}, "data": "# AI21 Labs This page covers how to use the AI21 ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific AI21 wrappers. ## Installation and Setup - Get an AI21 api key and set it as an environment variable (`AI21_API_KEY`) ## Wrappers ### LLM There exists an AI21 LLM wrapper, which you can access with ```python from langchain.llms import AI21 ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/ainetwork.mdx"}, "data": "# AINetwork >[AI Network]( is a layer 1 blockchain designed to accommodate > large-scale AI models, utilizing a decentralized GPU network powered by the > [$AIN token]( enriching AI-driven `NFTs` (`AINFTs`). ## Installation and Setup You need to install `ain-py` python package. ```bash pip install ain-py ``` You need to set the `AIN_BLOCKCHAIN_ACCOUNT_PRIVATE_KEY` environmental variable to your AIN Blockchain Account Private Key. ## Toolkit See a [usage example](/docs/integrations/toolkits/ainetwork). ```python from langchain.agents.agent_toolkits.ainetwork.toolkit import AINetworkToolkit ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/airbyte.mdx"}, "data": "# Airbyte >[Airbyte]( is a data integration platform for ELT pipelines from APIs, > databases & files to warehouses & lakes. It has the largest catalog of ELT connectors to data warehouses and databases. ## Installation and Setup This instruction shows how to load any source from `Airbyte` into a local `JSON` file that can be read in as a document. **Prerequisites:** Have `docker desktop` installed. **Steps:** 1. Clone Airbyte from GitHub - `git clone 2. Switch into Airbyte directory - `cd airbyte`. 3. Start Airbyte - `docker compose up`. 4. In your browser, just visit You will be asked for a username and password. By default, that's username `airbyte` and password `password`. 5. Setup any source you wish. 6. Set destination as Local JSON, with specified destination path - lets say `/json_data`. Set up a manual sync. 7. Run the connection. 8. To see what files are created, navigate to: `file:///tmp/airbyte_local/`. ## Document Loader See a [usage example](/docs/integrations/document_loaders/airbyte_json). ```python from langchain.document_loaders import AirbyteJSONLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/airtable.md"}, "data": "# Airtable >[Airtable]( is a cloud collaboration service. `Airtable` is a spreadsheet-database hybrid, with the features of a database but applied to a spreadsheet. > The fields in an Airtable table are similar to cells in a spreadsheet, but have types such as 'checkbox', > 'phone number', and 'drop-down list', and can reference file attachments like images. >Users can create a database, set up column types, add records, link tables to one another, collaborate, sort records > and publish views to external websites. ## Installation and Setup ```bash pip install pyairtable ``` * Get your [API key]( * Get the [ID of your base]( * Get the [table ID from the table url]( ## Document Loader ```python from langchain.document_loaders import AirtableLoader ``` See an [example](/docs/integrations/document_loaders/airtable)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/aleph_alpha.mdx"}, "data": "# Aleph Alpha >[Aleph Alpha]( was founded in 2019 with the mission to research and build the foundational technology for an era of strong AI. The team of international scientists, engineers, and innovators researches, develops, and deploys transformative AI like large language and multimodal models and runs the fastest European commercial AI cluster. >[The Luminous series]( is a family of large language models. ## Installation and Setup ```bash pip install aleph-alpha-client ``` You have to create a new token. Please, see [instructions]( ```python from getpass import getpass ALEPH_ALPHA_API_KEY = getpass() ``` ## LLM See a [usage example](/docs/integrations/llms/aleph_alpha). ```python from langchain.llms import AlephAlpha ``` ## Text Embedding Models See a [usage example](/docs/integrations/text_embedding/aleph_alpha). ```python from langchain.embeddings import AlephAlphaSymmetricSemanticEmbedding, AlephAlphaAsymmetricSemanticEmbedding ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/alibabacloud_opensearch.md"}, "data": "# Alibaba Cloud Opensearch [Alibaba Cloud Opensearch]( OpenSearch is a one-stop platform to develop intelligent search services. OpenSearch was built based on the large-scale distributed search engine developed by Alibaba. OpenSearch serves more than 500 business cases in Alibaba Group and thousands of Alibaba Cloud customers. OpenSearch helps develop search services in different search scenarios, including e-commerce, O2O, multimedia, the content industry, communities and forums, and big data query in enterprises. OpenSearch helps you develop high quality, maintenance-free, and high performance intelligent search services to provide your users with high search efficiency and accuracy. OpenSearch provides the vector search feature. In specific scenarios,especially in question retrieval and image search scenarios, you can use the vector search feature together with the multimodal search feature to improve the accuracy of search results. ## Purchase an instance and configure it - Purchase OpenSearch Vector Search Edition from [Alibaba Cloud]( and configure the instance according to the help [documentation]( ## Alibaba Cloud Opensearch Vector Store Wrappers supported functions: - `add_texts` - `add_documents` - `from_texts` - `from_documents` - `similarity_search` - `asimilarity_search` - `similarity_search_by_vector` - `asimilarity_search_by_vector` - `similarity_search_with_relevance_scores` - `delete_doc_by_texts` For a more detailed walk through of the Alibaba Cloud OpenSearch wrapper, see [this notebook](../modules/indexes/vectorstores/examples/alibabacloud_opensearch.ipynb) If you encounter any problems during use, please feel free to contact [xingshaomin.xsm@alibaba-inc.com](xingshaomin.xsm@alibaba-inc.com) , and we will do our best to provide you with assistance and support."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/analyticdb.mdx"}, "data": "# AnalyticDB This page covers how to use the AnalyticDB ecosystem within LangChain. ### VectorStore There exists a wrapper around AnalyticDB, allowing you to use it as a vectorstore, whether for semantic search or example selection. To import this vectorstore: ```python from langchain.vectorstores import AnalyticDB ``` For a more detailed walkthrough of the AnalyticDB wrapper, see [this notebook](/docs/integrations/vectorstores/analyticdb)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/annoy.mdx"}, "data": "# Annoy > [Annoy]( (`Approximate Nearest Neighbors Oh Yeah`) is a C++ library with Python bindings to search for points in space that are close to a given query point. It also creates large read-only file-based data structures that are mmapped into memory so that many processes may share the same data. ## Installation and Setup ```bash pip install annoy ``` ## Vectorstore See a [usage example](/docs/integrations/vectorstores/annoy). ```python from langchain.vectorstores import Annoy ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/anyscale.mdx"}, "data": "# Anyscale This page covers how to use the Anyscale ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Anyscale wrappers. ## Installation and Setup - Get an Anyscale Service URL, route and API key and set them as environment variables (`ANYSCALE_SERVICE_URL`,`ANYSCALE_SERVICE_ROUTE`, `ANYSCALE_SERVICE_TOKEN`). - Please see [the Anyscale docs]( for more details. ## Wrappers ### LLM There exists an Anyscale LLM wrapper, which you can access with ```python from langchain.llms import Anyscale ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/apify.mdx"}, "data": "# Apify This page covers how to use [Apify]( within LangChain. ## Overview Apify is a cloud platform for web scraping and data extraction, which provides an [ecosystem]( of more than a thousand ready-made apps called *Actors* for various scraping, crawling, and extraction use cases. [![Apify Actors](/img/ApifyActors.png)]( This integration enables you run Actors on the Apify platform and load their results into LangChain to feed your vector indexes with documents and data from the web, e.g. to generate answers from websites with documentation, blogs, or knowledge bases. ## Installation and Setup - Install the Apify API client for Python with `pip install apify-client` - Get your [Apify API token]( and either set it as an environment variable (`APIFY_API_TOKEN`) or pass it to the `ApifyWrapper` as `apify_api_token` in the constructor. ## Wrappers ### Utility You can use the `ApifyWrapper` to run Actors on the Apify platform. ```python from langchain.utilities import ApifyWrapper ``` For a more detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/apify). ### Loader You can also use our `ApifyDatasetLoader` to get data from Apify dataset. ```python from langchain.document_loaders import ApifyDatasetLoader ``` For a more detailed walkthrough of this loader, see [this notebook](/docs/integrations/document_loaders/apify_dataset)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/arangodb.mdx"}, "data": "# ArangoDB >[ArangoDB]( is a scalable graph database system to drive value from connected data, faster. Native graphs, an integrated search engine, and JSON support, via a single query language. ArangoDB runs on-prem, in the cloud \u2013 anywhere. ## Dependencies Install the [ArangoDB Python Driver]( package with ```bash pip install python-arango ``` ## Graph QA Chain Connect your ArangoDB Database with a chat model to get insights on your data. See the notebook example [here](/docs/use_cases/graph/graph_arangodb_qa). ```python from arango import ArangoClient from langchain.graphs import ArangoGraph from langchain.chains import ArangoGraphQAChain ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/argilla.mdx"}, "data": "# Argilla ![Argilla - Open-source data platform for LLMs]( >[Argilla]( is an open-source data curation platform for LLMs. > Using Argilla, everyone can build robust language models through faster data curation > using both human and machine feedback. We provide support for each step in the MLOps cycle, > from data labelling to model monitoring. ## Installation and Setup First, you'll need to install the `argilla` Python package as follows: ```bash pip install argilla --upgrade ``` If you already have an Argilla Server running, then you're good to go; but if you don't, follow the next steps to install it. If you don't you can refer to [Argilla - Quickstart]( to deploy Argilla either on HuggingFace Spaces, locally, or on a server. ## Tracking See a [usage example of `ArgillaCallbackHandler`](/docs/integrations/callbacks/argilla). ```python from langchain.callbacks import ArgillaCallbackHandler ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/arxiv.mdx"}, "data": "# Arxiv >[arXiv]( is an open-access archive for 2 million scholarly articles in the fields of physics, > mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and > systems science, and economics. ## Installation and Setup First, you need to install `arxiv` python package. ```bash pip install arxiv ``` Second, you need to install `PyMuPDF` python package which transforms PDF files downloaded from the `arxiv.org` site into the text format. ```bash pip install pymupdf ``` ## Document Loader See a [usage example](/docs/integrations/document_loaders/arxiv). ```python from langchain.document_loaders import ArxivLoader ``` ## Retriever See a [usage example](/docs/integrations/retrievers/arxiv). ```python from langchain.retrievers import ArxivRetriever ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/astradb.mdx"}, "data": "# Astra DB This page lists the integrations available with [Astra DB]( and [Apache Cassandra]( ### Setup Install the following Python package: ```bash pip install \"astrapy>=0.5.3\" ``` ## Astra DB > DataStax [Astra DB]( is a serverless vector-capable database built on Cassandra and made conveniently available > through an easy-to-use JSON API. ### Vector Store ```python from langchain.vectorstores import AstraDB vector_store = AstraDB( embedding=my_embedding, collection_name=\"my_store\", api_endpoint=\"...\", token=\"...\", ) ``` Learn more in the [example notebook](/docs/integrations/vectorstores/astradb). ## Apache Cassandra and Astra DB through CQL > [Cassandra]( is a NoSQL, row-oriented, highly scalable and highly available database. > Starting with version 5.0, the database ships with [vector search capabilities]( > DataStax [Astra DB through CQL]( is a managed serverless database built on Cassandra, offering the same interface and strengths. These databases use the CQL protocol (Cassandra Query Language). Hence, a different set of connectors, outlined below, shall be used. ### Vector Store ```python from langchain.vectorstores import Cassandra vector_store = Cassandra( embedding=my_embedding, table_name=\"my_store\", ) ``` Learn more in the [example notebook](/docs/integrations/vectorstores/astradb) (scroll down to the CQL-specific section). ### Memory ```python from langchain.memory import CassandraChatMessageHistory message_history = CassandraChatMessageHistory(session_id=\"my-session\") ``` Learn more in the [example notebook](/docs/integrations/memory/cassandra_chat_message_history). ### LLM Cache ```python from langchain.cache import CassandraCache langchain.llm_cache = CassandraCache() ``` Learn more in the [example notebook](/docs/integrations/llms/llm_caching) (scroll to the Cassandra section). ### Semantic LLM Cache ```python from langchain.cache import CassandraSemanticCache cassSemanticCache = CassandraSemanticCache( embedding=my_embedding, table_name=\"my_store\", ) ``` Learn more in the [example notebook](/docs/integrations/llms/llm_caching) (scroll to the appropriate section)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/atlas.mdx"}, "data": "# Atlas >[Nomic Atlas]( is a platform for interacting with both > small and internet scale unstructured datasets. ## Installation and Setup - Install the Python package with `pip install nomic` - `Nomic` is also included in langchains poetry extras `poetry install -E all` ## VectorStore See a [usage example](/docs/integrations/vectorstores/atlas). ```python from langchain.vectorstores import AtlasDB ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/awadb.md"}, "data": "# AwaDB >[AwaDB]( is an AI Native database for the search and storage of embedding vectors used by LLM Applications. ## Installation and Setup ```bash pip install awadb ``` ## Vector Store ```python from langchain.vectorstores import AwaDB ``` See a [usage example](/docs/integrations/vectorstores/awadb). ## Text Embedding Model ```python from langchain.embeddings import AwaEmbeddings ``` See a [usage example](/docs/integrations/text_embedding/awadb)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/aws_dynamodb.mdx"}, "data": "# AWS DynamoDB >[AWS DynamoDB]( > is a fully managed `NoSQL` database service that provides fast and predictable performance with seamless scalability. ## Installation and Setup We have to configur the [AWS CLI]( We need to install the `boto3` library. ```bash pip install boto3 ``` ## Memory See a [usage example](/docs/integrations/memory/aws_dynamodb). ```python from langchain.memory import DynamoDBChatMessageHistory ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/azlyrics.mdx"}, "data": "# AZLyrics >[AZLyrics]( is a large, legal, every day growing collection of lyrics. ## Installation and Setup There isn't any special setup for it. ## Document Loader See a [usage example](/docs/integrations/document_loaders/azlyrics). ```python from langchain.document_loaders import AZLyricsLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/bageldb.mdx"}, "data": "# BagelDB > [BagelDB]( (`Open Vector Database for AI`), is like GitHub for AI data. It is a collaborative platform where users can create, share, and manage vector datasets. It can support private projects for independent developers, internal collaborations for enterprises, and public contributions for data DAOs. ## Installation and Setup ```bash pip install betabageldb ``` ## VectorStore See a [usage example](/docs/integrations/vectorstores/bageldb). ```python from langchain.vectorstores import Bagel ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/bananadev.mdx"}, "data": "# Banana Banana provided serverless GPU inference for AI models, including a CI/CD build pipeline and a simple Python framework (Potassium) to server your models. This page covers how to use the [Banana]( ecosystem within LangChain. It is broken into two parts: * installation and setup, * and then references to specific Banana wrappers. ## Installation and Setup - Install with `pip install banana-dev` - Get an Banana api key from the [Banana.dev dashboard]( and set it as an environment variable (`BANANA_API_KEY`) - Get your model's key and url slug from the model's details page ## Define your Banana Template You'll need to set up a Github repo for your Banana app. You can get started in 5 minutes using [this guide]( Alternatively, for a ready-to-go LLM example, you can check out Banana's [CodeLlama-7B-Instruct-GPTQ]( GitHub repository. Just fork it and deploy it within Banana. Other starter repos are available [here]( ## Build the Banana app To use Banana apps within Langchain, they must include the `outputs` key in the returned json, and the value must be a string. ```python # Return the results as a dictionary result = {'outputs': result} ``` An example inference function would be: ```python @app.handler(\"/\") def handler(context: dict, request: Request) -> Response: \"\"\"Handle a request to generate code from a prompt.\"\"\" model = context.get(\"model\") tokenizer = context.get(\"tokenizer\") max_new_tokens = request.json.get(\"max_new_tokens\", 512) temperature = request.json.get(\"temperature\", 0.7) prompt = request.json.get(\"prompt\") prompt_template=f'''[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```: {prompt} [/INST] ''' input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda() output = model.generate(inputs=input_ids, temperature=temperature, max_new_tokens=max_new_tokens) result = tokenizer.decode(output[0]) return Response(json={\"outputs\": result}, status=200) ``` This example is from the `app.py` file in [CodeLlama-7B-Instruct-GPTQ]( ## Wrappers ### LLM Within Langchain, there exists a Banana LLM wrapper, which you can access with ```python from langchain.llms import Banana ``` You need to provide a model key and model url slug, which you can get from the model's details page in the [Banana.dev dashboard]( ```python llm = Banana(model_key=\"YOUR_MODEL_KEY\", model_url_slug=\"YOUR_MODEL_URL_SLUG\") ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/baseten.md"}, "data": "# Baseten Learn how to use LangChain with models deployed on Baseten. ## Installation and setup - Create a [Baseten]( account and [API key]( - Install the Baseten Python client with `pip install baseten` - Use your API key to authenticate with `baseten login` ## Invoking a model Baseten integrates with LangChain through the LLM module, which provides a standardized and interoperable interface for models that are deployed on your Baseten workspace. You can deploy foundation models like WizardLM and Alpaca with one click from the [Baseten model library]( or if you have your own model, [deploy it with this tutorial]( In this example, we'll work with WizardLM. [Deploy WizardLM here]( and follow along with the deployed [model's version ID]( ```python from langchain.llms import Baseten wizardlm = Baseten(model=\"MODEL_VERSION_ID\", verbose=True) wizardlm(\"What is the difference between a Wizard and a Sorcerer?\") ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/beam.mdx"}, "data": "# Beam This page covers how to use Beam within LangChain. It is broken into two parts: installation and setup, and then references to specific Beam wrappers. ## Installation and Setup - [Create an account]( - Install the Beam CLI with `curl -sSfL | sh` - Register API keys with `beam configure` - Set environment variables (`BEAM_CLIENT_ID`) and (`BEAM_CLIENT_SECRET`) - Install the Beam SDK `pip install beam-sdk` ## Wrappers ### LLM There exists a Beam LLM wrapper, which you can access with ```python from langchain.llms.beam import Beam ``` ## Define your Beam app. This is the environment you\u2019ll be developing against once you start the app. It's also used to define the maximum response length from the model. ```python llm = Beam(model_name=\"gpt2\", name=\"langchain-gpt2-test\", cpu=8, memory=\"32Gi\", gpu=\"A10G\", python_version=\"python3.8\", python_packages=[ \"diffusers[torch]>=0.10\", \"transformers\", \"torch\", \"pillow\", \"accelerate\", \"safetensors\", \"xformers\",], max_length=\"50\", verbose=False) ``` ## Deploy your Beam app Once defined, you can deploy your Beam app by calling your model's `_deploy()` method. ```python llm._deploy() ``` ## Call your Beam app Once a beam model is deployed, it can be called by callying your model's `_call()` method. This returns the GPT2 text response to your prompt. ```python response = llm._call(\"Running machine learning on a remote GPU\") ``` An example script which deploys the model and calls it would be: ```python from langchain.llms.beam import Beam import time llm = Beam(model_name=\"gpt2\", name=\"langchain-gpt2-test\", cpu=8, memory=\"32Gi\", gpu=\"A10G\", python_version=\"python3.8\", python_packages=[ \"diffusers[torch]>=0.10\", \"transformers\", \"torch\", \"pillow\", \"accelerate\", \"safetensors\", \"xformers\",], max_length=\"50\", verbose=False) llm._deploy() response = llm._call(\"Running machine learning on a remote GPU\") print(response) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/beautiful_soup.mdx"}, "data": "# Beautiful Soup >[Beautiful Soup]( is a Python package for parsing > HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). > It creates a parse tree for parsed pages that can be used to extract data from HTML,[3] which > is useful for web scraping. ## Installation and Setup ```bash pip install beautifulsoup4 ``` ## Document Transformer See a [usage example](/docs/integrations/document_transformers/beautiful_soup). ```python from langchain.document_loaders import BeautifulSoupTransformer ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/bilibili.mdx"}, "data": "# BiliBili >[Bilibili]( is one of the most beloved long-form video sites in China. ## Installation and Setup ```bash pip install bilibili-api-python ``` ## Document Loader See a [usage example](/docs/integrations/document_loaders/bilibili). ```python from langchain.document_loaders import BiliBiliLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/bittensor.mdx"}, "data": "# NIBittensor This page covers how to use the BittensorLLM inference runtime within LangChain. It is broken into two parts: installation and setup, and then examples of NIBittensorLLM usage. ## Installation and Setup - Install the Python package with `pip install langchain` ## Wrappers ### LLM There exists a NIBittensor LLM wrapper, which you can access with: ```python from langchain.llms import NIBittensorLLM ``` It provides a unified interface for all models: ```python llm = NIBittensorLLM(system_prompt=\"Your task is to provide concise and accurate response based on user prompt\") print(llm('Write a fibonacci function in python with golder ratio')) ``` Multiple responses from top miners can be accessible using the `top_responses` parameter: ```python multi_response_llm = NIBittensorLLM(top_responses=10) multi_resp = multi_response_llm(\"What is Neural Network Feeding Mechanism?\") json_multi_resp = json.loads(multi_resp) print(json_multi_resp) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/blackboard.mdx"}, "data": "# Blackboard >[Blackboard Learn]( (previously the `Blackboard Learning Management System`) > is a web-based virtual learning environment and learning management system developed by Blackboard Inc. > The software features course management, customizable open architecture, and scalable design that allows > integration with student information systems and authentication protocols. It may be installed on local servers, > hosted by `Blackboard ASP Solutions`, or provided as Software as a Service hosted on Amazon Web Services. > Its main purposes are stated to include the addition of online elements to courses traditionally delivered > face-to-face and development of completely online courses with few or no face-to-face meetings. ## Installation and Setup There isn't any special setup for it. ## Document Loader See a [usage example](/docs/integrations/document_loaders/blackboard). ```python from langchain.document_loaders import BlackboardLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/brave_search.mdx"}, "data": "# Brave Search >[Brave Search]( is a search engine developed by Brave Software. > - `Brave Search` uses its own web index. As of May 2022, it covered over 10 billion pages and was used to serve 92% > of search results without relying on any third-parties, with the remainder being retrieved > server-side from the Bing API or (on an opt-in basis) client-side from Google. According > to Brave, the index was kept \"intentionally smaller than that of Google or Bing\" in order to > help avoid spam and other low-quality content, with the disadvantage that \"Brave Search is > not yet as good as Google in recovering long-tail queries.\" >- `Brave Search Premium`: As of April 2023 Brave Search is an ad-free website, but it will > eventually switch to a new model that will include ads and premium users will get an ad-free experience. > User data including IP addresses won't be collected from its users by default. A premium account > will be required for opt-in data-collection. ## Installation and Setup To get access to the Brave Search API, you need to [create an account and get an API key]( ## Document Loader See a [usage example](/docs/integrations/document_loaders/brave_search). ```python from langchain.document_loaders import BraveSearchLoader ``` ## Tool See a [usage example](/docs/integrations/tools/brave_search). ```python from langchain.tools import BraveSearch ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/cerebriumai.mdx"}, "data": "# CerebriumAI This page covers how to use the CerebriumAI ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific CerebriumAI wrappers. ## Installation and Setup - Install with `pip install cerebrium` - Get an CerebriumAI api key and set it as an environment variable (`CEREBRIUMAI_API_KEY`) ## Wrappers ### LLM There exists an CerebriumAI LLM wrapper, which you can access with ```python from langchain.llms import CerebriumAI ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/chaindesk.mdx"}, "data": "# Chaindesk >[Chaindesk]( is an [open-source]( document retrieval platform that helps to connect your personal data with Large Language Models. ## Installation and Setup We need to sign up for Chaindesk, create a datastore, add some data and get your datastore api endpoint url. We need the [API Key]( ## Retriever See a [usage example](/docs/integrations/retrievers/chaindesk). ```python from langchain.retrievers import ChaindeskRetriever ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/chroma.mdx"}, "data": "# Chroma >[Chroma]( is a database for building AI applications with embeddings. ## Installation and Setup ```bash pip install chromadb ``` ## VectorStore There exists a wrapper around Chroma vector databases, allowing you to use it as a vectorstore, whether for semantic search or example selection. ```python from langchain.vectorstores import Chroma ``` For a more detailed walkthrough of the Chroma wrapper, see [this notebook](/docs/integrations/vectorstores/chroma) ## Retriever See a [usage example](/docs/modules/data_connection/retrievers/how_to/self_query/chroma_self_query). ```python from langchain.retrievers import SelfQueryRetriever ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/clarifai.mdx"}, "data": "# Clarifai >[Clarifai]( is one of first deep learning platforms having been founded in 2013. Clarifai provides an AI platform with the full AI lifecycle for data exploration, data labeling, model training, evaluation and inference around images, video, text and audio data. In the LangChain ecosystem, as far as we're aware, Clarifai is the only provider that supports LLMs, embeddings and a vector store in one production scale platform, making it an excellent choice to operationalize your LangChain implementations. ## Installation and Setup - Install the Python SDK: ```bash pip install clarifai ``` [Sign-up]( for a Clarifai account, then get a personal access token to access the Clarifai API from your [security settings]( and set it as an environment variable (`CLARIFAI_PAT`). ## Models Clarifai provides 1,000s of AI models for many different use cases. You can [explore them here]( to find the one most suited for your use case. These models include those created by other providers such as OpenAI, Anthropic, Cohere, AI21, etc. as well as state of the art from open source such as Falcon, InstructorXL, etc. so that you build the best in AI into your products. You'll find these organized by the creator's user_id and into projects we call applications denoted by their app_id. Those IDs will be needed in additional to the model_id and optionally the version_id, so make note of all these IDs once you found the best model for your use case! Also note that given there are many models for images, video, text and audio understanding, you can build some interested AI agents that utilize the variety of AI models as experts to understand those data types. ### LLMs To find the selection of LLMs in the Clarifai platform you can select the text to text model type [here]( ```python from langchain.llms import Clarifai llm = Clarifai(pat=CLARIFAI_PAT, user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID) ``` For more details, the docs on the Clarifai LLM wrapper provide a [detailed walkthrough](/docs/integrations/llms/clarifai). ### Text Embedding Models To find the selection of text embeddings models in the Clarifai platform you can select the text to embedding model type [here]( There is a Clarifai Embedding model in LangChain, which you can access with: ```python from langchain.embeddings import ClarifaiEmbeddings embeddings = ClarifaiEmbeddings(pat=CLARIFAI_PAT, user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID) ``` For more details, the docs on the Clarifai Embeddings wrapper provide a [detailed walkthrough](/docs/integrations/text_embedding/clarifai). ## Vectorstore Clarifai's vector DB was launched in 2016 and has been optimized to support live search queries. With workflows in the Clarifai platform, you data is automatically indexed by am embedding model and optionally other models as well to index that information in the DB for search. You can query the DB not only via the vectors but also filter by metadata matches, other AI predicted concepts, and even do geo-coordinate search. Simply create an application, select the appropriate base workflow for your type of data, and upload it (through the API as [documented here]( or the UIs at clarifai.com). You can also add data directly from LangChain as well, and the auto-indexing will take place for you. You'll notice this is a little different than other vectorstores where you need to provide an embedding model in their constructor and have LangChain coordinate getting the embeddings from text and writing those to the index. Not only is it more convenient, but it's much more scalable to use Clarifai's distributed cloud to do all the index in the background. ```python from langchain.vectorstores import Clarifai clarifai_vector_db = Clarifai.from_texts(user_id=USER_ID, app_id=APP_ID, texts=texts, pat=CLARIFAI_PAT, number_of_docs=NUMBER_OF_DOCS, metadatas = metadatas) ``` For more details, the docs on the Clarifai vector store provide a [detailed walkthrough](/docs/integrations/vectorstores/clarifai)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/clickhouse.mdx"}, "data": "# ClickHouse > [ClickHouse]( is the fast and resource efficient open-source database for real-time > apps and analytics with full SQL support and a wide range of functions to assist users in writing analytical queries. > It has data structures and distance search functions (like `L2Distance`) as well as > [approximate nearest neighbor search indexes]( > That enables ClickHouse to be used as a high performance and scalable vector database to store and search vectors with SQL. ## Installation and Setup We need to install `clickhouse-connect` python package. ```bash pip install clickhouse-connect ``` ## Vector Store See a [usage example](/docs/integrations/vectorstores/clickhouse). ```python from langchain.vectorstores import Clickhouse, ClickhouseSettings ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/cnosdb.mdx"}, "data": "# CnosDB > [CnosDB]( is an open-source distributed time series database with high performance, high compression rate and high ease of use. ## Installation and Setup ```python pip install cnos-connector ``` ## Connecting to CnosDB You can connect to CnosDB using the `SQLDatabase.from_cnosdb()` method. ### Syntax ```python def SQLDatabase.from_cnosdb(url: str = \"127.0.0.1:8902\", user: str = \"root\", password: str = \"\", tenant: str = \"cnosdb\", database: str = \"public\") ``` Args: 1. url (str): The HTTP connection host name and port number of the CnosDB service, excluding \" or \" with a default value of \"127.0.0.1:8902\". 2. user (str): The username used to connect to the CnosDB service, with a default value of \"root\". 3. password (str): The password of the user connecting to the CnosDB service, with a default value of \"\". 4. tenant (str): The name of the tenant used to connect to the CnosDB service, with a default value of \"cnosdb\". 5. database (str): The name of the database in the CnosDB tenant. ## Examples ```python # Connecting to CnosDB with SQLDatabase Wrapper from langchain.utilities import SQLDatabase db = SQLDatabase.from_cnosdb() ``` ```python # Creating a OpenAI Chat LLM Wrapper from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\") ``` ### SQL Database Chain This example demonstrates the use of the SQL Chain for answering a question over a CnosDB. ```python from langchain.utilities import SQLDatabaseChain db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True) db_chain.run( \"What is the average temperature of air at station XiaoMaiDao between October 19, 2022 and Occtober 20, 2022?\" ) ``` ```shell > Entering new chain... What is the average temperature of air at station XiaoMaiDao between October 19, 2022 and Occtober 20, 2022? SQLQuery:SELECT AVG(temperature) FROM air WHERE station = 'XiaoMaiDao' AND time >= '2022-10-19' AND time Finished chain. ``` ### SQL Database Agent This example demonstrates the use of the SQL Database Agent for answering questions over a CnosDB. ```python from langchain.agents import create_sql_agent from langchain.agents.agent_toolkits import SQLDatabaseToolkit toolkit = SQLDatabaseToolkit(db=db, llm=llm) agent = create_sql_agent(llm=llm, toolkit=toolkit, verbose=True) ``` ```python agent.run( \"What is the average temperature of air at station XiaoMaiDao between October 19, 2022 and Occtober 20, 2022?\" ) ``` ```shell > Entering new chain... Action: sql_db_list_tables Action Input: \"\" Observation: air Thought:The \"air\" table seems relevant to the question. I should query the schema of the \"air\" table to see what columns are available. Action: sql_db_schema Action Input: \"air\" Observation: CREATE TABLE air ( pressure FLOAT, station STRING, temperature FLOAT, time TIMESTAMP, visibility FLOAT ) /* 3 rows from air table: pressure station temperature time visibility 75.0 XiaoMaiDao 67.0 2022-10-19T03:40:00 54.0 77.0 XiaoMaiDao 69.0 2022-10-19T04:40:00 56.0 76.0 XiaoMaiDao 68.0 2022-10-19T05:40:00 55.0 */ Thought:The \"temperature\" column in the \"air\" table is relevant to the question. I can query the average temperature between the specified dates. Action: sql_db_query Action Input: \"SELECT AVG(temperature) FROM air WHERE station = 'XiaoMaiDao' AND time >= '2022-10-19' AND time Finished chain. ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/cohere.mdx"}, "data": "# Cohere >[Cohere]( is a Canadian startup that provides natural language processing models > that help companies improve human-machine interactions. ## Installation and Setup - Install the Python SDK : ```bash pip install cohere ``` Get a [Cohere api key]( and set it as an environment variable (`COHERE_API_KEY`) ## LLM There exists an Cohere LLM wrapper, which you can access with See a [usage example](/docs/integrations/llms/cohere). ```python from langchain.llms import Cohere ``` ## Text Embedding Model There exists an Cohere Embedding model, which you can access with ```python from langchain.embeddings import CohereEmbeddings ``` For a more detailed walkthrough of this, see [this notebook](/docs/integrations/text_embedding/cohere) ## Retriever See a [usage example](/docs/integrations/retrievers/cohere-reranker). ```python from langchain.retrievers.document_compressors import CohereRerank ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/college_confidential.mdx"}, "data": "# College Confidential >[College Confidential]( gives information on 3,800+ colleges and universities. ## Installation and Setup There isn't any special setup for it. ## Document Loader See a [usage example](/docs/integrations/document_loaders/college_confidential). ```python from langchain.document_loaders import CollegeConfidentialLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/confident.mdx"}, "data": "# Confident AI ![Confident - Unit Testing for LLMs]( >[DeepEval]( package for unit testing LLMs. > Using Confident, everyone can build robust language models through faster iterations > using both unit testing and integration testing. We provide support for each step in the iteration > from synthetic data creation to testing. ## Installation and Setup First, you'll need to install the `DeepEval` Python package as follows: ```bash pip install deepeval ``` Afterwards, you can get started in as little as a few lines of code. ```python from langchain.callbacks import DeepEvalCallback ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/confluence.mdx"}, "data": "# Confluence >[Confluence]( is a wiki collaboration platform that saves and organizes all of the project-related material. `Confluence` is a knowledge base that primarily handles content management activities. ## Installation and Setup ```bash pip install atlassian-python-api ``` We need to set up `username/api_key` or `Oauth2 login`. See [instructions]( ## Document Loader See a [usage example](/docs/integrations/document_loaders/confluence). ```python from langchain.document_loaders import ConfluenceLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/ctransformers.mdx"}, "data": "# C Transformers This page covers how to use the [C Transformers]( library within LangChain. It is broken into two parts: installation and setup, and then references to specific C Transformers wrappers. ## Installation and Setup - Install the Python package with `pip install ctransformers` - Download a supported [GGML model]( (see [Supported Models]( ## Wrappers ### LLM There exists a CTransformers LLM wrapper, which you can access with: ```python from langchain.llms import CTransformers ``` It provides a unified interface for all models: ```python llm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2') print(llm('AI is going to')) ``` If you are getting `illegal instruction` error, try using `lib='avx'` or `lib='basic'`: ```py llm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2', lib='avx') ``` It can be used with models hosted on the Hugging Face Hub: ```py llm = CTransformers(model='marella/gpt-2-ggml') ``` If a model repo has multiple model files (`.bin` files), specify a model file using: ```py llm = CTransformers(model='marella/gpt-2-ggml', model_file='ggml-model.bin') ``` Additional parameters can be passed using the `config` parameter: ```py config = {'max_new_tokens': 256, 'repetition_penalty': 1.1} llm = CTransformers(model='marella/gpt-2-ggml', config=config) ``` See [Documentation]( for a list of available parameters. For a more detailed walkthrough of this, see [this notebook](/docs/integrations/llms/ctransformers)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/dashvector.mdx"}, "data": "# DashVector > [DashVector]( is a fully-managed vectorDB service that supports high-dimension dense and sparse vectors, real-time insertion and filtered search. It is built to scale automatically and can adapt to different application requirements. This document demonstrates to leverage DashVector within the LangChain ecosystem. In particular, it shows how to install DashVector, and how to use it as a VectorStore plugin in LangChain. It is broken into two parts: installation and setup, and then references to specific DashVector wrappers. ## Installation and Setup Install the Python SDK: ```bash pip install dashvector ``` ## VectorStore A DashVector Collection is wrapped as a familiar VectorStore for native usage within LangChain, which allows it to be readily used for various scenarios, such as semantic search or example selection. You may import the vectorstore by: ```python from langchain.vectorstores import DashVector ``` For a detailed walkthrough of the DashVector wrapper, please refer to [this notebook](/docs/integrations/vectorstores/dashvector)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/databricks.md"}, "data": "Databricks ========== The [Databricks]( Lakehouse Platform unifies data, analytics, and AI on one platform. Databricks embraces the LangChain ecosystem in various ways: 1. Databricks connector for the SQLDatabase Chain: SQLDatabase.from_databricks() provides an easy way to query your data on Databricks through LangChain 2. Databricks MLflow integrates with LangChain: Tracking and serving LangChain applications with fewer steps 3. Databricks MLflow AI Gateway 4. Databricks as an LLM provider: Deploy your fine-tuned LLMs on Databricks via serving endpoints or cluster driver proxy apps, and query it as langchain.llms.Databricks 5. Databricks Dolly: Databricks open-sourced Dolly which allows for commercial use, and can be accessed through the Hugging Face Hub Databricks connector for the SQLDatabase Chain ---------------------------------------------- You can connect to [Databricks runtimes]( and [Databricks SQL]( using the SQLDatabase wrapper of LangChain. See the notebook [Connect to Databricks](/docs/use_cases/qa_structured/integrations/databricks) for details. Databricks MLflow integrates with LangChain ------------------------------------------- MLflow is an open-source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. See the notebook [MLflow Callback Handler](/docs/integrations/providers/mlflow_tracking) for details about MLflow's integration with LangChain. Databricks provides a fully managed and hosted version of MLflow integrated with enterprise security features, high availability, and other Databricks workspace features such as experiment and run management and notebook revision capture. MLflow on Databricks offers an integrated experience for tracking and securing machine learning model training runs and running machine learning projects. See [MLflow guide]( for more details. Databricks MLflow makes it more convenient to develop LangChain applications on Databricks. For MLflow tracking, you don't need to set the tracking uri. For MLflow Model Serving, you can save LangChain Chains in the MLflow langchain flavor, and then register and serve the Chain with a few clicks on Databricks, with credentials securely managed by MLflow Model Serving. Databricks MLflow AI Gateway ---------------------------- See [MLflow AI Gateway](/docs/integrations/providers/mlflow_ai_gateway). Databricks as an LLM provider ----------------------------- The notebook [Wrap Databricks endpoints as LLMs](/docs/integrations/llms/databricks) illustrates the method to wrap Databricks endpoints as LLMs in LangChain. It supports two types of endpoints: the serving endpoint, which is recommended for both production and development, and the cluster driver proxy app, which is recommended for interactive development. Databricks endpoints support Dolly, but are also great for hosting models like MPT-7B or any other models from the Hugging Face ecosystem. Databricks endpoints can also be used with proprietary models like OpenAI to provide a governance layer for enterprises. Databricks Dolly ---------------- Databricks\u2019 Dolly is an instruction-following large language model trained on the Databricks machine learning platform that is licensed for commercial use. The model is available on Hugging Face Hub as databricks/dolly-v2-12b. See the notebook [Hugging Face Hub](/docs/integrations/llms/huggingface_hub) for instructions to access it through the Hugging Face Hub integration with LangChain."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/datadog.mdx"}, "data": "# Datadog Tracing >[ddtrace]( is a Datadog application performance monitoring (APM) library which provides an integration to monitor your LangChain application. Key features of the ddtrace integration for LangChain: - Traces: Capture LangChain requests, parameters, prompt-completions, and help visualize LangChain operations. - Metrics: Capture LangChain request latency, errors, and token/cost usage (for OpenAI LLMs and chat models). - Logs: Store prompt completion data for each LangChain operation. - Dashboard: Combine metrics, logs, and trace data into a single plane to monitor LangChain requests. - Monitors: Provide alerts in response to spikes in LangChain request latency or error rate. Note: The ddtrace LangChain integration currently provides tracing for LLMs, chat models, Text Embedding Models, Chains, and Vectorstores. ## Installation and Setup 1. Enable APM and StatsD in your Datadog Agent, along with a Datadog API key. For example, in Docker: ``` docker run -d --cgroupns host \\ --pid host \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ -v /proc/:/host/proc/:ro \\ -v /sys/fs/cgroup/:/host/sys/fs/cgroup:ro \\ -e DD_API_KEY= \\ -p 127.0.0.1:8126:8126/tcp \\ -p 127.0.0.1:8125:8125/udp \\ -e DD_DOGSTATSD_NON_LOCAL_TRAFFIC=true \\ -e DD_APM_ENABLED=true \\ gcr.io/datadoghq/agent:latest ``` 2. Install the Datadog APM Python library. ``` pip install ddtrace>=1.17 ``` 3. The LangChain integration can be enabled automatically when you prefix your LangChain Python application command with `ddtrace-run`: ``` DD_SERVICE=\"my-service\" DD_ENV=\"staging\" DD_API_KEY= ddtrace-run python .py ``` **Note**: If the Agent is using a non-default hostname or port, be sure to also set `DD_AGENT_HOST`, `DD_TRACE_AGENT_PORT`, or `DD_DOGSTATSD_PORT`. Additionally, the LangChain integration can be enabled programmatically by adding `patch_all()` or `patch(langchain=True)` before the first import of `langchain` in your application. Note that using `ddtrace-run` or `patch_all()` will also enable the `requests` and `aio integrations which trace HTTP requests to LLM providers, as well as the `openai` integration which traces requests to the OpenAI library. ```python from ddtrace import config, patch # Note: be sure to configure the integration before calling ``patch()``! # e.g. config.langchain[\"logs_enabled\"] = True patch(langchain=True) # to trace synchronous HTTP requests # patch(langchain=True, requests=True) # to trace asynchronous HTTP requests (to the OpenAI library) # patch(langchain=True, aio # to include underlying OpenAI spans from the OpenAI integration # patch(langchain=True, openai=True)patch_all ``` See the [APM Python library documentation][ for more advanced usage. ## Configuration See the [APM Python library documentation][ for all the available configuration options. ### Log Prompt & Completion Sampling To enable log prompt and completion sampling, set the `DD_LANGCHAIN_LOGS_ENABLED=1` environment variable. By default, 10% of traced requests will emit logs containing the prompts and completions. To adjust the log sample rate, see the [APM library documentation][ **Note**: Logs submission requires `DD_API_KEY` to be specified when running `ddtrace-run`. ## Troubleshooting Need help? Create an issue on [ddtrace]( or contact [Datadog support]["}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/datadog_logs.mdx"}, "data": "# Datadog Logs >[Datadog]( is a monitoring and analytics platform for cloud-scale applications. ## Installation and Setup ```bash pip install datadog_api_client ``` We must initialize the loader with the Datadog API key and APP key, and we need to set up the query to extract the desired logs. ## Document Loader See a [usage example](/docs/integrations/document_loaders/datadog_logs). ```python from langchain.document_loaders import DatadogLogsLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/dataforseo.mdx"}, "data": "# DataForSEO This page provides instructions on how to use the DataForSEO search APIs within LangChain. ## Installation and Setup - Get a DataForSEO API Access login and password, and set them as environment variables (`DATAFORSEO_LOGIN` and `DATAFORSEO_PASSWORD` respectively). You can find it in your dashboard. ## Wrappers ### Utility The DataForSEO utility wraps the API. To import this utility, use: ```python from langchain.utilities.dataforseo_api_search import DataForSeoAPIWrapper ``` For a detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/dataforseo.ipynb). ### Tool You can also load this wrapper as a Tool to use with an Agent: ```python from langchain.agents import load_tools tools = load_tools([\"dataforseo-api-search\"]) ``` ## Example usage ```python dataforseo = DataForSeoAPIWrapper(api_login=\"your_login\", api_password=\"your_password\") result = dataforseo.run(\"Bill Gates\") print(result) ``` ## Environment Variables You can store your DataForSEO API Access login and password as environment variables. The wrapper will automatically check for these environment variables if no values are provided: ```python import os os.environ[\"DATAFORSEO_LOGIN\"] = \"your_login\" os.environ[\"DATAFORSEO_PASSWORD\"] = \"your_password\" dataforseo = DataForSeoAPIWrapper() result = dataforseo.run(\"weather in Los Angeles\") print(result) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/deepinfra.mdx"}, "data": "# DeepInfra This page covers how to use the DeepInfra ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific DeepInfra wrappers. ## Installation and Setup - Get your DeepInfra api key from this link [here]( - Get an DeepInfra api key and set it as an environment variable (`DEEPINFRA_API_TOKEN`) ## Available Models DeepInfra provides a range of Open Source LLMs ready for deployment. You can list supported models for [text-generation]( and [embeddings]( google/flan\\* models can be viewed [here]( You can view a [list of request and response parameters]( ## Wrappers ### LLM There exists an DeepInfra LLM wrapper, which you can access with ```python from langchain.llms import DeepInfra ``` ### Embeddings There is also an DeepInfra Embeddings wrapper, you can access with ```python from langchain.embeddings import DeepInfraEmbeddings ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/deepsparse.mdx"}, "data": "# DeepSparse This page covers how to use the [DeepSparse]( inference runtime within LangChain. It is broken into two parts: installation and setup, and then examples of DeepSparse usage. ## Installation and Setup - Install the Python package with `pip install deepsparse` - Choose a [SparseZoo model]( or export a support model to ONNX [using Optimum]( ## Wrappers ### LLM There exists a DeepSparse LLM wrapper, which you can access with: ```python from langchain.llms import DeepSparse ``` It provides a unified interface for all models: ```python llm = DeepSparse(model='zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base-none') print(llm('def fib():')) ``` Additional parameters can be passed using the `config` parameter: ```python config = {'max_generated_tokens': 256} llm = DeepSparse(model='zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base-none', config=config) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/diffbot.mdx"}, "data": "# Diffbot >[Diffbot]( is a service to read web pages. Unlike traditional web scraping tools, > `Diffbot` doesn't require any rules to read the content on a page. >It starts with computer vision, which classifies a page into one of 20 possible types. Content is then interpreted by a machine learning model trained to identify the key attributes on a page based on its type. >The result is a website transformed into clean-structured data (like JSON or CSV), ready for your application. ## Installation and Setup Read [instructions]( how to get the Diffbot API Token. ## Document Loader See a [usage example](/docs/integrations/document_loaders/diffbot). ```python from langchain.document_loaders import DiffbotLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/dingo.mdx"}, "data": "# DingoDB This page covers how to use the DingoDB ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific DingoDB wrappers. ## Installation and Setup - Install the Python SDK with `pip install dingodb` ## VectorStore There exists a wrapper around DingoDB indexes, allowing you to use it as a vectorstore, whether for semantic search or example selection. To import this vectorstore: ```python from langchain.vectorstores import Dingo ``` For a more detailed walkthrough of the DingoDB wrapper, see [this notebook](/docs/integrations/vectorstores/dingo)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/discord.mdx"}, "data": "# Discord >[Discord]( is a VoIP and instant messaging social platform. Users have the ability to communicate > with voice calls, video calls, text messaging, media and files in private chats or as part of communities called > \"servers\". A server is a collection of persistent chat rooms and voice channels which can be accessed via invite links. ## Installation and Setup ```bash pip install pandas ``` Follow these steps to download your `Discord` data: 1. Go to your **User Settings** 2. Then go to **Privacy and Safety** 3. Head over to the **Request all of my Data** and click on **Request Data** button It might take 30 days for you to receive your data. You'll receive an email at the address which is registered with Discord. That email will have a download button using which you would be able to download your personal Discord data. ## Document Loader See a [usage example](/docs/integrations/document_loaders/discord). ```python from langchain.document_loaders import DiscordChatLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/docarray.mdx"}, "data": "# DocArray > [DocArray]( is a library for nested, unstructured, multimodal data in transit, > including text, image, audio, video, 3D mesh, etc. It allows deep-learning engineers to efficiently process, > embed, search, recommend, store, and transfer multimodal data with a Pythonic API. ## Installation and Setup We need to install `docarray` python package. ```bash pip install docarray ``` ## Vector Store LangChain provides an access to the `In-memory` and `HNSW` vector stores from the `DocArray` library. See a [usage example](/docs/integrations/vectorstores/docarray_hnsw). ```python from langchain.vectorstores DocArrayHnswSearch ``` See a [usage example](/docs/integrations/vectorstores/docarray_in_memory). ```python from langchain.vectorstores DocArrayInMemorySearch ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/doctran.mdx"}, "data": "# Doctran >[Doctran]( is a python package. It uses LLMs and open-source > NLP libraries to transform raw text into clean, structured, information-dense documents > that are optimized for vector space retrieval. You can think of `Doctran` as a black box where > messy strings go in and nice, clean, labelled strings come out. ## Installation and Setup ```bash pip install doctran ``` ## Document Transformers ### Document Interrogator See a [usage example for DoctranQATransformer](/docs/integrations/document_transformers/doctran_interrogate_document). ```python from langchain.document_loaders import DoctranQATransformer ``` ### Property Extractor See a [usage example for DoctranPropertyExtractor](/docs/integrations/document_transformers/doctran_extract_properties). ```python from langchain.document_loaders import DoctranPropertyExtractor ``` ### Document Translator See a [usage example for DoctranTextTranslator](/docs/integrations/document_transformers/doctran_translate_document). ```python from langchain.document_loaders import DoctranTextTranslator ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/docugami.mdx"}, "data": "# Docugami >[Docugami]( converts business documents into a Document XML Knowledge Graph, generating forests > of XML semantic trees representing entire documents. This is a rich representation that includes the semantic and > structural characteristics of various chunks in the document as an XML tree. ## Installation and Setup ```bash pip install lxml ``` ## Document Loader See a [usage example](/docs/integrations/document_loaders/docugami). ```python from langchain.document_loaders import DocugamiLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/duckdb.mdx"}, "data": "# DuckDB >[DuckDB]( is an in-process SQL OLAP database management system. ## Installation and Setup First, you need to install `duckdb` python package. ```bash pip install duckdb ``` ## Document Loader See a [usage example](/docs/integrations/document_loaders/duckdb). ```python from langchain.document_loaders import DuckDBLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/elasticsearch.mdx"}, "data": "# Elasticsearch > [Elasticsearch]( is a distributed, RESTful search and analytics engine. > It provides a distributed, multi-tenant-capable full-text search engine with an HTTP web interface and schema-free > JSON documents. ## Installation and Setup There are two ways to get started with Elasticsearch: #### Install Elasticsearch on your local machine via docker Example: Run a single-node Elasticsearch instance with security disabled. This is not recommended for production use. ```bash docker run -p 9200:9200 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" -e \"xpack.security. docker.elastic.co/elasticsearch/elasticsearch:8.9.0 ``` #### Deploy Elasticsearch on Elastic Cloud Elastic Cloud is a managed Elasticsearch service. Signup for a [free trial]( ### Install Client ```bash pip install elasticsearch ``` ## Vector Store The vector store is a simple wrapper around Elasticsearch. It provides a simple interface to store and retrieve vectors. ```python from langchain.vectorstores import ElasticsearchStore from langchain.document_loaders import TextLoader from langchain.text_splitter import CharacterTextSplitter loader = TextLoader(\"./state_of_the_union.txt\") documents = loader.load() text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0) docs = text_splitter.split_documents(documents) embeddings = OpenAIEmbeddings() db = ElasticsearchStore.from_documents( docs, embeddings, es_url=\" index_name=\"test-basic\", ) db.client.indices.refresh(index=\"test-basic\") query = \"What did the president say about Ketanji Brown Jackson\" results = db.similarity_search(query) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/epsilla.mdx"}, "data": "# Epsilla This page covers how to use [Epsilla]( within LangChain. It is broken into two parts: installation and setup, and then references to specific Epsilla wrappers. ## Installation and Setup - Install the Python SDK with `pip/pip3 install pyepsilla` ## Wrappers ### VectorStore There exists a wrapper around Epsilla vector databases, allowing you to use it as a vectorstore, whether for semantic search or example selection. To import this vectorstore: ```python from langchain.vectorstores import Epsilla ``` For a more detailed walkthrough of the Epsilla wrapper, see [this notebook](/docs/integrations/vectorstores/epsilla)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/evernote.mdx"}, "data": "# EverNote >[EverNote]( is intended for archiving and creating notes in which photos, audio and saved web content can be embedded. Notes are stored in virtual \"notebooks\" and can be tagged, annotated, edited, searched, and exported. ## Installation and Setup First, you need to install `lxml` and `html2text` python packages. ```bash pip install lxml pip install html2text ``` ## Document Loader See a [usage example](/docs/integrations/document_loaders/evernote). ```python from langchain.document_loaders import EverNoteLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/facebook_chat.mdx"}, "data": "# Facebook Chat >[Messenger]( is an American proprietary instant messaging app and > platform developed by `Meta Platforms`. Originally developed as `Facebook Chat` in 2008, the company revamped its > messaging service in 2010. ## Installation and Setup First, you need to install `pandas` python package. ```bash pip install pandas ``` ## Document Loader See a [usage example](/docs/integrations/document_loaders/facebook_chat). ```python from langchain.document_loaders import FacebookChatLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/facebook_faiss.mdx"}, "data": "# Facebook Faiss >[Facebook AI Similarity Search (Faiss)]( > is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that > search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting > code for evaluation and parameter tuning. [Faiss documentation]( ## Installation and Setup We need to install `faiss` python package. ```bash pip install faiss-gpu # For CUDA 7.5+ supported GPU's. ``` OR ```bash pip install faiss-cpu # For CPU Installation ``` ## Vector Store See a [usage example](/docs/integrations/vectorstores/faiss). ```python from langchain.vectorstores import FAISS ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/figma.mdx"}, "data": "# Figma >[Figma]( is a collaborative web application for interface design. ## Installation and Setup The Figma API requires an `access token`, `node_ids`, and a `file key`. The `file key` can be pulled from the URL. `Node IDs` are also available in the URL. Click on anything and look for the '?node-id={node_id}' param. `Access token` [instructions]( ## Document Loader See a [usage example](/docs/integrations/document_loaders/figma). ```python from langchain.document_loaders import FigmaFileLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/fireworks.md"}, "data": "# Fireworks This page covers how to use [Fireworks]( models within Langchain. ## Installation and setup - Install the Fireworks client library. ``` pip install fireworks-ai ``` - Get a Fireworks API key by signing up at [app.fireworks.ai]( - Authenticate by setting the FIREWORKS_API_KEY environment variable. ## Authentication There are two ways to authenticate using your Fireworks API key: 1. Setting the `FIREWORKS_API_KEY` environment variable. ```python os.environ[\"FIREWORKS_API_KEY\"] = \"\" ``` 2. Setting `fireworks_api_key` field in the Fireworks LLM module. ```python llm = Fireworks(fireworks_api_key=\"\") ``` ## Using the Fireworks LLM module Fireworks integrates with Langchain through the LLM module. In this example, we will work the llama-v2-13b-chat model. ```python from langchain.llms.fireworks import Fireworks llm = Fireworks( fireworks_api_key=\"\", model=\"accounts/fireworks/models/llama-v2-13b-chat\", max_tokens=256) llm(\"Name 3 sports.\") ``` For a more detailed walkthrough, see [here](/docs/integrations/llms/Fireworks)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/flyte.mdx"}, "data": "# Flyte > [Flyte]( is an open-source orchestrator that facilitates building production-grade data and ML pipelines. > It is built for scalability and reproducibility, leveraging Kubernetes as its underlying platform. The purpose of this notebook is to demonstrate the integration of a `FlyteCallback` into your Flyte task, enabling you to effectively monitor and track your LangChain experiments. ## Installation & Setup - Install the Flytekit library by running the command `pip install flytekit`. - Install the Flytekit-Envd plugin by running the command `pip install flytekitplugins-envd`. - Install LangChain by running the command `pip install langchain`. - Install [Docker]( on your system. ## Flyte Tasks A Flyte [task]( serves as the foundational building block of Flyte. To execute LangChain experiments, you need to write Flyte tasks that define the specific steps and operations involved. NOTE: The [getting started guide]( offers detailed, step-by-step instructions on installing Flyte locally and running your initial Flyte pipeline. First, import the necessary dependencies to support your LangChain experiments. ```python import os from flytekit import ImageSpec, task from langchain.agents import AgentType, initialize_agent, load_tools from langchain.callbacks import FlyteCallbackHandler from langchain.chains import LLMChain from langchain.chat_models import ChatOpenAI from langchain.prompts import PromptTemplate from langchain.schema import HumanMessage ``` Set up the necessary environment variables to utilize the OpenAI API and Serp API: ```python # Set OpenAI API key os.environ[\"OPENAI_API_KEY\"] = \"\" # Set Serp API key os.environ[\"SERPAPI_API_KEY\"] = \"\" ``` Replace `` and `` with your respective API keys obtained from OpenAI and Serp API. To guarantee reproducibility of your pipelines, Flyte tasks are containerized. Each Flyte task must be associated with an image, which can either be shared across the entire Flyte [workflow]( or provided separately for each task. To streamline the process of supplying the required dependencies for each Flyte task, you can initialize an [`ImageSpec`]( object. This approach automatically triggers a Docker build, alleviating the need for users to manually create a Docker image. ```python custom_image = ImageSpec( name=\"langchain-flyte\", packages=[ \"langchain\", \"openai\", \"spacy\", \" \"textstat\", \"google-search-results\", ], registry=\"\", ) ``` You have the flexibility to push the Docker image to a registry of your preference. [Docker Hub]( or [GitHub Container Registry (GHCR)]( is a convenient option to begin with. Once you have selected a registry, you can proceed to create Flyte tasks that log the LangChain metrics to Flyte Deck. The following examples demonstrate tasks related to OpenAI LLM, chains and agent with tools: ### LLM ```python @task(disable_deck=False, container_image=custom_image) def langchain_llm() -> str: llm = ChatOpenAI( model_name=\"gpt-3.5-turbo\", temperature=0.2, callbacks=[FlyteCallbackHandler()], ) return llm([HumanMessage(content=\"Tell me a joke\")]).content ``` ### Chain ```python @task(disable_deck=False, container_image=custom_image) def langchain_chain() -> list[dict[str, str]]: template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title. Title: {title} Playwright: This is a synopsis for the above play:\"\"\" llm = ChatOpenAI( model_name=\"gpt-3.5-turbo\", temperature=0, callbacks=[FlyteCallbackHandler()], ) prompt_template = PromptTemplate(input_variables=[\"title\"], template=template) synopsis_chain = LLMChain( llm=llm, prompt=prompt_template, callbacks=[FlyteCallbackHandler()] ) test_prompts = [ { \"title\": \"documentary about good video games that push the boundary of game design\" }, ] return synopsis_chain.apply(test_prompts) ``` ### Agent ```python @task(disable_deck=False, container_image=custom_image) def langchain_agent() -> str: llm = OpenAI( model_name=\"gpt-3.5-turbo\", temperature=0, callbacks=[FlyteCallbackHandler()], ) tools = load_tools( [\"serpapi\", \"llm-math\"], llm=llm, callbacks=[FlyteCallbackHandler()] ) agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, callbacks=[FlyteCallbackHandler()], verbose=True, ) return agent.run( \"Who is Leonardo DiCaprio's girlfriend? Could you calculate her current age and raise it to the power of 0.43?\" ) ``` These tasks serve as a starting point for running your LangChain experiments within Flyte. ## Execute the Flyte Tasks on Kubernetes To execute the Flyte tasks on the configured Flyte backend, use the following command: ```bash pyflyte run --image langchain_flyte.py langchain_llm ``` This command will initiate the execution of the `langchain_llm` task on the Flyte backend. You can trigger the remaining two tasks in a similar manner. The metrics will be displayed on the Flyte UI as follows: ![LangChain LLM]("}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/forefrontai.mdx"}, "data": "# ForefrontAI This page covers how to use the ForefrontAI ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific ForefrontAI wrappers. ## Installation and Setup - Get an ForefrontAI api key and set it as an environment variable (`FOREFRONTAI_API_KEY`) ## Wrappers ### LLM There exists an ForefrontAI LLM wrapper, which you can access with ```python from langchain.llms import ForefrontAI ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/git.mdx"}, "data": "# Git >[Git]( is a distributed version control system that tracks changes in any set of computer files, usually used for coordinating work among programmers collaboratively developing source code during software development. ## Installation and Setup First, you need to install `GitPython` python package. ```bash pip install GitPython ``` ## Document Loader See a [usage example](/docs/integrations/document_loaders/git). ```python from langchain.document_loaders import GitLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/gitbook.mdx"}, "data": "# GitBook >[GitBook]( is a modern documentation platform where teams can document everything from products to internal knowledge bases and APIs. ## Installation and Setup There isn't any special setup for it. ## Document Loader See a [usage example](/docs/integrations/document_loaders/gitbook). ```python from langchain.document_loaders import GitbookLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/golden.mdx"}, "data": "# Golden >[Golden]( provides a set of natural language APIs for querying and enrichment using the Golden Knowledge Graph e.g. queries such as: `Products from OpenAI`, `Generative ai companies with series a funding`, and `rappers who invest` can be used to retrieve structured data about relevant entities. > >The `golden-query` langchain tool is a wrapper on top of the [Golden Query API]( which enables programmatic access to these results. >See the [Golden Query API docs]( for more information. ## Installation and Setup - Go to the [Golden API docs]( to get an overview about the Golden API. - Get your API key from the [Golden API Settings]( page. - Save your API key into GOLDEN_API_KEY env variable ## Wrappers ### Utility There exists a GoldenQueryAPIWrapper utility which wraps this API. To import this utility: ```python from langchain.utilities.golden_query import GoldenQueryAPIWrapper ``` For a more detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/golden_query). ### Tool You can also easily load this wrapper as a Tool (to use with an Agent). You can do this with: ```python from langchain.agents import load_tools tools = load_tools([\"golden-query\"]) ``` For more information on tools, see [this page](/docs/modules/agents/tools/)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/google_serper.mdx"}, "data": "# Serper - Google Search API This page covers how to use the [Serper]( Google Search API within LangChain. Serper is a low-cost Google Search API that can be used to add answer box, knowledge graph, and organic results data from Google Search. It is broken into two parts: setup, and then references to the specific Google Serper wrapper. ## Setup - Go to [serper.dev]( to sign up for a free account - Get the api key and set it as an environment variable (`SERPER_API_KEY`) ## Wrappers ### Utility There exists a GoogleSerperAPIWrapper utility which wraps this API. To import this utility: ```python from langchain.utilities import GoogleSerperAPIWrapper ``` You can use it as part of a Self Ask chain: ```python from langchain.utilities import GoogleSerperAPIWrapper from langchain.llms.openai import OpenAI from langchain.agents import initialize_agent, Tool from langchain.agents import AgentType import os os.environ[\"SERPER_API_KEY\"] = \"\" os.environ['OPENAI_API_KEY'] = \"\" llm = OpenAI(temperature=0) search = GoogleSerperAPIWrapper() tools = [ Tool( name=\"Intermediate Answer\", func=search.run, description=\"useful for when you need to ask with search\" ) ] self_ask_with_search = initialize_agent(tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True) self_ask_with_search.run(\"What is the hometown of the reigning men's U.S. Open champion?\") ``` #### Output ``` Entering new AgentExecutor chain... Yes. Follow up: Who is the reigning men's U.S. Open champion? Intermediate answer: Current champions Carlos Alcaraz, 2022 men's singles champion. Follow up: Where is Carlos Alcaraz from? Intermediate answer: El Palmar, Spain So the final answer is: El Palmar, Spain > Finished chain. 'El Palmar, Spain' ``` For a more detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/google_serper). ### Tool You can also easily load this wrapper as a Tool (to use with an Agent). You can do this with: ```python from langchain.agents import load_tools tools = load_tools([\"google-serper\"]) ``` For more information on tools, see [this page](/docs/modules/agents/tools/)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/gooseai.mdx"}, "data": "# GooseAI This page covers how to use the GooseAI ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific GooseAI wrappers. ## Installation and Setup - Install the Python SDK with `pip install openai` - Get your GooseAI api key from this link [here]( - Set the environment variable (`GOOSEAI_API_KEY`). ```python import os os.environ[\"GOOSEAI_API_KEY\"] = \"YOUR_API_KEY\" ``` ## Wrappers ### LLM There exists an GooseAI LLM wrapper, which you can access with: ```python from langchain.llms import GooseAI ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/gpt4all.mdx"}, "data": "# GPT4All This page covers how to use the `GPT4All` wrapper within LangChain. The tutorial is divided into two parts: installation and setup, followed by usage with an example. ## Installation and Setup - Install the Python package with `pip install pyllamacpp` - Download a [GPT4All model]( and place it in your desired directory ## Usage ### GPT4All To use the GPT4All wrapper, you need to provide the path to the pre-trained model file and the model's configuration. ```python from langchain.llms import GPT4All # Instantiate the model. Callbacks support token-wise streaming model = GPT4All(model=\"./models/gpt4all-model.bin\", n_ctx=512, n_threads=8) # Generate text response = model(\"Once upon a time, \") ``` You can also customize the generation parameters, such as n_predict, temp, top_p, top_k, and others. To stream the model's predictions, add in a CallbackManager. ```python from langchain.llms import GPT4All from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler # There are many CallbackHandlers supported, such as # from langchain.callbacks.streamlit import StreamlitCallbackHandler callbacks = [StreamingStdOutCallbackHandler()] model = GPT4All(model=\"./models/gpt4all-model.bin\", n_ctx=512, n_threads=8) # Generate text. Tokens are streamed through the callback manager. model(\"Once upon a time, \", callbacks=callbacks) ``` ## Model File You can find links to model file downloads in the [pyllamacpp]( repository. For a more detailed walkthrough of this, see [this notebook](/docs/integrations/llms/gpt4all)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/gradient.mdx"}, "data": "# Gradient >[Gradient]( allows to fine tune and get completions on LLMs with a simple web API. ## Installation and Setup - Install the Python SDK : ```bash pip install gradientai ``` Get a [Gradient access token and workspace]( and set it as an environment variable (`Gradient_ACCESS_TOKEN`) and (`GRADIENT_WORKSPACE_ID`) ## LLM There exists an Gradient LLM wrapper, which you can access with See a [usage example](/docs/integrations/llms/gradient). ```python from langchain.llms import GradientLLM ``` ## Text Embedding Model There exists an Gradient Embedding model, which you can access with ```python from langchain.embeddings import GradientEmbeddings ``` For a more detailed walkthrough of this, see [this notebook](/docs/integrations/text_embedding/gradient)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/graphsignal.mdx"}, "data": "# Graphsignal This page covers how to use [Graphsignal]( to trace and monitor LangChain. Graphsignal enables full visibility into your application. It provides latency breakdowns by chains and tools, exceptions with full context, data monitoring, compute/GPU utilization, OpenAI cost analytics, and more. ## Installation and Setup - Install the Python library with `pip install graphsignal` - Create free Graphsignal account [here]( - Get an API key and set it as an environment variable (`GRAPHSIGNAL_API_KEY`) ## Tracing and Monitoring Graphsignal automatically instruments and starts tracing and monitoring chains. Traces and metrics are then available in your [Graphsignal dashboards]( Initialize the tracer by providing a deployment name: ```python import graphsignal graphsignal.configure(deployment='my-langchain-app-prod') ``` To additionally trace any function or code, you can use a decorator or a context manager: ```python @graphsignal.trace_function def handle_request(): chain.run(\"some initial text\") ``` ```python with graphsignal.start_trace('my-chain'): chain.run(\"some initial text\") ``` Optionally, enable profiling to record function-level statistics for each trace. ```python with graphsignal.start_trace( 'my-chain', options=graphsignal.TraceOptions(enable_profiling=True)): chain.run(\"some initial text\") ``` See the [Quick Start]( guide for complete setup instructions."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/grobid.mdx"}, "data": "# Grobid GROBID is a machine learning library for extracting, parsing, and re-structuring raw documents. It is designed and expected to be used to parse academic papers, where it works particularly well. *Note*: if the articles supplied to Grobid are large documents (e.g. dissertations) exceeding a certain number of elements, they might not be processed. This page covers how to use the Grobid to parse articles for LangChain. ## Installation The grobid installation is described in details in However, it is probably easier and less troublesome to run grobid through a docker container, as documented [here]( ## Use Grobid with LangChain Once grobid is installed and up and running (you can check by accessing it you're ready to go. You can now use the GrobidParser to produce documents ```python from langchain.document_loaders.parsers import GrobidParser from langchain.document_loaders.generic import GenericLoader #Produce chunks from article paragraphs loader = GenericLoader.from_filesystem( \"/Users/31treehaus/Desktop/Papers/\", glob=\"*\", suffixes=[\".pdf\"], parser= GrobidParser(segment_sentences=False) ) docs = loader.load() #Produce chunks from article sentences loader = GenericLoader.from_filesystem( \"/Users/31treehaus/Desktop/Papers/\", glob=\"*\", suffixes=[\".pdf\"], parser= GrobidParser(segment_sentences=True) ) docs = loader.load() ``` Chunk metadata will include Bounding Boxes. Although these are a bit funky to parse, they are explained in"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/gutenberg.mdx"}, "data": "# Gutenberg >[Project Gutenberg]( is an online library of free eBooks. ## Installation and Setup There isn't any special setup for it. ## Document Loader See a [usage example](/docs/integrations/document_loaders/gutenberg). ```python from langchain.document_loaders import GutenbergLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/hacker_news.mdx"}, "data": "# Hacker News >[Hacker News]( (sometimes abbreviated as `HN`) is a social news > website focusing on computer science and entrepreneurship. It is run by the investment fund and startup > incubator `Y Combinator`. In general, content that can be submitted is defined as \"anything that gratifies > one's intellectual curiosity.\" ## Installation and Setup There isn't any special setup for it. ## Document Loader See a [usage example](/docs/integrations/document_loaders/hacker_news). ```python from langchain.document_loaders import HNLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/hazy_research.mdx"}, "data": "# Hazy Research This page covers how to use the Hazy Research ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Hazy Research wrappers. ## Installation and Setup - To use the `manifest`, install it with `pip install manifest-ml` ## Wrappers ### LLM There exists an LLM wrapper around Hazy Research's `manifest` library. `manifest` is a python library which is itself a wrapper around many model providers, and adds in caching, history, and more. To use this wrapper: ```python from langchain.llms.manifest import ManifestWrapper ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/helicone.mdx"}, "data": "# Helicone This page covers how to use the [Helicone]( ecosystem within LangChain. ## What is Helicone? Helicone is an [open-source]( observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage. ![Helicone](/img/HeliconeDashboard.png) ## Quick start With your LangChain environment you can just add the following parameter. ```bash export OPENAI_API_BASE=\" ``` Now head over to [helicone.ai]( to create your account, and add your OpenAI API key within our dashboard to view your logs. ![Helicone](/img/HeliconeKeys.png) ## How to enable Helicone caching ```python from langchain.llms import OpenAI import openai openai.api_base = \" llm = OpenAI(temperature=0.9, headers={\"Helicone-Cache-Enabled\": \"true\"}) text = \"What is a helicone?\" print(llm(text)) ``` [Helicone caching docs]( ## How to use Helicone custom properties ```python from langchain.llms import OpenAI import openai openai.api_base = \" llm = OpenAI(temperature=0.9, headers={ \"Helicone-Property-Session\": \"24\", \"Helicone-Property-Conversation\": \"support_issue_2\", \"Helicone-Property-App\": \"mobile\", }) text = \"What is a helicone?\" print(llm(text)) ``` [Helicone property docs]("}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/hologres.mdx"}, "data": "# Hologres >[Hologres]( is a unified real-time data warehousing service developed by Alibaba Cloud. You can use Hologres to write, update, process, and analyze large amounts of data in real time. >`Hologres` supports standard `SQL` syntax, is compatible with `PostgreSQL`, and supports most PostgreSQL functions. Hologres supports online analytical processing (OLAP) and ad hoc analysis for up to petabytes of data, and provides high-concurrency and low-latency online data services. >`Hologres` provides **vector database** functionality by adopting [Proxima]( >`Proxima` is a high-performance software library developed by `Alibaba DAMO Academy`. It allows you to search for the nearest neighbors of vectors. Proxima provides higher stability and performance than similar open-source software such as Faiss. Proxima allows you to search for similar text or image embeddings with high throughput and low latency. Hologres is deeply integrated with Proxima to provide a high-performance vector search service. ## Installation and Setup Click [here]( to fast deploy a Hologres cloud instance. ```bash pip install psycopg2 ``` ## Vector Store See a [usage example](/docs/integrations/vectorstores/hologres). ```python from langchain.vectorstores import Hologres ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/html2text.mdx"}, "data": "# HTML to text >[html2text]( is a Python package that converts a page of `HTML` into clean, easy-to-read plain `ASCII text`. The ASCII also happens to be a valid `Markdown` (a text-to-HTML format). ## Installation and Setup ```bash pip install html2text ``` ## Document Transformer See a [usage example](/docs/integrations/document_transformers/html2text). ```python from langchain.document_loaders import Html2TextTransformer ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/huggingface.mdx"}, "data": "# Hugging Face This page covers how to use the Hugging Face ecosystem (including the [Hugging Face Hub]( within LangChain. It is broken into two parts: installation and setup, and then references to specific Hugging Face wrappers. ## Installation and Setup If you want to work with the Hugging Face Hub: - Install the Hub client library with `pip install huggingface_hub` - Create a Hugging Face account (it's free!) - Create an [access token]( and set it as an environment variable (`HUGGINGFACEHUB_API_TOKEN`) If you want work with the Hugging Face Python libraries: - Install `pip install transformers` for working with models and tokenizers - Install `pip install datasets` for working with datasets ## Wrappers ### LLM There exists two Hugging Face LLM wrappers, one for a local pipeline and one for a model hosted on Hugging Face Hub. Note that these wrappers only work for models that support the following tasks: [`text2text-generation`]( [`text-generation`]( To use the local pipeline wrapper: ```python from langchain.llms import HuggingFacePipeline ``` To use a the wrapper for a model hosted on Hugging Face Hub: ```python from langchain.llms import HuggingFaceHub ``` For a more detailed walkthrough of the Hugging Face Hub wrapper, see [this notebook](/docs/integrations/llms/huggingface_hub) ### Embeddings There exists two Hugging Face Embeddings wrappers, one for a local model and one for a model hosted on Hugging Face Hub. Note that these wrappers only work for [`sentence-transformers` models]( To use the local pipeline wrapper: ```python from langchain.embeddings import HuggingFaceEmbeddings ``` To use a the wrapper for a model hosted on Hugging Face Hub: ```python from langchain.embeddings import HuggingFaceHubEmbeddings ``` For a more detailed walkthrough of this, see [this notebook](/docs/integrations/text_embedding/huggingfacehub) ### Tokenizer There are several places you can use tokenizers available through the `transformers` package. By default, it is used to count tokens for all LLMs. You can also use it to count tokens when splitting documents with ```python from langchain.text_splitter import CharacterTextSplitter CharacterTextSplitter.from_huggingface_tokenizer(...) ``` For a more detailed walkthrough of this, see [this notebook](/docs/modules/data_connection/document_transformers/text_splitters/huggingface_length_function) ### Datasets The Hugging Face Hub has lots of great [datasets]( that can be used to evaluate your LLM chains. For a detailed walkthrough of how to use them to do so, see [this notebook](/docs/integrations/document_loaders/hugging_face_dataset)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/ifixit.mdx"}, "data": "# iFixit >[iFixit]( is the largest, open repair community on the web. The site contains nearly 100k > repair manuals, 200k Questions & Answers on 42k devices, and all the data is licensed under `CC-BY-NC-SA 3.0`. ## Installation and Setup There isn't any special setup for it. ## Document Loader See a [usage example](/docs/integrations/document_loaders/ifixit). ```python from langchain.document_loaders import IFixitLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/imsdb.mdx"}, "data": "# IMSDb >[IMSDb]( is the `Internet Movie Script Database`. > ## Installation and Setup There isn't any special setup for it. ## Document Loader See a [usage example](/docs/integrations/document_loaders/imsdb). ```python from langchain.document_loaders import IMSDbLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/infino.mdx"}, "data": "# Infino >[Infino]( is an open-source observability platform that stores both metrics and application logs together. Key features of `Infino` include: - **Metrics Tracking**: Capture time taken by LLM model to handle request, errors, number of tokens, and costing indication for the particular LLM. - **Data Tracking**: Log and store prompt, request, and response data for each LangChain interaction. - **Graph Visualization**: Generate basic graphs over time, depicting metrics such as request duration, error occurrences, token count, and cost. ## Installation and Setup First, you'll need to install the `infinopy` Python package as follows: ```bash pip install infinopy ``` If you already have an `Infino Server` running, then you're good to go; but if you don't, follow the next steps to start it: - Make sure you have Docker installed - Run the following in your terminal: ``` docker run --rm --detach --name infino-example -p 3000:3000 infinohq/infino:latest ``` ## Using Infino See a [usage example of `InfinoCallbackHandler`](/docs/integrations/callbacks/infino). ```python from langchain.callbacks import InfinoCallbackHandler ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/javelin_ai_gateway.mdx"}, "data": "# Javelin AI Gateway [The Javelin AI Gateway]( service is a high-performance, enterprise grade API Gateway for AI applications. It is designed to streamline the usage and access of various large language model (LLM) providers, such as OpenAI, Cohere, Anthropic and custom large language models within an organization by incorporating robust access security for all interactions with LLMs. Javelin offers a high-level interface that simplifies the interaction with LLMs by providing a unified endpoint to handle specific LLM related requests. See the Javelin AI Gateway [documentation]( for more details. [Javelin Python SDK]( is an easy to use client library meant to be embedded into AI Applications ## Installation and Setup Install `javelin_sdk` to interact with Javelin AI Gateway: ```sh pip install 'javelin_sdk' ``` Set the Javelin's API key as an environment variable: ```sh export JAVELIN_API_KEY=... ``` ## Completions Example ```python from langchain.chains import LLMChain from langchain.llms import JavelinAIGateway from langchain.prompts import PromptTemplate route_completions = \"eng_dept03\" gateway = JavelinAIGateway( gateway_uri=\" route=route_completions, model_name=\"text-davinci-003\", ) llmchain = LLMChain(llm=gateway, prompt=prompt) result = llmchain.run(\"podcast player\") print(result) ``` ## Embeddings Example ```python from langchain.embeddings import JavelinAIGatewayEmbeddings from langchain.embeddings.openai import OpenAIEmbeddings embeddings = JavelinAIGatewayEmbeddings( gateway_uri=\" route=\"embeddings\", ) print(embeddings.embed_query(\"hello\")) print(embeddings.embed_documents([\"hello\"])) ``` ## Chat Example ```python from langchain.chat_models import ChatJavelinAIGateway from langchain.schema import HumanMessage, SystemMessage messages = [ SystemMessage( content=\"You are a helpful assistant that translates English to French.\" ), HumanMessage( content=\"Artificial Intelligence has the power to transform humanity and make the world a better place\" ), ] chat = ChatJavelinAIGateway( gateway_uri=\" route=\"mychatbot_route\", model_name=\"gpt-3.5-turbo\" params={ \"temperature\": 0.1 } ) print(chat(messages)) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/jina.mdx"}, "data": "# Jina This page covers how to use the Jina ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Jina wrappers. ## Installation and Setup - Install the Python SDK with `pip install jina` - Get a Jina AI Cloud auth token from [here]( and set it as an environment variable (`JINA_AUTH_TOKEN`) ## Wrappers ### Embeddings There exists a Jina Embeddings wrapper, which you can access with ```python from langchain.embeddings import JinaEmbeddings ``` For a more detailed walkthrough of this, see [this notebook](/docs/integrations/text_embedding/jina) ## Deployment [Langchain-serve]( powered by Jina, helps take LangChain apps to production with easy to use REST/WebSocket APIs and Slack bots. ### Usage Install the package from PyPI. ```bash pip install langchain-serve ``` Wrap your LangChain app with the `@serving` decorator. ```python # app.py from lcserve import serving @serving def ask(input: str) -> str: from langchain.chains import LLMChain from langchain.llms import OpenAI from langchain.agents import AgentExecutor, ZeroShotAgent tools = [...] # list of tools prompt = ZeroShotAgent.create_prompt( tools, input_variables=[\"input\", \"agent_scratchpad\"], ) llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt) agent = ZeroShotAgent( llm_chain=llm_chain, allowed_tools=[tool.name for tool in tools] ) agent_executor = AgentExecutor.from_agent_and_tools( agent=agent, tools=tools, verbose=True, ) return agent_executor.run(input) ``` Deploy on Jina AI Cloud with `lc-serve deploy jcloud app`. Once deployed, we can send a POST request to the API endpoint to get a response. ```bash curl -X 'POST' ' \\ -d '{ \"input\": \"Your Question here?\", \"envs\": { \"OPENAI_API_KEY\": \"sk-***\" } }' ``` You can also self-host the app on your infrastructure with Docker-compose or Kubernetes. See [here]( for more details. Langchain-serve also allows to deploy the apps with WebSocket APIs and Slack Bots both on [Jina AI Cloud]( or self-hosted infrastructure."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/johnsnowlabs.mdx"}, "data": "# Johnsnowlabs Gain access to the [johnsnowlabs]( ecosystem of enterprise NLP libraries with over 21.000 enterprise NLP models in over 200 languages with the open source `johnsnowlabs` library. For all 24.000+ models, see the [John Snow Labs Model Models Hub]( ## Installation and Setup ```bash pip install johnsnowlabs ``` To [install enterprise features]( run: ```python # for more details see nlp.install() ``` You can embed your queries and documents with either `gpu`,`cpu`,`apple_silicon`,`aarch` based optimized binaries. By default cpu binaries are used. Once a session is started, you must restart your notebook to switch between GPU or CPU, or changes will not take effect. ## Embed Query with CPU: ```python document = \"foo bar\" embedding = JohnSnowLabsEmbeddings('embed_sentence.bert') output = embedding.embed_query(document) ``` ## Embed Query with GPU: ```python document = \"foo bar\" embedding = JohnSnowLabsEmbeddings('embed_sentence.bert','gpu') output = embedding.embed_query(document) ``` ## Embed Query with Apple Silicon (M1,M2,etc..): ```python documents = [\"foo bar\", 'bar foo'] embedding = JohnSnowLabsEmbeddings('embed_sentence.bert','apple_silicon') output = embedding.embed_query(document) ``` ## Embed Query with AARCH: ```python documents = [\"foo bar\", 'bar foo'] embedding = JohnSnowLabsEmbeddings('embed_sentence.bert','aarch') output = embedding.embed_query(document) ``` ## Embed Document with CPU: ```python documents = [\"foo bar\", 'bar foo'] embedding = JohnSnowLabsEmbeddings('embed_sentence.bert','gpu') output = embedding.embed_documents(documents) ``` ## Embed Document with GPU: ```python documents = [\"foo bar\", 'bar foo'] embedding = JohnSnowLabsEmbeddings('embed_sentence.bert','gpu') output = embedding.embed_documents(documents) ``` ## Embed Document with Apple Silicon (M1,M2,etc..): ```python ```python documents = [\"foo bar\", 'bar foo'] embedding = JohnSnowLabsEmbeddings('embed_sentence.bert','apple_silicon') output = embedding.embed_documents(documents) ``` ## Embed Document with AARCH: ```python ```python documents = [\"foo bar\", 'bar foo'] embedding = JohnSnowLabsEmbeddings('embed_sentence.bert','aarch') output = embedding.embed_documents(documents) ``` Models are loaded with [nlp.load]( and spark session is started with [nlp.start()]( under the hood."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/konko.mdx"}, "data": "# Konko This page covers how to run models on Konko within LangChain. Konko API is a fully managed API designed to help application developers: Select the right LLM(s) for their application Prototype with various open-source and proprietary LLMs Move to production in-line with their security, privacy, throughput, latency SLAs without infrastructure set-up or administration using Konko AI's SOC 2 compliant infrastructure ## Installation and Setup ### First you'll need an API key You can request it by messaging [support@konko.ai](mailto:support@konko.ai) ### Install Konko AI's Python SDK #### 1. Enable a Python3.8+ environment #### 2. Set API Keys ##### Option 1: Set Environment Variables 1. You can set environment variables for 1. KONKO_API_KEY (Required) 2. OPENAI_API_KEY (Optional) 2. In your current shell session, use the export command: ```shell export KONKO_API_KEY={your_KONKO_API_KEY_here} export OPENAI_API_KEY={your_OPENAI_API_KEY_here} #Optional ``` Alternatively, you can add the above lines directly to your shell startup script (such as .bashrc or .bash_profile for Bash shell and .zshrc for Zsh shell) to have them set automatically every time a new shell session starts. ##### Option 2: Set API Keys Programmatically If you prefer to set your API keys directly within your Python script or Jupyter notebook, you can use the following commands: ```python konko.set_api_key('your_KONKO_API_KEY_here') konko.set_openai_api_key('your_OPENAI_API_KEY_here') # Optional ``` #### 3. Install the SDK ```shell pip install konko ``` #### 4. Verify Installation & Authentication ```python #Confirm konko has installed successfully import konko #Confirm API keys from Konko and OpenAI are set properly konko.Model.list() ``` ## Calling a model Find a model on the [Konko Introduction page]( For example, for this [LLama 2 model]( The model id would be: `\"meta-llama/Llama-2-13b-chat-hf\"` Another way to find the list of models running on the Konko instance is through this [endpoint]( From here, we can initialize our model: ```python chat_instance = ChatKonko(max_tokens=10, model = 'meta-llama/Llama-2-13b-chat-hf') ``` And run it: ```python msg = HumanMessage(content=\"Hi\") chat_response = chat_instance([msg]) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/lancedb.mdx"}, "data": "# LanceDB This page covers how to use [LanceDB]( within LangChain. It is broken into two parts: installation and setup, and then references to specific LanceDB wrappers. ## Installation and Setup - Install the Python SDK with `pip install lancedb` ## Wrappers ### VectorStore There exists a wrapper around LanceDB databases, allowing you to use it as a vectorstore, whether for semantic search or example selection. To import this vectorstore: ```python from langchain.vectorstores import LanceDB ``` For a more detailed walkthrough of the LanceDB wrapper, see [this notebook](/docs/integrations/vectorstores/lancedb)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/langchain_decorators.mdx"}, "data": "# LangChain Decorators lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar for writing custom langchain prompts and chains For Feedback, Issues, Contributions - please raise an issue here: [ju-bezdek/langchain-decorators]( Main principles and benefits: - more `pythonic` way of writing code - write multiline prompts that won't break your code flow with indentation - making use of IDE in-built support for **hinting**, **type checking** and **popup with docs** to quickly peek in the function to see the prompt, parameters it consumes etc. - leverage all the power of LangChain ecosystem - adding support for **optional parameters** - easily share parameters between the prompts by binding them to one class Here is a simple example of a code written with **LangChain Decorators ** ``` python @llm_prompt def write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\")->str: \"\"\" Write me a short header for my post about {topic} for {platform} platform. It should be for {audience} audience. (Max 15 words) \"\"\" return # run it naturally write_me_short_post(topic=\"starwars\") # or write_me_short_post(topic=\"starwars\", platform=\"redit\") ``` # Quick start ## Installation ```bash pip install langchain_decorators ``` ## Examples Good idea on how to start is to review the examples here: - [jupyter notebook]( - [colab notebook]( # Defining other parameters Here we are just marking a function as a prompt with `llm_prompt` decorator, turning it effectively into a LLMChain. Instead of running it Standard LLMchain takes much more init parameter than just inputs_variables and prompt... here is this implementation detail hidden in the decorator. Here is how it works: 1. Using **Global settings**: ``` python # define global settings for all prompty (if not set - chatGPT is the current default) from langchain_decorators import GlobalSettings GlobalSettings.define_settings( default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming ) ``` 2. Using predefined **prompt types** ``` python #You can change the default prompt types from langchain_decorators import PromptTypes, PromptTypeSettings PromptTypes.AGENT_REASONING.llm = ChatOpenAI() # Or you can just define your own ones: class MyCustomPromptTypes(PromptTypes): GPT4=PromptTypeSettings(llm=ChatOpenAI(model=\"gpt-4\")) @llm_prompt(prompt_type=MyCustomPromptTypes.GPT4) def write_a_complicated_code(app_idea:str)->str: ... ``` 3. Define the settings **directly in the decorator** ``` python from langchain.llms import OpenAI @llm_prompt( llm=OpenAI(temperature=0.7), stop_tokens=[\"\\nObservation\"], ... ) def creative_writer(book_title:str)->str: ... ``` ## Passing a memory and/or callbacks: To pass any of these, just declare them in the function (or use kwargs to pass anything) ```python @llm_prompt() async def write_me_short_post(topic:str, platform:str=\"twitter\", memory:SimpleMemory = None): \"\"\" {history_key} Write me a short header for my post about {topic} for {platform} platform. It should be for {audience} audience. (Max 15 words) \"\"\" pass await write_me_short_post(topic=\"old movies\") ``` # Simplified streaming If we want to leverage streaming: - we need to define prompt as async function - turn on the streaming on the decorator, or we can define PromptType with streaming on - capture the stream using StreamingContext This way we just mark which prompt should be streamed, not needing to tinker with what LLM should we use, passing around the creating and distribute streaming handler into particular part of our chain... just turn the streaming on/off on prompt/prompt type... The streaming will happen only if we call it in streaming context ... there we can define a simple function to handle the stream ``` python # this code example is complete and should run as it is from langchain_decorators import StreamingContext, llm_prompt # this will mark the prompt for streaming (useful if we want stream just some prompts in our app... but don't want to pass distribute the callback handlers) # note that only async functions can be streamed (will get an error if it's not) @llm_prompt(capture_stream=True) async def write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\"): \"\"\" Write me a short header for my post about {topic} for {platform} platform. It should be for {audience} audience. (Max 15 words) \"\"\" pass # just an arbitrary function to demonstrate the streaming... will be some websockets code in the real world tokens=[] def capture_stream_func(new_token:str): tokens.append(new_token) # if we want to capture the stream, we need to wrap the execution into StreamingContext... # this will allow us to capture the stream even if the prompt call is hidden inside higher level method # only the prompts marked with capture_stream will be captured here with StreamingContext(stream_to_stdout=True, callback=capture_stream_func): result = await run_prompt() print(\"Stream finished ... we can distinguish tokens thanks to alternating colors\") print(\"\\nWe've captured\",len(tokens),\"tokens\\n\") print(\"Here is the result:\") print(result) ``` # Prompt declarations By default the prompt is is the whole function docs, unless you mark your prompt ## Documenting your prompt We can specify what part of our docs is the prompt definition, by specifying a code block with `` language tag ``` python @llm_prompt def write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\"): \"\"\" Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs. It needs to be a code block, marked as a `` language ``` Write me a short header for my post about {topic} for {platform} platform. It should be for {audience} audience. (Max 15 words) ``` Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers. (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly)) \"\"\" return ``` ## Chat messages prompt For chat models is very useful to define prompt as a set of message templates... here is how to do it: ``` python @llm_prompt def simulate_conversation(human_input:str, agent_role:str=\"a pirate\"): \"\"\" ## System message - note the `:system` sufix inside the tag ``` You are a {agent_role} hacker. You mus act like one. You reply always in code, using python or javascript code block... for example: ... do not reply with anything else.. just with code - respecting your role. ``` # human message (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user) ``` Helo, who are you ``` a reply: ``` \\``` python {history} ``` ``` {human_input} ``` Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers. (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly)) \"\"\" pass ``` the roles here are model native roles (assistant, user, system for chatGPT) # Optional sections - you can define a whole sections of your prompt that should be optional - if any input in the section is missing, the whole section won't be rendered the syntax for this is as follows: ``` python @llm_prompt def prompt_with_optional_partials(): \"\"\" this text will be rendered always, but {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | \"\") ?} you can also place it in between the words this too will be rendered{? , but this block will be rendered only if {this_value} and {this_value} is not empty?} ! \"\"\" ``` # Output parsers - llm_prompt decorator natively tries to detect the best output parser based on the output type. (if not set, it returns the raw string) - list, dict and pydantic outputs are also supported natively (automatically) ``` python # this code example is complete and should run as it is from langchain_decorators import llm_prompt @llm_prompt def write_name_suggestions(company_business:str, count:int)->list: \"\"\" Write me {count} good name suggestions for company that {company_business} \"\"\" pass write_name_suggestions(company_business=\"sells cookies\", count=5) ``` ## More complex structures for dict / pydantic you need to specify the formatting instructions... this can be tedious, that's why you can let the output parser gegnerate you the instructions based on the model (pydantic) ``` python from langchain_decorators import llm_prompt from pydantic import BaseModel, Field class TheOutputStructureWeExpect(BaseModel): name:str = Field (description=\"The name of the company\") headline:str = Field( description=\"The description of the company (for landing page)\") employees:list[str] = Field(description=\"5-8 fake employee names with their positions\") @llm_prompt() def fake_company_generator(company_business:str)->TheOutputStructureWeExpect: \"\"\" Generate a fake company that {company_business} {FORMAT_INSTRUCTIONS} \"\"\" return company = fake_company_generator(company_business=\"sells cookies\") # print the result nicely formatted print(\"Company name: \",company.name) print(\"company headline: \",company.headline) print(\"company employees: \",company.employees) ``` # Binding the prompt to an object ``` python from pydantic import BaseModel from langchain_decorators import llm_prompt class AssistantPersonality(BaseModel): assistant_name:str assistant_role:str field:str @property def a_property(self): return \"whatever\" def hello_world(self, function_kwarg:str=None): \"\"\" We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method \"\"\" @llm_prompt def introduce_your_self(self)->str: \"\"\" ``` You are an assistant named {assistant_name}. Your role is to act as {assistant_role} ``` ``` Introduce your self (in less than 20 words) ``` \"\"\" personality = AssistantPersonality(assistant_name=\"John\", assistant_role=\"a pirate\") print(personality.introduce_your_self(personality)) ``` # More examples: - these and few more examples are also available in the [colab notebook here]( - including the [ReAct Agent re-implementation]( using purely langchain decorators"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/llamacpp.mdx"}, "data": "# Llama.cpp This page covers how to use [llama.cpp]( within LangChain. It is broken into two parts: installation and setup, and then references to specific Llama-cpp wrappers. ## Installation and Setup - Install the Python package with `pip install llama-cpp-python` - Download one of the [supported models]( and convert them to the llama.cpp format per the [instructions]( ## Wrappers ### LLM There exists a LlamaCpp LLM wrapper, which you can access with ```python from langchain.llms import LlamaCpp ``` For a more detailed walkthrough of this, see [this notebook](/docs/integrations/llms/llamacpp) ### Embeddings There exists a LlamaCpp Embeddings wrapper, which you can access with ```python from langchain.embeddings import LlamaCppEmbeddings ``` For a more detailed walkthrough of this, see [this notebook](/docs/integrations/text_embedding/llamacpp)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/log10.mdx"}, "data": "# Log10 This page covers how to use the [Log10]( within LangChain. ## What is Log10? Log10 is an [open-source]( proxiless LLM data management and application development platform that lets you log, debug and tag your Langchain calls. ## Quick start 1. Create your free account at [log10.io]( 2. Add your `LOG10_TOKEN` and `LOG10_ORG_ID` from the Settings and Organization tabs respectively as environment variables. 3. Also add `LOG10_URL= and your usual LLM API key: for e.g. `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` to your environment ## How to enable Log10 data management for Langchain Integration with log10 is a simple one-line `log10_callback` integration as shown below: ```python from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage from log10.langchain import Log10Callback from log10.llm import Log10Config log10_callback = Log10Callback(log10_config=Log10Config()) messages = [ HumanMessage(content=\"You are a ping pong machine\"), HumanMessage(content=\"Ping?\"), ] llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", callbacks=[log10_callback]) ``` [Log10 + Langchain + Logs docs]( [More details + screenshots]( including instructions for self-hosting logs ## How to use tags with Log10 ```python from langchain.llms import OpenAI from langchain.chat_models import ChatAnthropic from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage from log10.langchain import Log10Callback from log10.llm import Log10Config log10_callback = Log10Callback(log10_config=Log10Config()) messages = [ HumanMessage(content=\"You are a ping pong machine\"), HumanMessage(content=\"Ping?\"), ] llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", callbacks=[log10_callback], temperature=0.5, tags=[\"test\"]) completion = llm.predict_messages(messages, tags=[\"foobar\"]) print(completion) llm = ChatAnthropic(model=\"claude-2\", callbacks=[log10_callback], temperature=0.7, tags=[\"baz\"]) llm.predict_messages(messages) print(completion) llm = OpenAI(model_name=\"text-davinci-003\", callbacks=[log10_callback], temperature=0.5) completion = llm.predict(\"You are a ping pong machine.\\nPing?\\n\") print(completion) ``` You can also intermix direct OpenAI calls and Langchain LLM calls: ```python import os from log10.load import log10, log10_session import openai from langchain.llms import OpenAI log10(openai) with log10_session(tags=[\"foo\", \"bar\"]): # Log a direct OpenAI call response = openai.Completion.create( model=\"text-ada-001\", prompt=\"Where is the Eiffel Tower?\", temperature=0, max_tokens=1024, top_p=1, frequency_penalty=0, presence_penalty=0, ) print(response) # Log a call via Langchain llm = OpenAI(model_name=\"text-ada-001\", temperature=0.5) response = llm.predict(\"You are a ping pong machine.\\nPing?\\n\") print(response) ``` ## How to debug Langchain calls [Example of debugging]( [More Langchain examples]("}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/marqo.md"}, "data": "# Marqo This page covers how to use the Marqo ecosystem within LangChain. ### **What is Marqo?** Marqo is a tensor search engine that uses embeddings stored in in-memory HNSW indexes to achieve cutting edge search speeds. Marqo can scale to hundred-million document indexes with horizontal index sharding and allows for async and non-blocking data upload and search. Marqo uses the latest machine learning models from PyTorch, Huggingface, OpenAI and more. You can start with a pre-configured model or bring your own. The built in ONNX support and conversion allows for faster inference and higher throughput on both CPU and GPU. Because Marqo include its own inference your documents can have a mix of text and images, you can bring Marqo indexes with data from your other systems into the langchain ecosystem without having to worry about your embeddings being compatible. Deployment of Marqo is flexible, you can get started yourself with our docker image or [contact us about our managed cloud offering!]( To run Marqo locally with our docker image, [see our getting started.]( ## Installation and Setup - Install the Python SDK with `pip install marqo` ## Wrappers ### VectorStore There exists a wrapper around Marqo indexes, allowing you to use them within the vectorstore framework. Marqo lets you select from a range of models for generating embeddings and exposes some preprocessing configurations. The Marqo vectorstore can also work with existing multimodel indexes where your documents have a mix of images and text, for more information refer to [our documentation]( Note that instaniating the Marqo vectorstore with an existing multimodal index will disable the ability to add any new documents to it via the langchain vectorstore `add_texts` method. To import this vectorstore: ```python from langchain.vectorstores import Marqo ``` For a more detailed walkthrough of the Marqo wrapper and some of its unique features, see [this notebook](/docs/integrations/vectorstores/marqo)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/mediawikidump.mdx"}, "data": "# MediaWikiDump >[MediaWiki XML Dumps]( contain the content of a wiki > (wiki pages with all their revisions), without the site-related data. A XML dump does not create a full backup > of the wiki database, the dump does not contain user accounts, images, edit logs, etc. ## Installation and Setup We need to install several python packages. The `mediawiki-utilities` supports XML schema 0.11 in unmerged branches. ```bash pip install -qU git+ ``` The `mediawiki-utilities mwxml` has a bug, fix PR pending. ```bash pip install -qU git+ pip install -qU mwparserfromhell ``` ## Document Loader See a [usage example](/docs/integrations/document_loaders/mediawikidump). ```python from langchain.document_loaders import MWDumpLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/meilisearch.mdx"}, "data": "# Meilisearch > [Meilisearch]( is an open-source, lightning-fast, and hyper > relevant search engine. > It comes with great defaults to help developers build snappy search experiences. > > You can [self-host Meilisearch]( > or run on [Meilisearch Cloud]( > >`Meilisearch v1.3` supports vector search. ## Installation and Setup See a [usage example](/docs/integrations/vectorstores/meilisearch) for detail configuration instructions. We need to install `meilisearch` python package. ```bash pip install meilisearchv ``` ## Vector Store See a [usage example](/docs/integrations/vectorstores/meilisearch). ```python from langchain.vectorstores import Meilisearch ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/metal.mdx"}, "data": "# Metal This page covers how to use [Metal]( within LangChain. ## What is Metal? Metal is a managed retrieval & memory platform built for production. Easily index your data into `Metal` and run semantic search and retrieval on it. ![Metal](/img/MetalDash.png) ## Quick start Get started by [creating a Metal account]( Then, you can easily take advantage of the `MetalRetriever` class to start retrieving your data for semantic search, prompting context, etc. This class takes a `Metal` instance and a dictionary of parameters to pass to the Metal API. ```python from langchain.retrievers import MetalRetriever from metal_sdk.metal import Metal metal = Metal(\"API_KEY\", \"CLIENT_ID\", \"INDEX_ID\"); retriever = MetalRetriever(metal, params={\"limit\": 2}) docs = retriever.get_relevant_documents(\"search term\") ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/milvus.mdx"}, "data": "# Milvus >[Milvus]( is a database that stores, indexes, and manages > massive embedding vectors generated by deep neural networks and other machine learning (ML) models. ## Installation and Setup Install the Python SDK: ```bash pip install pymilvus ``` ## Vector Store There exists a wrapper around `Milvus` indexes, allowing you to use it as a vectorstore, whether for semantic search or example selection. To import this vectorstore: ```python from langchain.vectorstores import Milvus ``` For a more detailed walkthrough of the `Miluvs` wrapper, see [this notebook](/docs/integrations/vectorstores/milvus)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/minimax.mdx"}, "data": "# Minimax >[Minimax]( is a Chinese startup that provides natural language processing models > for companies and individuals. ## Installation and Setup Get a [Minimax api key]( and set it as an environment variable (`MINIMAX_API_KEY`) Get a [Minimax group id]( and set it as an environment variable (`MINIMAX_GROUP_ID`) ## LLM There exists a Minimax LLM wrapper, which you can access with See a [usage example](/docs/modules/model_io/llms/integrations/minimax). ```python from langchain.llms import Minimax ``` ## Chat Models See a [usage example](/docs/modules/model_io/chat/integrations/minimax) ```python from langchain.chat_models import MiniMaxChat ``` ## Text Embedding Model There exists a Minimax Embedding model, which you can access with ```python from langchain.embeddings import MiniMaxEmbeddings ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/mlflow_ai_gateway.mdx"}, "data": "# MLflow AI Gateway >[The MLflow AI Gateway]( service is a powerful tool designed to streamline the usage and management of various large > language model (LLM) providers, such as OpenAI and Anthropic, within an organization. It offers a high-level interface > that simplifies the interaction with these services by providing a unified endpoint to handle specific LLM related requests. > See [the MLflow AI Gateway documentation]( for more details. ## Installation and Setup Install `mlflow` with MLflow AI Gateway dependencies: ```sh pip install 'mlflow[gateway]' ``` Set the OpenAI API key as an environment variable: ```sh export OPENAI_API_KEY=... ``` Create a configuration file: ```yaml routes: - name: completions route_type: llm/v1/completions model: provider: openai name: text-davinci-003 config: openai_api_key: $OPENAI_API_KEY - name: embeddings route_type: llm/v1/embeddings model: provider: openai name: text-embedding-ada-002 config: openai_api_key: $OPENAI_API_KEY ``` Start the Gateway server: ```sh mlflow gateway start --config-path /path/to/config.yaml ``` ## Example provided by `MLflow` >The `mlflow.langchain` module provides an API for logging and loading `LangChain` models. > This module exports multivariate LangChain models in the langchain flavor and univariate LangChain > models in the pyfunc flavor. See the [API documentation and examples]( ## Completions Example ```python import mlflow from langchain.chains import LLMChain, PromptTemplate from langchain.llms import MlflowAIGateway gateway = MlflowAIGateway( gateway_uri=\" route=\"completions\", params={ \"temperature\": 0.0, \"top_p\": 0.1, }, ) llm_chain = LLMChain( llm=gateway, prompt=PromptTemplate( input_variables=[\"adjective\"], template=\"Tell me a {adjective} joke\", ), ) result = llm_chain.run(adjective=\"funny\") print(result) with mlflow.start_run(): model_info = mlflow.langchain.log_model(chain, \"model\") model = mlflow.pyfunc.load_model(model_info.model_uri) print(model.predict([{\"adjective\": \"funny\"}])) ``` ## Embeddings Example ```python from langchain.embeddings import MlflowAIGatewayEmbeddings embeddings = MlflowAIGatewayEmbeddings( gateway_uri=\" route=\"embeddings\", ) print(embeddings.embed_query(\"hello\")) print(embeddings.embed_documents([\"hello\"])) ``` ## Chat Example ```python from langchain.chat_models import ChatMLflowAIGateway from langchain.schema import HumanMessage, SystemMessage chat = ChatMLflowAIGateway( gateway_uri=\" route=\"chat\", params={ \"temperature\": 0.1 } ) messages = [ SystemMessage( content=\"You are a helpful assistant that translates English to French.\" ), HumanMessage( content=\"Translate this sentence from English to French: I love programming.\" ), ] print(chat(messages)) ``` ## Databricks MLflow AI Gateway Databricks MLflow AI Gateway is in private preview. Please contact a Databricks representative to enroll in the preview. ```python from langchain.chains import LLMChain from langchain.prompts import PromptTemplate from langchain.llms import MlflowAIGateway gateway = MlflowAIGateway( gateway_uri=\"databricks\", route=\"completions\", ) llm_chain = LLMChain( llm=gateway, prompt=PromptTemplate( input_variables=[\"adjective\"], template=\"Tell me a {adjective} joke\", ), ) result = llm_chain.run(adjective=\"funny\") print(result) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/modal.mdx"}, "data": "# Modal This page covers how to use the Modal ecosystem to run LangChain custom LLMs. It is broken into two parts: 1. Modal installation and web endpoint deployment 2. Using deployed web endpoint with `LLM` wrapper class. ## Installation and Setup - Install with `pip install modal` - Run `modal token new` ## Define your Modal Functions and Webhooks You must include a prompt. There is a rigid response structure: ```python class Item(BaseModel): prompt: str @stub.function() @modal.web_endpoint(method=\"POST\") def get_text(item: Item): return {\"prompt\": run_gpt2.call(item.prompt)} ``` The following is an example with the GPT2 model: ```python from pydantic import BaseModel import modal CACHE_PATH = \"/root/model_cache\" class Item(BaseModel): prompt: str stub = modal.Stub(name=\"example-get-started-with-langchain\") def download_model(): from transformers import GPT2Tokenizer, GPT2LMHeadModel tokenizer = GPT2Tokenizer.from_pretrained('gpt2') model = GPT2LMHeadModel.from_pretrained('gpt2') tokenizer.save_pretrained(CACHE_PATH) model.save_pretrained(CACHE_PATH) # Define a container image for the LLM function below, which # downloads and stores the GPT-2 model. image = modal.Image.debian_slim().pip_install( \"tokenizers\", \"transformers\", \"torch\", \"accelerate\" ).run_function(download_model) @stub.function( gpu=\"any\", image=image, retries=3, ) def run_gpt2(text: str): from transformers import GPT2Tokenizer, GPT2LMHeadModel tokenizer = GPT2Tokenizer.from_pretrained(CACHE_PATH) model = GPT2LMHeadModel.from_pretrained(CACHE_PATH) encoded_input = tokenizer(text, return_tensors='pt').input_ids output = model.generate(encoded_input, max_length=50, do_sample=True) return tokenizer.decode(output[0], skip_special_tokens=True) @stub.function() @modal.web_endpoint(method=\"POST\") def get_text(item: Item): return {\"prompt\": run_gpt2.call(item.prompt)} ``` ### Deploy the web endpoint Deploy the web endpoint to Modal cloud with the [`modal deploy`]( CLI command. Your web endpoint will acquire a persistent URL under the `modal.run` domain. ## LLM wrapper around Modal web endpoint The `Modal` LLM wrapper class which will accept your deployed web endpoint's URL. ```python from langchain.llms import Modal endpoint_url = \" # REPLACE ME with your deployed Modal web endpoint's URL llm = Modal(endpoint_url=endpoint_url) llm_chain = LLMChain(prompt=prompt, llm=llm) question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\" llm_chain.run(question) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/modelscope.mdx"}, "data": "# ModelScope >[ModelScope]( is a big repository of the models and datasets. This page covers how to use the modelscope ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific modelscope wrappers. ## Installation and Setup Install the `modelscope` package. ```bash pip install modelscope ``` ## Text Embedding Models ```python from langchain.embeddings import ModelScopeEmbeddings ``` For a more detailed walkthrough of this, see [this notebook](/docs/integrations/text_embedding/modelscope_hub)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/modern_treasury.mdx"}, "data": "# Modern Treasury >[Modern Treasury]( simplifies complex payment operations. It is a unified platform to power products and processes that move money. >- Connect to banks and payment systems >- Track transactions and balances in real-time >- Automate payment operations for scale ## Installation and Setup There isn't any special setup for it. ## Document Loader See a [usage example](/docs/integrations/document_loaders/modern_treasury). ```python from langchain.document_loaders import ModernTreasuryLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/momento.mdx"}, "data": "# Momento > [Momento Cache]( is the world's first truly serverless caching service, offering instant elasticity, scale-to-zero > capability, and blazing-fast performance. > > [Momento Vector Index]( stands out as the most productive, easiest-to-use, fully serverless vector index. > > For both services, simply grab the SDK, obtain an API key, input a few lines into your code, and you're set to go. Together, they provide a comprehensive solution for your LLM data needs. This page covers how to use the [Momento]( ecosystem within LangChain. ## Installation and Setup - Sign up for a free account [here]( to get an API key - Install the Momento Python SDK with `pip install momento` ## Cache Use Momento as a serverless, distributed, low-latency cache for LLM prompts and responses. The standard cache is the primary use case for Momento users in any environment. To integrate Momento Cache into your application: ```python from langchain.cache import MomentoCache ``` Then, set it up with the following code: ```python from datetime import timedelta from momento import CacheClient, Configurations, CredentialProvider from langchain.globals import set_llm_cache # Instantiate the Momento client cache_client = CacheClient( Configurations.Laptop.v1(), CredentialProvider.from_environment_variable(\"MOMENTO_API_KEY\"), default_ttl=timedelta(days=1)) # Choose a Momento cache name of your choice cache_name = \"langchain\" # Instantiate the LLM cache set_llm_cache(MomentoCache(cache_client, cache_name)) ``` ## Memory Momento can be used as a distributed memory store for LLMs. ### Chat Message History Memory See [this notebook](/docs/integrations/memory/momento_chat_message_history) for a walkthrough of how to use Momento as a memory store for chat message history. ## Vector Store Momento Vector Index (MVI) can be used as a vector store. See [this notebook](/docs/integrations/vectorstores/momento_vector_index) for a walkthrough of how to use MVI as a vector store."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/mongodb_atlas.mdx"}, "data": "# MongoDB Atlas >[MongoDB Atlas]( is a fully-managed cloud > database available in AWS, Azure, and GCP. It now has support for native > Vector Search on the MongoDB document data. ## Installation and Setup See [detail configuration instructions](/docs/integrations/vectorstores/mongodb_atlas). We need to install `pymongo` python package. ```bash pip install pymongo ``` ## Vector Store See a [usage example](/docs/integrations/vectorstores/mongodb_atlas). ```python from langchain.vectorstores import MongoDBAtlasVectorSearch ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/motherduck.mdx"}, "data": "# Motherduck >[Motherduck]( is a managed DuckDB-in-the-cloud service. ## Installation and Setup First, you need to install `duckdb` python package. ```bash pip install duckdb ``` You will also need to sign up for an account at [Motherduck]( After that, you should set up a connection string - we mostly integrate with Motherduck through SQLAlchemy. The connection string is likely in the form: ``` token=\"...\" conn_str = f\"duckdb:///md:{token}@my_db\" ``` ## SQLChain You can use the SQLChain to query data in your Motherduck instance in natural language. ``` from langchain.llms import OpenAI, SQLDatabase, SQLDatabaseChain db = SQLDatabase.from_uri(conn_str) db_chain = SQLDatabaseChain.from_llm(OpenAI(temperature=0), db, verbose=True) ``` From here, see the [SQL Chain](/docs/use_cases/tabular/sqlite) documentation on how to use. ## LLMCache You can also easily use Motherduck to cache LLM requests. Once again this is done through the SQLAlchemy wrapper. ``` import sqlalchemy from langchain.globals import set_llm_cache eng = sqlalchemy.create_engine(conn_str) set_llm_cache(SQLAlchemyCache(engine=eng)) ``` From here, see the [LLM Caching](/docs/modules/model_io/llms/how_to/llm_caching) documentation on how to use."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/motorhead.mdx"}, "data": "# Mot\u00f6rhead >[Mot\u00f6rhead]( is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications. ## Installation and Setup See instructions at [Mot\u00f6rhead]( for running the server locally. ## Memory See a [usage example](/docs/integrations/memory/motorhead_memory). ```python from langchain.memory import MotorheadMemory ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/myscale.mdx"}, "data": "# MyScale This page covers how to use MyScale vector database within LangChain. It is broken into two parts: installation and setup, and then references to specific MyScale wrappers. With MyScale, you can manage both structured and unstructured (vectorized) data, and perform joint queries and analytics on both types of data using SQL. Plus, MyScale's cloud-native OLAP architecture, built on top of ClickHouse, enables lightning-fast data processing even on massive datasets. ## Introduction [Overview to MyScale and High performance vector search]( You can now register on our SaaS and [start a cluster now!]( If you are also interested in how we managed to integrate SQL and vector, please refer to [this document]( for further syntax reference. We also deliver with live demo on huggingface! Please checkout our [huggingface space]( They search millions of vector within a blink! ## Installation and Setup - Install the Python SDK with `pip install clickhouse-connect` ### Setting up environments There are two ways to set up parameters for myscale index. 1. Environment Variables Before you run the app, please set the environment variable with `export`: `export MYSCALE_HOST='' MYSCALE_PORT= MYSCALE_USERNAME= MYSCALE_PASSWORD= ...` You can easily find your account, password and other info on our SaaS. For details please refer to [this document]( Every attributes under `MyScaleSettings` can be set with prefix `MYSCALE_` and is case insensitive. 2. Create `MyScaleSettings` object with parameters ```python from langchain.vectorstores import MyScale, MyScaleSettings config = MyScaleSetting(host=\"\", port=8443, ...) index = MyScale(embedding_function, config) index.add_documents(...) ``` ## Wrappers supported functions: - `add_texts` - `add_documents` - `from_texts` - `from_documents` - `similarity_search` - `asimilarity_search` - `similarity_search_by_vector` - `asimilarity_search_by_vector` - `similarity_search_with_relevance_scores` - `delete` ### VectorStore There exists a wrapper around MyScale database, allowing you to use it as a vectorstore, whether for semantic search or similar example retrieval. To import this vectorstore: ```python from langchain.vectorstores import MyScale ``` For a more detailed walkthrough of the MyScale wrapper, see [this notebook](/docs/integrations/vectorstores/myscale)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/neo4j.mdx"}, "data": "# Neo4j This page covers how to use the Neo4j ecosystem within LangChain. What is Neo4j? **Neo4j in a nutshell:** - Neo4j is an open-source database management system that specializes in graph database technology. - Neo4j allows you to represent and store data in nodes and edges, making it ideal for handling connected data and relationships. - Neo4j provides a Cypher Query Language, making it easy to interact with and query your graph data. - With Neo4j, you can achieve high-performance graph traversals and queries, suitable for production-level systems. - Get started quickly with Neo4j by visiting [their website]( ## Installation and Setup - Install the Python SDK with `pip install neo4j` ## Wrappers ### VectorStore There exists a wrapper around Neo4j vector index, allowing you to use it as a vectorstore, whether for semantic search or example selection. To import this vectorstore: ```python from langchain.vectorstores import Neo4jVector ``` For a more detailed walkthrough of the Neo4j vector index wrapper, see [documentation](/docs/integrations/vectorstores/neo4jvector) ### GraphCypherQAChain There exists a wrapper around Neo4j graph database that allows you to generate Cypher statements based on the user input and use them to retrieve relevant information from the database. ```python from langchain.graphs import Neo4jGraph from langchain.chains import GraphCypherQAChain ``` For a more detailed walkthrough of Cypher generating chain, see [documentation](/docs/use_cases/graph/graph_cypher_qa) ### Constructing a knowledge graph from text Text data often contain rich relationships and insights that can be useful for various analytics, recommendation engines, or knowledge management applications. Diffbot's NLP API allows for the extraction of entities, relationships, and semantic meaning from unstructured text data. By coupling Diffbot's NLP API with Neo4j, a graph database, you can create powerful, dynamic graph structures based on the information extracted from text. These graph structures are fully queryable and can be integrated into various applications. ```python from langchain.graphs import Neo4jGraph from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer ``` For a more detailed walkthrough generating graphs from text, see [documentation](/docs/use_cases/graph/diffbot_graphtransformer)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/nlpcloud.mdx"}, "data": "# NLPCloud >[NLP Cloud]( is an artificial intelligence platform that allows you to use the most advanced AI engines, and even train your own engines with your own data. ## Installation and Setup - Install the `nlpcloud` package. ```bash pip install nlpcloud ``` - Get an NLPCloud api key and set it as an environment variable (`NLPCLOUD_API_KEY`) ## LLM See a [usage example](/docs/integrations/llms/nlpcloud). ```python from langchain.llms import NLPCloud ``` ## Text Embedding Models See a [usage example](/docs/integrations/text_embedding/nlp_cloud) ```python from langchain.embeddings import NLPCloudEmbeddings ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/notion.mdx"}, "data": "# Notion DB >[Notion]( is a collaboration platform with modified Markdown support that integrates kanban > boards, tasks, wikis and databases. It is an all-in-one workspace for notetaking, knowledge and data management, > and project and task management. ## Installation and Setup All instructions are in examples below. ## Document Loader We have two different loaders: `NotionDirectoryLoader` and `NotionDBLoader`. See a [usage example for the NotionDirectoryLoader](/docs/integrations/document_loaders/notion). ```python from langchain.document_loaders import NotionDirectoryLoader ``` See a [usage example for the NotionDBLoader](/docs/integrations/document_loaders/notiondb). ```python from langchain.document_loaders import NotionDBLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/nuclia.mdx"}, "data": "# Nuclia >[Nuclia]( automatically indexes your unstructured data from any internal > and external source, providing optimized search results and generative answers. > It can handle video and audio transcription, image content extraction, and document parsing. >`Nuclia Understanding API` document transformer splits text into paragraphs and sentences, > identifies entities, provides a summary of the text and generates embeddings for all the sentences. ## Installation and Setup We need to install the `nucliadb-protos` package to use the `Nuclia Understanding API`. ```bash pip install nucliadb-protos ``` To use the `Nuclia Understanding API`, we need to have a `Nuclia account`. We can create one for free at [ and then [create a NUA key]( To use the Nuclia document transformer, we need to instantiate a `NucliaUnderstandingAPI` tool with `enable_ml` set to `True`: ```python from langchain.tools.nuclia import NucliaUnderstandingAPI nua = NucliaUnderstandingAPI(enable_ml=True) ``` ## Document Transformer See a [usage example](/docs/integrations/document_transformers/nuclia_transformer). ```python from langchain.document_transformers.nuclia_text_transform import NucliaTextTransformer ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/obsidian.mdx"}, "data": "# Obsidian >[Obsidian]( is a powerful and extensible knowledge base that works on top of your local folder of plain text files. ## Installation and Setup All instructions are in examples below. ## Document Loader See a [usage example](/docs/integrations/document_loaders/obsidian). ```python from langchain.document_loaders import ObsidianLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/openllm.mdx"}, "data": "# OpenLLM This page demonstrates how to use [OpenLLM]( with LangChain. `OpenLLM` is an open platform for operating large language models (LLMs) in production. It enables developers to easily run inference with any open-source LLMs, deploy to the cloud or on-premises, and build powerful AI apps. ## Installation and Setup Install the OpenLLM package via PyPI: ```bash pip install openllm ``` ## LLM OpenLLM supports a wide range of open-source LLMs as well as serving users' own fine-tuned LLMs. Use `openllm model` command to see all available models that are pre-optimized for OpenLLM. ## Wrappers There is a OpenLLM Wrapper which supports loading LLM in-process or accessing a remote OpenLLM server: ```python from langchain.llms import OpenLLM ``` ### Wrapper for OpenLLM server This wrapper supports connecting to an OpenLLM server via HTTP or gRPC. The OpenLLM server can run either locally or on the cloud. To try it out locally, start an OpenLLM server: ```bash openllm start flan-t5 ``` Wrapper usage: ```python from langchain.llms import OpenLLM llm = OpenLLM(server_url=' llm(\"What is the difference between a duck and a goose? And why there are so many Goose in Canada?\") ``` ### Wrapper for Local Inference You can also use the OpenLLM wrapper to load LLM in current Python process for running inference. ```python from langchain.llms import OpenLLM llm = OpenLLM(model_name=\"dolly-v2\", model_id='databricks/dolly-v2-7b') llm(\"What is the difference between a duck and a goose? And why there are so many Goose in Canada?\") ``` ### Usage For a more detailed walkthrough of the OpenLLM Wrapper, see the [example notebook](/docs/integrations/llms/openllm)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/opensearch.mdx"}, "data": "# OpenSearch This page covers how to use the OpenSearch ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific OpenSearch wrappers. ## Installation and Setup - Install the Python package with `pip install opensearch-py` ## Wrappers ### VectorStore There exists a wrapper around OpenSearch vector databases, allowing you to use it as a vectorstore for semantic search using approximate vector search powered by lucene, nmslib and faiss engines or using painless scripting and script scoring functions for bruteforce vector search. To import this vectorstore: ```python from langchain.vectorstores import OpenSearchVectorSearch ``` For a more detailed walkthrough of the OpenSearch wrapper, see [this notebook](/docs/integrations/vectorstores/opensearch)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/openweathermap.mdx"}, "data": "# OpenWeatherMap >[OpenWeatherMap]( provides all essential weather data for a specific location: >- Current weather >- Minute forecast for 1 hour >- Hourly forecast for 48 hours >- Daily forecast for 8 days >- National weather alerts >- Historical weather data for 40+ years back This page covers how to use the `OpenWeatherMap API` within LangChain. ## Installation and Setup - Install requirements with ```bash pip install pyowm ``` - Go to OpenWeatherMap and sign up for an account to get your API key [here]( - Set your API key as `OPENWEATHERMAP_API_KEY` environment variable ## Wrappers ### Utility There exists a OpenWeatherMapAPIWrapper utility which wraps this API. To import this utility: ```python from langchain.utilities.openweathermap import OpenWeatherMapAPIWrapper ``` For a more detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/openweathermap). ### Tool You can also easily load this wrapper as a Tool (to use with an Agent). You can do this with: ```python from langchain.agents import load_tools tools = load_tools([\"openweathermap-api\"]) ``` For more information on tools, see [this page](/docs/modules/agents/tools/)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/petals.mdx"}, "data": "# Petals This page covers how to use the Petals ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Petals wrappers. ## Installation and Setup - Install with `pip install petals` - Get a Hugging Face api key and set it as an environment variable (`HUGGINGFACE_API_KEY`) ## Wrappers ### LLM There exists an Petals LLM wrapper, which you can access with ```python from langchain.llms import Petals ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/pg_embedding.mdx"}, "data": "# Postgres Embedding > [pg_embedding]( is an open-source package for > vector similarity search using `Postgres` and the `Hierarchical Navigable Small Worlds` > algorithm for approximate nearest neighbor search. ## Installation and Setup We need to install several python packages. ```bash pip install openai pip install psycopg2-binary pip install tiktoken ``` ## Vector Store See a [usage example](/docs/integrations/vectorstores/pgembedding). ```python from langchain.vectorstores import PGEmbedding ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/pgvector.mdx"}, "data": "# PGVector This page covers how to use the Postgres [PGVector]( ecosystem within LangChain It is broken into two parts: installation and setup, and then references to specific PGVector wrappers. ## Installation - Install the Python package with `pip install pgvector` ## Setup 1. The first step is to create a database with the `pgvector` extension installed. Follow the steps at [PGVector Installation Steps]( to install the database and the extension. The docker image is the easiest way to get started. ## Wrappers ### VectorStore There exists a wrapper around Postgres vector databases, allowing you to use it as a vectorstore, whether for semantic search or example selection. To import this vectorstore: ```python from langchain.vectorstores.pgvector import PGVector ``` ### Usage For a more detailed walkthrough of the PGVector Wrapper, see [this notebook](/docs/integrations/vectorstores/pgvector)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/pinecone.mdx"}, "data": "# Pinecone >[Pinecone]( is a vector database with broad functionality. ## Installation and Setup Install the Python SDK: ```bash pip install pinecone-client ``` ## Vector store There exists a wrapper around Pinecone indexes, allowing you to use it as a vectorstore, whether for semantic search or example selection. ```python from langchain.vectorstores import Pinecone ``` For a more detailed walkthrough of the Pinecone vectorstore, see [this notebook](/docs/integrations/vectorstores/pinecone)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/pipelineai.mdx"}, "data": "# PipelineAI This page covers how to use the PipelineAI ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific PipelineAI wrappers. ## Installation and Setup - Install with `pip install pipeline-ai` - Get a Pipeline Cloud api key and set it as an environment variable (`PIPELINE_API_KEY`) ## Wrappers ### LLM There exists a PipelineAI LLM wrapper, which you can access with ```python from langchain.llms import PipelineAI ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/portkey/index.md"}, "data": "# Portkey >[Portkey]( is a platform designed to streamline the deployment > and management of Generative AI applications. > It provides comprehensive features for monitoring, managing models, > and improving the performance of your AI applications. ## LLMOps for Langchain Portkey brings production readiness to Langchain. With Portkey, you can - [x] view detailed **metrics & logs** for all requests, - [x] enable **semantic cache** to reduce latency & costs, - [x] implement automatic **retries & fallbacks** for failed requests, - [x] add **custom tags** to requests for better tracking and analysis and [more]( ### Using Portkey with Langchain Using Portkey is as simple as just choosing which Portkey features you want, enabling them via `headers=Portkey.Config` and passing it in your LLM calls. To start, get your Portkey API key by [signing up here]( (Click the profile icon on the top left, then click on \"Copy API Key\") For OpenAI, a simple integration with logging feature would look like this: ```python from langchain.llms import OpenAI from langchain.utilities import Portkey # Add the Portkey API Key from your account headers = Portkey.Config( api_key = \"\" ) llm = OpenAI(temperature=0.9, headers=headers) llm.predict(\"What would be a good company name for a company that makes colorful socks?\") ``` Your logs will be captured on your [Portkey dashboard]( A common Portkey X Langchain use case is to **trace a chain or an agent** and view all the LLM calls originating from that request. ### **Tracing Chains & Agents** ```python from langchain.agents import AgentType, initialize_agent, load_tools from langchain.llms import OpenAI from langchain.utilities import Portkey # Add the Portkey API Key from your account headers = Portkey.Config( api_key = \"\", trace_id = \"fef659\" ) llm = OpenAI(temperature=0, headers=headers) tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm) agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True) # Let's test it out! agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\") ``` **You can see the requests' logs along with the trace id on Portkey dashboard:** ## Advanced Features 1. **Logging:** Log all your LLM requests automatically by sending them through Portkey. Each request log contains `timestamp`, `model name`, `total cost`, `request time`, `request json`, `response json`, and additional Portkey features. 2. **Tracing:** Trace id can be passed along with each request and is visibe on the logs on Portkey dashboard. You can also set a **distinct trace id** for each request. You can [append user feedback]( to a trace id as well. 3. **Caching:** Respond to previously served customers queries from cache instead of sending them again to OpenAI. Match exact strings OR semantically similar strings. Cache can save costs and reduce latencies by 20x. 4. **Retries:** Automatically reprocess any unsuccessful API requests **`upto 5`** times. Uses an **`exponential backoff`** strategy, which spaces out retry attempts to prevent network overload. 5. **Tagging:** Track and audit each user interaction in high detail with predefined tags. | Feature | Config Key | Value (Type) | Required/Optional | | -- | -- | -- | -- | | API Key | `api_key` | API Key (`string`) | Required | | [Tracing Requests]( | `trace_id` | Custom `string` | Optional | | [Automatic Retries]( | `retry_count` | `integer` [1,2,3,4,5] | Optional | | [Enabling Cache]( | `cache` | `simple` OR `semantic` | Optional | | Cache Force Refresh | `cache_force_refresh` | `True` | Optional | | Set Cache Expiry | `cache_age` | `integer` (in seconds) | Optional | | [Add User]( | `user` | `string` | Optional | | [Add Organisation]( | `organisation` | `string` | Optional | | [Add Environment]( | `environment` | `string` | Optional | | [Add Prompt (version/id/string)]( | `prompt` | `string` | Optional | ## **Enabling all Portkey Features:** ```py headers = Portkey.Config( # Mandatory api_key=\"\", # Cache Options cache=\"semantic\", cache_force_refresh=\"True\", cache_age=1729, # Advanced retry_count=5, trace_id=\"langchain_agent\", # Metadata environment=\"production\", user=\"john\", organisation=\"acme\", prompt=\"Frost\" ) ``` For detailed information on each feature and how to use it, [please refer to the Portkey docs]( If you have any questions or need further assistance, [reach out to us on Twitter.]("}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/predibase.md"}, "data": "# Predibase Learn how to use LangChain with models on Predibase. ## Setup - Create a [Predibase]( account and [API key]( - Install the Predibase Python client with `pip install predibase` - Use your API key to authenticate ### LLM Predibase integrates with LangChain by implementing LLM module. You can see a short example below or a full notebook under LLM > Integrations > Predibase. ```python import os os.environ[\"PREDIBASE_API_TOKEN\"] = \"{PREDIBASE_API_TOKEN}\" from langchain.llms import Predibase model = Predibase(model = 'vicuna-13b', predibase_api_key=os.environ.get('PREDIBASE_API_TOKEN')) response = model(\"Can you recommend me a nice dry wine?\") print(response) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/predictionguard.mdx"}, "data": "# Prediction Guard This page covers how to use the Prediction Guard ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Prediction Guard wrappers. ## Installation and Setup - Install the Python SDK with `pip install predictionguard` - Get a Prediction Guard access token (as described [here]( and set it as an environment variable (`PREDICTIONGUARD_TOKEN`) ## LLM Wrapper There exists a Prediction Guard LLM wrapper, which you can access with ```python from langchain.llms import PredictionGuard ``` You can provide the name of the Prediction Guard model as an argument when initializing the LLM: ```python pgllm = PredictionGuard(model=\"MPT-7B-Instruct\") ``` You can also provide your access token directly as an argument: ```python pgllm = PredictionGuard(model=\"MPT-7B-Instruct\", token=\"\") ``` Finally, you can provide an \"output\" argument that is used to structure/ control the output of the LLM: ```python pgllm = PredictionGuard(model=\"MPT-7B-Instruct\", output={\"type\": \"boolean\"}) ``` ## Example usage Basic usage of the controlled or guarded LLM wrapper: ```python import os import predictionguard as pg from langchain.llms import PredictionGuard from langchain.prompts import PromptTemplate from langchain.chains import LLMChain # Your Prediction Guard API key. Get one at predictionguard.com os.environ[\"PREDICTIONGUARD_TOKEN\"] = \"\" # Define a prompt template template = \"\"\"Respond to the following query based on the context. Context: EVERY comment, DM + email suggestion has led us to this EXCITING announcement! We have officially added TWO new candle subscription box options! Exclusive Candle Box - $80 Monthly Candle Box - $45 (NEW!) Scent of The Month Box - $28 (NEW!) Head to stories to get ALL the deets on each box! BONUS: Save 50% on your first box with code 50OFF! Query: {query} Result: \"\"\" prompt = PromptTemplate(template=template, input_variables=[\"query\"]) # With \"guarding\" or controlling the output of the LLM. See the # Prediction Guard docs ( to learn how to # control the output with integer, float, boolean, JSON, and other types and # structures. pgllm = PredictionGuard(model=\"MPT-7B-Instruct\", output={ \"type\": \"categorical\", \"categories\": [ \"product announcement\", \"apology\", \"relational\" ] }) pgllm(prompt.format(query=\"What kind of post is this?\")) ``` Basic LLM Chaining with the Prediction Guard wrapper: ```python import os from langchain.prompts import PromptTemplate from langchain.chains import LLMChain from langchain.llms import PredictionGuard # Optional, add your OpenAI API Key. This is optional, as Prediction Guard allows # you to access all the latest open access models (see os.environ[\"OPENAI_API_KEY\"] = \"\" # Your Prediction Guard API key. Get one at predictionguard.com os.environ[\"PREDICTIONGUARD_TOKEN\"] = \"\" pgllm = PredictionGuard(model=\"OpenAI-text-davinci-003\") template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\" prompt = PromptTemplate(template=template, input_variables=[\"question\"]) llm_chain = LLMChain(prompt=prompt, llm=pgllm, verbose=True) question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\" llm_chain.predict(question=question) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/promptlayer.mdx"}, "data": "# PromptLayer This page covers how to use [PromptLayer]( within LangChain. It is broken into two parts: installation and setup, and then references to specific PromptLayer wrappers. ## Installation and Setup If you want to work with PromptLayer: - Install the promptlayer python library `pip install promptlayer` - Create a PromptLayer account - Create an api token and set it as an environment variable (`PROMPTLAYER_API_KEY`) ## Wrappers ### LLM There exists an PromptLayer OpenAI LLM wrapper, which you can access with ```python from langchain.llms import PromptLayerOpenAI ``` To tag your requests, use the argument `pl_tags` when initializing the LLM ```python from langchain.llms import PromptLayerOpenAI llm = PromptLayerOpenAI(pl_tags=[\"langchain-requests\", \"chatbot\"]) ``` To get the PromptLayer request id, use the argument `return_pl_id` when initializing the LLM ```python from langchain.llms import PromptLayerOpenAI llm = PromptLayerOpenAI(return_pl_id=True) ``` This will add the PromptLayer request ID in the `generation_info` field of the `Generation` returned when using `.generate` or `.agenerate` For example: ```python llm_results = llm.generate([\"hello world\"]) for res in llm_results.generations: print(\"pl request id: \", res[0].generation_info[\"pl_request_id\"]) ``` You can use the PromptLayer request ID to add a prompt, score, or other metadata to your request. [Read more about it here]( This LLM is identical to the [OpenAI](/docs/ecosystem/integrations/openai) LLM, except that - all your requests will be logged to your PromptLayer account - you can add `pl_tags` when instantiating to tag your requests on PromptLayer - you can add `return_pl_id` when instantiating to return a PromptLayer request id to use [while tracking requests]( PromptLayer also provides native wrappers for [`PromptLayerChatOpenAI`](/docs/integrations/chat/promptlayer_chatopenai) and `PromptLayerOpenAIChat`"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/providers/semadb.mdx"}, "data": "# SemaDB >[SemaDB]( is a no fuss vector similarity search engine. It provides a low-cost cloud hosted version to help you build AI applications with ease. With SemaDB Cloud, our hosted version, no fuss means no pod size calculations, no schema definitions, no partition settings, no parameter tuning, no search algorithm tuning, no complex installation, no complex API. It is integrated with [RapidAPI]( providing transparent billing, automatic sharding and an interactive API playground. ## Installation None required, get started directly with SemaDB Cloud at [RapidAPI]( ## Vector Store There is a basic wrapper around `SemaDB` collections allowing you to use it as a vectorstore. ```python from langchain.vectorstores import SemaDB ``` You can follow a tutorial on how to use the wrapper in [this notebook](/docs/integrations/vectorstores/semadb)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/psychic.mdx"}, "data": "# Psychic >[Psychic]( is a platform for integrating with SaaS tools like `Notion`, `Zendesk`, > `Confluence`, and `Google Drive` via OAuth and syncing documents from these applications to your SQL or vector > database. You can think of it like Plaid for unstructured data. ## Installation and Setup ```bash pip install psychicapi ``` Psychic is easy to set up - you import the `react` library and configure it with your `Sidekick API` key, which you get from the [Psychic dashboard]( When you connect the applications, you view these connections from the dashboard and retrieve data using the server-side libraries. 1. Create an account in the [dashboard]( 2. Use the [react library]( to add the Psychic link modal to your frontend react app. You will use this to connect the SaaS apps. 3. Once you have created a connection, you can use the `PsychicLoader` by following the [example notebook](/docs/integrations/document_loaders/psychic) ## Advantages vs Other Document Loaders 1. **Universal API:** Instead of building OAuth flows and learning the APIs for every SaaS app, you integrate Psychic once and leverage our universal API to retrieve data. 2. **Data Syncs:** Data in your customers' SaaS apps can get stale fast. With Psychic you can configure webhooks to keep your documents up to date on a daily or realtime basis. 3. **Simplified OAuth:** Psychic handles OAuth end-to-end so that you don't have to spend time creating OAuth clients for each integration, keeping access tokens fresh, and handling OAuth redirect logic."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/pubmed.md"}, "data": "# PubMed # PubMed >[PubMed]( by `The National Center for Biotechnology Information, National Library of Medicine` > comprises more than 35 million citations for biomedical literature from `MEDLINE`, life science journals, and online books. > Citations may include links to full text content from `PubMed Central` and publisher web sites. ## Setup You need to install a python package. ```bash pip install xmltodict ``` ### Retriever See a [usage example](/docs/integrations/retrievers/pubmed). ```python from langchain.retrievers import PubMedRetriever ``` ### Document Loader See a [usage example](/docs/integrations/document_loaders/pubmed). ```python from langchain.document_loaders import PubMedLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/qdrant.mdx"}, "data": "# Qdrant >[Qdrant]( (read: quadrant) is a vector similarity search engine. > It provides a production-ready service with a convenient API to store, search, and manage > points - vectors with an additional payload. `Qdrant` is tailored to extended filtering support. ## Installation and Setup Install the Python SDK: ```bash pip install qdrant-client ``` ## Vector Store There exists a wrapper around `Qdrant` indexes, allowing you to use it as a vectorstore, whether for semantic search or example selection. To import this vectorstore: ```python from langchain.vectorstores import Qdrant ``` For a more detailed walkthrough of the Qdrant wrapper, see [this notebook](/docs/integrations/vectorstores/qdrant)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/reddit.mdx"}, "data": "# Reddit >[Reddit]( is an American social news aggregation, content rating, and discussion website. ## Installation and Setup First, you need to install a python package. ```bash pip install praw ``` Make a [Reddit Application]( and initialize the loader with your Reddit API credentials. ## Document Loader See a [usage example](/docs/integrations/document_loaders/reddit). ```python from langchain.document_loaders import RedditPostsLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/redis.mdx"}, "data": "# Redis >[Redis (Remote Dictionary Server)]( is an open-source in-memory storage, > used as a distributed, in-memory key\u2013value database, cache and message broker, with optional durability. > Because it holds all data in memory and because of its design, `Redis` offers low-latency reads and writes, > making it particularly suitable for use cases that require a cache. Redis is the most popular NoSQL database, > and one of the most popular databases overall. This page covers how to use the [Redis]( ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Redis wrappers. ## Installation and Setup Install the Python SDK: ```bash pip install redis ``` ## Wrappers All wrappers need a redis url connection string to connect to the database support either a stand alone Redis server or a High-Availability setup with Replication and Redis Sentinels. ### Redis Standalone connection url For standalone `Redis` server, the official redis connection url formats can be used as describe in the python redis modules \"from_url()\" method [Redis.from_url]( Example: `redis_url = \"redis://:secret-pass@localhost:6379/0\"` ### Redis Sentinel connection url For [Redis sentinel setups]( the connection scheme is \"redis+sentinel\". This is an unofficial extensions to the official IANA registered protocol schemes as long as there is no connection url for Sentinels available. Example: `redis_url = \"redis+sentinel://:secret-pass@sentinel-host:26379/mymaster/0\"` The format is `redis+sentinel://[[username]:[password]]@[host-or-ip]:[port]/[service-name]/[db-number]` with the default values of \"service-name = mymaster\" and \"db-number = 0\" if not set explicit. The service-name is the redis server monitoring group name as configured within the Sentinel. The current url format limits the connection string to one sentinel host only (no list can be given) and booth Redis server and sentinel must have the same password set (if used). ### Redis Cluster connection url Redis cluster is not supported right now for all methods requiring a \"redis_url\" parameter. The only way to use a Redis Cluster is with LangChain classes accepting a preconfigured Redis client like `RedisCache` (example below). ### Cache The Cache wrapper allows for [Redis]( to be used as a remote, low-latency, in-memory cache for LLM prompts and responses. #### Standard Cache The standard cache is the Redis bread & butter of use case in production for both [open-source]( and [enterprise]( users globally. To import this cache: ```python from langchain.cache import RedisCache ``` To use this cache with your LLMs: ```python from langchain.globals import set_llm_cache import redis redis_client = redis.Redis.from_url(...) set_llm_cache(RedisCache(redis_client)) ``` #### Semantic Cache Semantic caching allows users to retrieve cached prompts based on semantic similarity between the user input and previously cached results. Under the hood it blends Redis as both a cache and a vectorstore. To import this cache: ```python from langchain.cache import RedisSemanticCache ``` To use this cache with your LLMs: ```python from langchain.globals import set_llm_cache import redis # use any embedding provider... from tests.integration_tests.vectorstores.fake_embeddings import FakeEmbeddings redis_url = \"redis://localhost:6379\" set_llm_cache(RedisSemanticCache( embedding=FakeEmbeddings(), redis_url=redis_url )) ``` ### VectorStore The vectorstore wrapper turns Redis into a low-latency [vector database]( for semantic search or LLM content retrieval. To import this vectorstore: ```python from langchain.vectorstores import Redis ``` For a more detailed walkthrough of the Redis vectorstore wrapper, see [this notebook](/docs/integrations/vectorstores/redis). ### Retriever The Redis vector store retriever wrapper generalizes the vectorstore class to perform low-latency document retrieval. To create the retriever, simply call `.as_retriever()` on the base vectorstore class. ### Memory Redis can be used to persist LLM conversations. #### Vector Store Retriever Memory For a more detailed walkthrough of the `VectorStoreRetrieverMemory` wrapper, see [this notebook](/docs/modules/memory/types/vectorstore_retriever_memory). #### Chat Message History Memory For a detailed example of Redis to cache conversation message history, see [this notebook](/docs/integrations/memory/redis_chat_message_history)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/replicate.mdx"}, "data": "# Replicate This page covers how to run models on Replicate within LangChain. ## Installation and Setup - Create a [Replicate]( account. Get your API key and set it as an environment variable (`REPLICATE_API_TOKEN`) - Install the [Replicate python client]( with `pip install replicate` ## Calling a model Find a model on the [Replicate explore page]( and then paste in the model name and version in this format: `owner-name/model-name:version` For example, for this [dolly model]( click on the API tab. The model name/version would be: `\"replicate/dolly-v2-12b:ef0e1aefc61f8e096ebe4db6b2bacc297daf2ef6899f0f7e001ec445893500e5\"` Only the `model` param is required, but any other model parameters can also be passed in with the format `input={model_param: value, ...}` For example, if we were running stable diffusion and wanted to change the image dimensions: ``` Replicate(model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\", input={'image_dimensions': '512x512'}) ``` *Note that only the first output of a model will be returned.* From here, we can initialize our model: ```python llm = Replicate(model=\"replicate/dolly-v2-12b:ef0e1aefc61f8e096ebe4db6b2bacc297daf2ef6899f0f7e001ec445893500e5\") ``` And run it: ```python prompt = \"\"\" Answer the following yes/no question by reasoning step by step. Can a dog drive a car? \"\"\" llm(prompt) ``` We can call any Replicate model (not just LLMs) using this syntax. For example, we can call [Stable Diffusion]( ```python text2image = Replicate(model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\", input={'image_dimensions':'512x512'}) image_output = text2image(\"A cat riding a motorcycle by Picasso\") ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/roam.mdx"}, "data": "# Roam >[ROAM]( is a note-taking tool for networked thought, designed to create a personal knowledge base. ## Installation and Setup There isn't any special setup for it. ## Document Loader See a [usage example](/docs/integrations/document_loaders/roam). ```python from langchain.document_loaders import RoamLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/rockset.mdx"}, "data": "# Rockset >[Rockset]( is a real-time analytics database service for serving low latency, high concurrency analytical queries at scale. It builds a Converged Index on structured and semi-structured data with an efficient store for vector embeddings. Its support for running SQL on schemaless data makes it a perfect choice for running vector search with metadata filters. ## Installation and Setup Make sure you have Rockset account and go to the web console to get the API key. Details can be found on [the website]( ```bash pip install rockset ``` ## Vector Store See a [usage example](/docs/integrations/vectorstores/rockset). ```python from langchain.vectorstores import Rockset ``` ## Document Loader See a [usage example](/docs/integrations/document_loaders/rockset). ```python from langchain.document_loaders import RocksetLoader ``` ## Chat Message History See a [usage example](/docs/integrations/memory/rockset_chat_message_history). ```python from langchain.memory.chat_message_histories import RocksetChatMessageHistory ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/runhouse.mdx"}, "data": "# Runhouse This page covers how to use the [Runhouse]( ecosystem within LangChain. It is broken into three parts: installation and setup, LLMs, and Embeddings. ## Installation and Setup - Install the Python SDK with `pip install runhouse` - If you'd like to use on-demand cluster, check your cloud credentials with `sky check` ## Self-hosted LLMs For a basic self-hosted LLM, you can use the `SelfHostedHuggingFaceLLM` class. For more custom LLMs, you can use the `SelfHostedPipeline` parent class. ```python from langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLM ``` For a more detailed walkthrough of the Self-hosted LLMs, see [this notebook](/docs/integrations/llms/runhouse) ## Self-hosted Embeddings There are several ways to use self-hosted embeddings with LangChain via Runhouse. For a basic self-hosted embedding from a Hugging Face Transformers model, you can use the `SelfHostedEmbedding` class. ```python from langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLM ``` For a more detailed walkthrough of the Self-hosted Embeddings, see [this notebook](/docs/integrations/text_embedding/self-hosted)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/rwkv.mdx"}, "data": "# RWKV-4 This page covers how to use the `RWKV-4` wrapper within LangChain. It is broken into two parts: installation and setup, and then usage with an example. ## Installation and Setup - Install the Python package with `pip install rwkv` - Install the tokenizer Python package with `pip install tokenizer` - Download a [RWKV model]( and place it in your desired directory - Download the [tokens file]( ## Usage ### RWKV To use the RWKV wrapper, you need to provide the path to the pre-trained model file and the tokenizer's configuration. ```python from langchain.llms import RWKV # Test the model ```python def generate_prompt(instruction, input=None): if input: return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. # Instruction: {instruction} # Input: {input} # Response: \"\"\" else: return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request. # Instruction: {instruction} # Response: \"\"\" model = RWKV(model=\"./models/RWKV-4-Raven-3B-v7-Eng-20230404-ctx4096.pth\", strategy=\"cpu fp32\", tokens_path=\"./rwkv/20B_tokenizer.json\") response = model(generate_prompt(\"Once upon a time, \")) ``` ## Model File You can find links to model file downloads at the [RWKV-4-Raven]( repository. ### Rwkv-4 models -> recommended VRAM ``` RWKV VRAM Model | 8bit | bf16/fp16 | fp32 14B | 16GB | 28GB | >50GB 7B | 8GB | 14GB | 28GB 3B | 2.8GB| 6GB | 12GB 1b5 | 1.3GB| 3GB | 6GB ``` See the [rwkv pip]( page for more information about strategies, including streaming and cuda support."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/salute_devices.mdx"}, "data": "# Salute Devices Salute Devices provides GigaChat LLM's models. For more info how to get access to GigaChat [follow here]( ## Installation and Setup GigaChat package can be installed via pip from PyPI: ```bash pip install gigachat ``` ## LLMs See a [usage example](/docs/integrations/llms/gigachat). ```python from langchain.llms import GigaChat ``` ## Chat models See a [usage example](/docs/integrations/chat/gigachat). ```python from langchain.chat_models import GigaChat ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/scann.mdx"}, "data": "# ScaNN >[Google ScaNN]( > (Scalable Nearest Neighbors) is a python package. > >`ScaNN` is a method for efficient vector similarity search at scale. >ScaNN includes search space pruning and quantization for Maximum Inner > Product Search and also supports other distance functions such as > Euclidean distance. The implementation is optimized for x86 processors > with AVX2 support. See its [Google Research github]( > for more details. ## Installation and Setup We need to install `scann` python package. ```bash pip install scann ``` ## Vector Store See a [usage example](/docs/integrations/vectorstores/scann). ```python from langchain.vectorstores import ScaNN ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/searchapi.mdx"}, "data": "# SearchApi This page covers how to use the [SearchApi]( Google Search API within LangChain. SearchApi is a real-time SERP API for easy SERP scraping. ## Setup - Go to [ to sign up for a free account - Get the api key and set it as an environment variable (`SEARCHAPI_API_KEY`) ## Wrappers ### Utility There is a SearchApiAPIWrapper utility which wraps this API. To import this utility: ```python from langchain.utilities import SearchApiAPIWrapper ``` You can use it as part of a Self Ask chain: ```python from langchain.utilities import SearchApiAPIWrapper from langchain.llms.openai import OpenAI from langchain.agents import initialize_agent, Tool from langchain.agents import AgentType import os os.environ[\"SEARCHAPI_API_KEY\"] = \"\" os.environ['OPENAI_API_KEY'] = \"\" llm = OpenAI(temperature=0) search = SearchApiAPIWrapper() tools = [ Tool( name=\"Intermediate Answer\", func=search.run, description=\"useful for when you need to ask with search\" ) ] self_ask_with_search = initialize_agent(tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True) self_ask_with_search.run(\"Who lived longer: Plato, Socrates, or Aristotle?\") ``` #### Output ``` > Entering new AgentExecutor chain... Yes. Follow up: How old was Plato when he died? Intermediate answer: eighty Follow up: How old was Socrates when he died? Intermediate answer: | Socrates | | -------- | | Born | c. 470 BC Deme Alopece, Athens | | Died | 399 BC (aged approximately 71) Athens | | Cause of death | Execution by forced suicide by poisoning | | Spouse(s) | Xanthippe, Myrto | Follow up: How old was Aristotle when he died? Intermediate answer: 62 years So the final answer is: Plato > Finished chain. 'Plato' ``` ### Tool You can also easily load this wrapper as a Tool (to use with an Agent). You can do this with: ```python from langchain.agents import load_tools tools = load_tools([\"searchapi\"]) ``` For more information on tools, see [this page](/docs/modules/agents/tools/)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/searx.mdx"}, "data": "# SearxNG Search API This page covers how to use the SearxNG search API within LangChain. It is broken into two parts: installation and setup, and then references to the specific SearxNG API wrapper. ## Installation and Setup While it is possible to utilize the wrapper in conjunction with [public searx instances]( these instances frequently do not permit API access (see note on output format below) and have limitations on the frequency of requests. It is recommended to opt for a self-hosted instance instead. ### Self Hosted Instance: See [this page]( for installation instructions. When you install SearxNG, the only active output format by default is the HTML format. You need to activate the `json` format to use the API. This can be done by adding the following line to the `settings.yml` file: ```yaml search: formats: - html - json ``` You can make sure that the API is working by issuing a curl request to the API endpoint: `curl -kLX GET --data-urlencode q='langchain' -d format=json This should return a JSON object with the results. ## Wrappers ### Utility To use the wrapper we need to pass the host of the SearxNG instance to the wrapper with: 1. the named parameter `searx_host` when creating the instance. 2. exporting the environment variable `SEARXNG_HOST`. You can use the wrapper to get results from a SearxNG instance. ```python from langchain.utilities import SearxSearchWrapper s = SearxSearchWrapper(searx_host=\" s.run(\"what is a large language model?\") ``` ### Tool You can also load this wrapper as a Tool (to use with an Agent). You can do this with: ```python from langchain.agents import load_tools tools = load_tools([\"searx-search\"], searx_host=\" engines=[\"github\"]) ``` Note that we could _optionally_ pass custom engines to use. If you want to obtain results with metadata as *json* you can use: ```python tools = load_tools([\"searx-search-results-json\"], searx_host=\" num_results=5) ``` #### Quickly creating tools This examples showcases a quick way to create multiple tools from the same wrapper. ```python from langchain.tools.searx_search.tool import SearxSearchResults wrapper = SearxSearchWrapper(searx_host=\"**\") github_tool = SearxSearchResults(name=\"Github\", wrapper=wrapper, kwargs = { \"engines\": [\"github\"], }) arxiv_tool = SearxSearchResults(name=\"Arxiv\", wrapper=wrapper, kwargs = { \"engines\": [\"arxiv\"] }) ``` For more information on tools, see [this page](/docs/modules/agents/tools/)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/serpapi.mdx"}, "data": "# SerpAPI This page covers how to use the SerpAPI search APIs within LangChain. It is broken into two parts: installation and setup, and then references to the specific SerpAPI wrapper. ## Installation and Setup - Install requirements with `pip install google-search-results` - Get a SerpAPI api key and either set it as an environment variable (`SERPAPI_API_KEY`) ## Wrappers ### Utility There exists a SerpAPI utility which wraps this API. To import this utility: ```python from langchain.utilities import SerpAPIWrapper ``` For a more detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/serpapi). ### Tool You can also easily load this wrapper as a Tool (to use with an Agent). You can do this with: ```python from langchain.agents import load_tools tools = load_tools([\"serpapi\"]) ``` For more information on this, see [this page](/docs/modules/agents/tools)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/shaleprotocol.md"}, "data": "# Shale Protocol [Shale Protocol]( provides production-ready inference APIs for open LLMs. It's a Plug & Play API as it's hosted on a highly scalable GPU cloud infrastructure. Our free tier supports up to 1K daily requests per key as we want to eliminate the barrier for anyone to start building genAI apps with LLMs. With Shale Protocol, developers/researchers can create apps and explore the capabilities of open LLMs at no cost. This page covers how Shale-Serve API can be incorporated with LangChain. As of June 2023, the API supports Vicuna-13B by default. We are going to support more LLMs such as Falcon-40B in future releases. ## How to ### 1. Find the link to our Discord on Generate an API key through the \"Shale Bot\" on our Discord. No credit card is required and no free trials. It's a forever free tier with 1K limit per day per API key. ### 2. Use as OpenAI API drop-in replacement For example ```python from langchain.llms import OpenAI from langchain.prompts import PromptTemplate from langchain.chains import LLMChain import os os.environ['OPENAI_API_BASE'] = \" os.environ['OPENAI_API_KEY'] = \"ENTER YOUR API KEY\" llm = OpenAI() template = \"\"\"Question: {question} # Answer: Let's think step by step.\"\"\" prompt = PromptTemplate(template=template, input_variables=[\"question\"]) llm_chain = LLMChain(prompt=prompt, llm=llm) question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\" llm_chain.run(question) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/singlestoredb.mdx"}, "data": "# SingleStoreDB >[SingleStoreDB]( is a high-performance distributed SQL database that supports deployment both in the [cloud]( and on-premises. It provides vector storage, and vector functions including [dot_product]( and [euclidean_distance]( thereby supporting AI applications that require text similarity matching. ## Installation and Setup There are several ways to establish a [connection]( to the database. You can either set up environment variables or pass named parameters to the `SingleStoreDB constructor`. Alternatively, you may provide these parameters to the `from_documents` and `from_texts` methods. ```bash pip install singlestoredb ``` ## Vector Store See a [usage example](/docs/integrations/vectorstores/singlestoredb). ```python from langchain.vectorstores import SingleStoreDB ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/sklearn.mdx"}, "data": "# scikit-learn >[scikit-learn]( is an open-source collection of machine learning algorithms, > including some implementations of the [k nearest neighbors]( `SKLearnVectorStore` wraps this implementation and adds the possibility to persist the vector store in json, bson (binary json) or Apache Parquet format. ## Installation and Setup - Install the Python package with `pip install scikit-learn` ## Vector Store `SKLearnVectorStore` provides a simple wrapper around the nearest neighbor implementation in the scikit-learn package, allowing you to use it as a vectorstore. To import this vectorstore: ```python from langchain.vectorstores import SKLearnVectorStore ``` For a more detailed walkthrough of the SKLearnVectorStore wrapper, see [this notebook](/docs/integrations/vectorstores/sklearn)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/slack.mdx"}, "data": "# Slack >[Slack]( is an instant messaging program. ## Installation and Setup There isn't any special setup for it. ## Document Loader See a [usage example](/docs/integrations/document_loaders/slack). ```python from langchain.document_loaders import SlackDirectoryLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/spacy.mdx"}, "data": "# spaCy >[spaCy]( is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. ## Installation and Setup ```bash pip install spacy ``` ## Text Splitter See a [usage example](/docs/modules/data_connection/document_transformers/text_splitters/split_by_token#spacy). ```python from langchain.text_splitter import SpacyTextSplitter ``` ## Text Embedding Models See a [usage example](/docs/integrations/text_embedding/spacy_embedding) ```python from langchain.embeddings.spacy_embeddings import SpacyEmbeddings ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/spreedly.mdx"}, "data": "# Spreedly >[Spreedly]( is a service that allows you to securely store credit cards and use them to transact against any number of payment gateways and third party APIs. It does this by simultaneously providing a card tokenization/vault service as well as a gateway and receiver integration service. Payment methods tokenized by Spreedly are stored at `Spreedly`, allowing you to independently store a card and then pass that card to different end points based on your business requirements. ## Installation and Setup See [setup instructions](/docs/integrations/document_loaders/spreedly). ## Document Loader See a [usage example](/docs/integrations/document_loaders/spreedly). ```python from langchain.document_loaders import SpreedlyLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/starrocks.mdx"}, "data": "# StarRocks >[StarRocks]( is a High-Performance Analytical Database. `StarRocks` is a next-gen sub-second MPP database for full analytics scenarios, including multi-dimensional analytics, real-time analytics and ad-hoc query. >Usually `StarRocks` is categorized into OLAP, and it has showed excellent performance in [ClickBench \u2014 a Benchmark For Analytical DBMS]( Since it has a super-fast vectorized execution engine, it could also be used as a fast vectordb. ## Installation and Setup ```bash pip install pymysql ``` ## Vector Store See a [usage example](/docs/integrations/vectorstores/starrocks). ```python from langchain.vectorstores import StarRocks ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/stochasticai.mdx"}, "data": "# StochasticAI This page covers how to use the StochasticAI ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific StochasticAI wrappers. ## Installation and Setup - Install with `pip install stochasticx` - Get an StochasticAI api key and set it as an environment variable (`STOCHASTICAI_API_KEY`) ## Wrappers ### LLM There exists an StochasticAI LLM wrapper, which you can access with ```python from langchain.llms import StochasticAI ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/stripe.mdx"}, "data": "# Stripe >[Stripe]( is an Irish-American financial services and software as a service (SaaS) company. It offers payment-processing software and application programming interfaces for e-commerce websites and mobile applications. ## Installation and Setup See [setup instructions](/docs/integrations/document_loaders/stripe). ## Document Loader See a [usage example](/docs/integrations/document_loaders/stripe). ```python from langchain.document_loaders import StripeLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/supabase.mdx"}, "data": "# Supabase (Postgres) >[Supabase]( is an open-source `Firebase` alternative. > `Supabase` is built on top of `PostgreSQL`, which offers strong `SQL` > querying capabilities and enables a simple interface with already-existing tools and frameworks. >[PostgreSQL]( also known as `Postgres`, > is a free and open-source relational database management system (RDBMS) > emphasizing extensibility and `SQL` compliance. ## Installation and Setup We need to install `supabase` python package. ```bash pip install supabase ``` ## Vector Store See a [usage example](/docs/integrations/vectorstores/supabase). ```python from langchain.vectorstores import SupabaseVectorStore ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/symblai_nebula.mdx"}, "data": "# Nebula This page covers how to use [Nebula]( [Symbl.ai]( LLM, ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Nebula wrappers. ## Installation and Setup - Get an [Nebula API Key]( and set as environment variable `NEBULA_API_KEY` - Please see the [Nebula documentation]( for more details. - No time? Visit the [Nebula Quickstart Guide]( ### LLM There exists an Nebula LLM wrapper, which you can access with ```python from langchain.llms import Nebula llm = Nebula() ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/tair.mdx"}, "data": "# Tair This page covers how to use the Tair ecosystem within LangChain. ## Installation and Setup Install Tair Python SDK with `pip install tair`. ## Wrappers ### VectorStore There exists a wrapper around TairVector, allowing you to use it as a vectorstore, whether for semantic search or example selection. To import this vectorstore: ```python from langchain.vectorstores import Tair ``` For a more detailed walkthrough of the Tair wrapper, see [this notebook](/docs/integrations/vectorstores/tair)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/telegram.mdx"}, "data": "# Telegram >[Telegram Messenger]( is a globally accessible freemium, cross-platform, encrypted, cloud-based and centralized instant messaging service. The application also provides optional end-to-end encrypted chats and video calling, VoIP, file sharing and several other features. ## Installation and Setup See [setup instructions](/docs/integrations/document_loaders/telegram). ## Document Loader See a [usage example](/docs/integrations/document_loaders/telegram). ```python from langchain.document_loaders import TelegramChatFileLoader from langchain.document_loaders import TelegramChatApiLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/tencentvectordb.mdx"}, "data": "# TencentVectorDB This page covers how to use the TencentVectorDB ecosystem within LangChain. ### VectorStore There exists a wrapper around TencentVectorDB, allowing you to use it as a vectorstore, whether for semantic search or example selection. To import this vectorstore: ```python from langchain.vectorstores import TencentVectorDB ``` For a more detailed walkthrough of the TencentVectorDB wrapper, see [this notebook](/docs/integrations/vectorstores/tencentvectordb)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/tensorflow_datasets.mdx"}, "data": "# TensorFlow Datasets >[TensorFlow Datasets]( is a collection of datasets ready to use, > with TensorFlow or other Python ML frameworks, such as Jax. All datasets are exposed > as [tf.data.Datasets]( > enabling easy-to-use and high-performance input pipelines. To get started see > the [guide]( and > the [list of datasets]( ## Installation and Setup You need to install `tensorflow` and `tensorflow-datasets` python packages. ```bash pip install tensorflow ``` ```bash pip install tensorflow-dataset ``` ## Document Loader See a [usage example](/docs/integrations/document_loaders/tensorflow_datasets). ```python from langchain.document_loaders import TensorflowDatasetLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/tigris.mdx"}, "data": "# Tigris > [Tigris]( is an open-source Serverless NoSQL Database and Search Platform designed to simplify building high-performance vector search applications. > `Tigris` eliminates the infrastructure complexity of managing, operating, and synchronizing multiple tools, allowing you to focus on building great applications instead. ## Installation and Setup ```bash pip install tigrisdb openapi-schema-pydantic openai tiktoken ``` ## Vector Store See a [usage example](/docs/integrations/vectorstores/tigris). ```python from langchain.vectorstores import Tigris ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/tomarkdown.mdx"}, "data": "# 2Markdown >[2markdown]( service transforms website content into structured markdown files. ## Installation and Setup We need the `API key`. See [instructions how to get it]( ## Document Loader See a [usage example](/docs/integrations/document_loaders/tomarkdown). ```python from langchain.document_loaders import ToMarkdownLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/trello.mdx"}, "data": "# Trello >[Trello]( is a web-based project management and collaboration tool that allows individuals and teams to organize and track their tasks and projects. It provides a visual interface known as a \"board\" where users can create lists and cards to represent their tasks and activities. >The TrelloLoader allows us to load cards from a `Trello` board. ## Installation and Setup ```bash pip install py-trello beautifulsoup4 ``` See [setup instructions](/docs/integrations/document_loaders/trello). ## Document Loader See a [usage example](/docs/integrations/document_loaders/trello). ```python from langchain.document_loaders import TrelloLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/trulens.mdx"}, "data": "# TruLens This page covers how to use [TruLens]( to evaluate and track LLM apps built on langchain. ## What is TruLens? TruLens is an [open-source]( package that provides instrumentation and evaluation tools for large language model (LLM) based applications. ## Quick start Once you've created your LLM chain, you can use TruLens for evaluation and tracking. TruLens has a number of [out-of-the-box Feedback Functions]( and is also an extensible framework for LLM evaluation. ```python # create a feedback function from trulens_eval.feedback import Feedback, Huggingface, OpenAI # Initialize HuggingFace-based feedback function collection class: hugs = Huggingface() openai = OpenAI() # Define a language match feedback function using HuggingFace. lang_match = Feedback(hugs.language_match).on_input_output() # By default this will check language match on the main app input and main app # output. # Question/answer relevance between overall question and answer. qa_relevance = Feedback(openai.relevance).on_input_output() # By default this will evaluate feedback on main app input and main app output. # Toxicity of input toxicity = Feedback(openai.toxicity).on_input() ``` After you've set up Feedback Function(s) for evaluating your LLM, you can wrap your application with TruChain to get detailed tracing, logging and evaluation of your LLM app. ```python # wrap your chain with TruChain truchain = TruChain( chain, app_id='Chain1_ChatApplication', feedbacks=[lang_match, qa_relevance, toxicity] ) # Note: any `feedbacks` specified here will be evaluated and logged whenever the chain is used. truchain(\"que hora es?\") ``` Now you can explore your LLM-based application! Doing so will help you understand how your LLM application is performing at a glance. As you iterate new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up. You'll also be able to view evaluations at a record level, and explore the chain metadata for each record. ```python tru.run_dashboard() # open a Streamlit app to explore ``` For more information on TruLens, visit [trulens.org]("}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/twitter.mdx"}, "data": "# Twitter >[Twitter]( is an online social media and social networking service. ## Installation and Setup ```bash pip install tweepy ``` We must initialize the loader with the `Twitter API` token, and we need to set up the Twitter `username`. ## Document Loader See a [usage example](/docs/integrations/document_loaders/twitter). ```python from langchain.document_loaders import TwitterTweetLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/typesense.mdx"}, "data": "# Typesense > [Typesense]( is an open-source, in-memory search engine, that you can either > [self-host]( or run > on [Typesense Cloud]( > `Typesense` focuses on performance by storing the entire index in RAM (with a backup on disk) and also > focuses on providing an out-of-the-box developer experience by simplifying available options and setting good defaults. ## Installation and Setup ```bash pip install typesense openapi-schema-pydantic openai tiktoken ``` ## Vector Store See a [usage example](/docs/integrations/vectorstores/typesense). ```python from langchain.vectorstores import Typesense ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/unstructured.mdx"}, "data": "# Unstructured >The `unstructured` package from [Unstructured.IO]( extracts clean text from raw source documents like PDFs and Word documents. This page covers how to use the [`unstructured`]( ecosystem within LangChain. ## Installation and Setup If you are using a loader that runs locally, use the following steps to get `unstructured` and its dependencies running locally. - Install the Python SDK with `pip install unstructured`. - You can install document specific dependencies with extras, i.e. `pip install \"unstructured[docx]\"`. - To install the dependencies for all document types, use `pip install \"unstructured[all-docs]\"`. - Install the following system dependencies if they are not already available on your system. Depending on what document types you're parsing, you may not need all of these. - `libmagic-dev` (filetype detection) - `poppler-utils` (images and PDFs) - `tesseract-ocr`(images and PDFs) - `libreoffice` (MS Office docs) - `pandoc` (EPUBs) If you want to get up and running with less set up, you can simply run `pip install unstructured` and use `UnstructuredAPIFileLoader` or `UnstructuredAPIFileIOLoader`. That will process your document using the hosted Unstructured API. The Unstructured API requires API keys to make requests. You can generate a free API key [here]( and start using it today! Checkout the README [here]( here to get started making API calls. We'd love to hear your feedback, let us know how it goes in our [community slack]( And stay tuned for improvements to both quality and performance! Check out the instructions [here]( if you'd like to self-host the Unstructured API or run it locally. ## Wrappers ### Data Loaders The primary `unstructured` wrappers within `langchain` are data loaders. The following shows how to use the most basic unstructured data loader. There are other file-specific data loaders available in the `langchain.document_loaders` module. ```python from langchain.document_loaders import UnstructuredFileLoader loader = UnstructuredFileLoader(\"state_of_the_union.txt\") loader.load() ``` If you instantiate the loader with `UnstructuredFileLoader(mode=\"elements\")`, the loader will track additional metadata like the page number and text type (i.e. title, narrative text) when that information is available."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/upstash.mdx"}, "data": "# Upstash Redis Upstash offers developers serverless databases and messaging platforms to build powerful applications without having to worry about the operational complexity of running databases at scale. This page covers how to use [Upstash Redis]( with LangChain. ## Installation and Setup - Upstash Redis Python SDK can be installed with `pip install upstash-redis` - A globally distributed, low-latency and highly available database can be created at the [Upstash Console]( ## Integrations All of Upstash-LangChain integrations are based on `upstash-redis` Python SDK being utilized as wrappers for LangChain. This SDK utilizes Upstash Redis DB by giving UPSTASH_REDIS_REST_URL and UPSTASH_REDIS_REST_TOKEN parameters from the console. One significant advantage of this is that, this SDK uses a REST API. This means, you can run this in serverless platforms, edge or any platform that does not support TCP connections. ### Cache [Upstash Redis]( can be used as a cache for LLM prompts and responses. To import this cache: ```python from langchain.cache import UpstashRedisCache ``` To use with your LLMs: ```python import langchain from upstash_redis import Redis URL = \"\" TOKEN = \"\" langchain.llm_cache = UpstashRedisCache(redis_=Redis(url=URL, token=TOKEN)) ``` ### Memory Upstash Redis can be used to persist LLM conversations. #### Chat Message History Memory An example of Upstash Redis for caching conversation message history can be seen in [this notebook](/docs/integrations/memory/upstash_redis_chat_message_history)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/usearch.mdx"}, "data": "# USearch >[USearch]( is a Smaller & Faster Single-File Vector Search Engine. >`USearch's` base functionality is identical to `FAISS`, and the interface should look > familiar if you have ever investigated Approximate Nearest Neighbors search. > `USearch` and `FAISS` both employ `HNSW` algorithm, but they differ significantly > in their design principles. `USearch` is compact and broadly compatible with FAISS without > sacrificing performance, with a primary focus on user-defined metrics and fewer dependencies. > ## Installation and Setup We need to install `usearch` python package. ```bash pip install usearch ``` ## Vector Store See a [usage example](/docs/integrations/vectorstores/usearch). ```python from langchain.vectorstores import USearch ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/vearch.md"}, "data": "# Vearch [Vearch]( is a scalable distributed system for efficient similarity search of deep learning vectors. # Installation and Setup Vearch Python SDK enables vearch to use locally. Vearch python sdk can be installed easily by pip install vearch. # Vectorstore Vearch also can used as vectorstore. Most detalis in [this notebook](docs/modules/indexes/vectorstores/examples/vearch.ipynb) ```python from langchain.vectorstores import Vearch ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/vectara/index.mdx"}, "data": "# Vectara >[Vectara]( is a GenAI platform for developers. It provides a simple API to build Grounded Generation >(aka Retrieval-augmented-generation or RAG) applications. **Vectara Overview:** - `Vectara` is developer-first API platform for building GenAI applications - To use Vectara - first [sign up]( and create an account. Then create a corpus and an API key for indexing and searching. - You can use Vectara's [indexing API]( to add documents into Vectara's index - You can use Vectara's [Search API]( to query Vectara's index (which also supports Hybrid search implicitly). - You can use Vectara's integration with LangChain as a Vector store or using the Retriever abstraction. ## Installation and Setup To use `Vectara` with LangChain no special installation steps are required. To get started, [sign up]( and follow our [quickstart]( guide to create a corpus and an API key. Once you have these, you can provide them as arguments to the Vectara vectorstore, or you can set them as environment variables. - export `VECTARA_CUSTOMER_ID`=\"your_customer_id\" - export `VECTARA_CORPUS_ID`=\"your_corpus_id\" - export `VECTARA_API_KEY`=\"your-vectara-api-key\" ## Vector Store There exists a wrapper around the Vectara platform, allowing you to use it as a vectorstore, whether for semantic search or example selection. To import this vectorstore: ```python from langchain.vectorstores import Vectara ``` To create an instance of the Vectara vectorstore: ```python vectara = Vectara( vectara_customer_id=customer_id, vectara_corpus_id=corpus_id, vectara_api_key=api_key ) ``` The customer_id, corpus_id and api_key are optional, and if they are not supplied will be read from the environment variables `VECTARA_CUSTOMER_ID`, `VECTARA_CORPUS_ID` and `VECTARA_API_KEY`, respectively. After you have the vectorstore, you can `add_texts` or `add_documents` as per the standard `VectorStore` interface, for example: ```python vectara.add_texts([\"to be or not to be\", \"that is the question\"]) ``` Since Vectara supports file-upload, we also added the ability to upload files (PDF, TXT, HTML, PPT, DOC, etc) directly as file. When using this method, the file is uploaded directly to the Vectara backend, processed and chunked optimally there, so you don't have to use the LangChain document loader or chunking mechanism. As an example: ```python vectara.add_files([\"path/to/file1.pdf\", \"path/to/file2.pdf\",...]) ``` To query the vectorstore, you can use the `similarity_search` method (or `similarity_search_with_score`), which takes a query string and returns a list of results: ```python results = vectara.similarity_score(\"what is LangChain?\") ``` `similarity_search_with_score` also supports the following additional arguments: - `k`: number of results to return (defaults to 5) - `lambda_val`: the [lexical matching]( factor for hybrid search (defaults to 0.025) - `filter`: a [filter]( to apply to the results (default None) - `n_sentence_context`: number of sentences to include before/after the actual matching segment when returning results. This defaults to 2. The results are returned as a list of relevant documents, and a relevance score of each document. For a more detailed examples of using the Vectara wrapper, see one of these two sample notebooks: * [Chat Over Documents with Vectara](./vectara_chat.html) * [Vectara Text Generation](./vectara_text_generation.html)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/vespa.mdx"}, "data": "# Vespa >[Vespa]( is a fully featured search engine and vector database. > It supports vector search (ANN), lexical search, and search in structured data, all in the same query. ## Installation and Setup ```bash pip install pyvespa ``` ## Retriever See a [usage example](/docs/integrations/retrievers/vespa). ```python from langchain.retrievers import VespaRetriever ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/weather.mdx"}, "data": "# Weather >[OpenWeatherMap]( is an open-source weather service provider. ## Installation and Setup ```bash pip install pyowm ``` We must set up the `OpenWeatherMap API token`. ## Document Loader See a [usage example](/docs/integrations/document_loaders/weather). ```python from langchain.document_loaders import WeatherDataLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/weaviate.mdx"}, "data": "# Weaviate >[Weaviate]( is an open-source vector database. It allows you to store data objects and vector embeddings from >your favorite ML models, and scale seamlessly into billions of data objects. What is `Weaviate`? - Weaviate is an open-source \u200bdatabase of the type \u200bvector search engine. - Weaviate allows you to store JSON documents in a class property-like fashion while attaching machine learning vectors to these documents to represent them in vector space. - Weaviate can be used stand-alone (aka bring your vectors) or with a variety of modules that can do the vectorization for you and extend the core capabilities. - Weaviate has a GraphQL-API to access your data easily. - We aim to bring your vector search set up to production to query in mere milliseconds (check our [open-source benchmarks]( to see if Weaviate fits your use case). - Get to know Weaviate in the [basics getting started guide]( in under five minutes. **Weaviate in detail:** `Weaviate` is a low-latency vector search engine with out-of-the-box support for different media types (text, images, etc.). It offers Semantic Search, Question-Answer Extraction, Classification, Customizable Models (PyTorch/TensorFlow/Keras), etc. Built from scratch in Go, Weaviate stores both objects and vectors, allowing for combining vector search with structured filtering and the fault tolerance of a cloud-native database. It is all accessible through GraphQL, REST, and various client-side programming languages. ## Installation and Setup Install the Python SDK: ```bash pip install weaviate-client ``` ## Vector Store There exists a wrapper around `Weaviate` indexes, allowing you to use it as a vectorstore, whether for semantic search or example selection. To import this vectorstore: ```python from langchain.vectorstores import Weaviate ``` For a more detailed walkthrough of the Weaviate wrapper, see [this notebook](/docs/integrations/vectorstores/weaviate)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/whatsapp.mdx"}, "data": "# WhatsApp >[WhatsApp]( (also called `WhatsApp Messenger`) is a freeware, cross-platform, centralized instant messaging (IM) and voice-over-IP (VoIP) service. It allows users to send text and voice messages, make voice and video calls, and share images, documents, user locations, and other content. ## Installation and Setup There isn't any special setup for it. ## Document Loader See a [usage example](/docs/integrations/document_loaders/whatsapp_chat). ```python from langchain.document_loaders import WhatsAppChatLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/wikipedia.mdx"}, "data": "# Wikipedia >[Wikipedia]( is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. `Wikipedia` is the largest and most-read reference work in history. ## Installation and Setup ```bash pip install wikipedia ``` ## Document Loader See a [usage example](/docs/integrations/document_loaders/wikipedia). ```python from langchain.document_loaders import WikipediaLoader ``` ## Retriever See a [usage example](/docs/integrations/retrievers/wikipedia). ```python from langchain.retrievers import WikipediaRetriever ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/wolfram_alpha.mdx"}, "data": "# Wolfram Alpha >[WolframAlpha]( is an answer engine developed by `Wolfram Research`. > It answers factual queries by computing answers from externally sourced data. This page covers how to use the `Wolfram Alpha API` within LangChain. ## Installation and Setup - Install requirements with ```bash pip install wolframalpha ``` - Go to wolfram alpha and sign up for a developer account [here]( - Create an app and get your `APP ID` - Set your APP ID as an environment variable `WOLFRAM_ALPHA_APPID` ## Wrappers ### Utility There exists a WolframAlphaAPIWrapper utility which wraps this API. To import this utility: ```python from langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper ``` For a more detailed walkthrough of this wrapper, see [this notebook](/docs/integrations/tools/wolfram_alpha). ### Tool You can also easily load this wrapper as a Tool (to use with an Agent). You can do this with: ```python from langchain.agents import load_tools tools = load_tools([\"wolfram-alpha\"]) ``` For more information on tools, see [this page](/docs/modules/agents/tools/)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/writer.mdx"}, "data": "# Writer This page covers how to use the Writer ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Writer wrappers. ## Installation and Setup - Get an Writer api key and set it as an environment variable (`WRITER_API_KEY`) ## Wrappers ### LLM There exists an Writer LLM wrapper, which you can access with ```python from langchain.llms import Writer ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/xata.mdx"}, "data": "# Xata > [Xata]( is a serverless data platform, based on `PostgreSQL`. > It provides a Python SDK for interacting with your database, and a UI > for managing your data. > `Xata` has a native vector type, which can be added to any table, and > supports similarity search. LangChain inserts vectors directly to `Xata`, > and queries it for the nearest neighbors of a given vector, so that you can > use all the LangChain Embeddings integrations with `Xata`. ## Installation and Setup We need to install `xata` python package. ```bash pip install xata==1.0.0a7 ``` ## Vector Store See a [usage example](/docs/integrations/vectorstores/xata). ```python from langchain.vectorstores import XataVectorStore ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/xinference.mdx"}, "data": "# Xorbits Inference (Xinference) This page demonstrates how to use [Xinference]( with LangChain. `Xinference` is a powerful and versatile library designed to serve LLMs, speech recognition models, and multimodal models, even on your laptop. With Xorbits Inference, you can effortlessly deploy and serve your or state-of-the-art built-in models using just a single command. ## Installation and Setup Xinference can be installed via pip from PyPI: ```bash pip install \"xinference[all]\" ``` ## LLM Xinference supports various models compatible with GGML, including chatglm, baichuan, whisper, vicuna, and orca. To view the builtin models, run the command: ```bash xinference list --all ``` ### Wrapper for Xinference You can start a local instance of Xinference by running: ```bash xinference ``` You can also deploy Xinference in a distributed cluster. To do so, first start an Xinference supervisor on the server you want to run it: ```bash xinference-supervisor -H \"${supervisor_host}\" ``` Then, start the Xinference workers on each of the other servers where you want to run them on: ```bash xinference-worker -e \" ``` You can also start a local instance of Xinference by running: ```bash xinference ``` Once Xinference is running, an endpoint will be accessible for model management via CLI or Xinference client. For local deployment, the endpoint will be For cluster deployment, the endpoint will be Then, you need to launch a model. You can specify the model names and other attributes including model_size_in_billions and quantization. You can use command line interface (CLI) to do it. For example, ```bash xinference launch -n orca -s 3 -q q4_0 ``` A model uid will be returned. Example usage: ```python from langchain.llms import Xinference llm = Xinference( server_url=\" model_uid = {model_uid} # replace model_uid with the model UID return from launching the model ) llm( prompt=\"Q: where can we visit in the capital of France? A:\", generate_config={\"max_tokens\": 1024, \"stream\": True}, ) ``` ### Usage For more information and detailed examples, refer to the [example for xinference LLMs](/docs/integrations/llms/xinference) ### Embeddings Xinference also supports embedding queries and documents. See [example for xinference embeddings](/docs/integrations/text_embedding/xinference) for a more detailed demo."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/yandex.mdx"}, "data": "# Yandex All functionality related to Yandex Cloud >[Yandex Cloud]( is a public cloud platform. ## Installation and Setup Yandex Cloud SDK can be installed via pip from PyPI: ```bash pip install yandexcloud ``` ## LLMs ### YandexGPT See a [usage example](/docs/integrations/llms/yandex). ```python from langchain.llms import YandexGPT ``` ## Chat models ### YandexGPT See a [usage example](/docs/integrations/chat/yandex). ```python from langchain.chat_models import ChatYandexGPT ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/yeagerai.mdx"}, "data": "# Yeager.ai This page covers how to use [Yeager.ai]( to generate LangChain tools and agents. ## What is Yeager.ai? Yeager.ai is an ecosystem designed to simplify the process of creating AI agents and tools. It features yAgents, a No-code LangChain Agent Builder, which enables users to build, test, and deploy AI solutions with ease. Leveraging the LangChain framework, yAgents allows seamless integration with various language models and resources, making it suitable for developers, researchers, and AI enthusiasts across diverse applications. ## yAgents Low code generative agent designed to help you build, prototype, and deploy Langchain tools with ease. ### How to use? ``` pip install yeagerai-agent yeagerai-agent ``` Go to This will install the necessary dependencies and set up yAgents on your system. After the first run, yAgents will create a .env file where you can input your OpenAI API key. You can do the same directly from the Gradio interface under the tab \"Settings\". `OPENAI_API_KEY=` We recommend using GPT-4,. However, the tool can also work with GPT-3 if the problem is broken down sufficiently. ### Creating and Executing Tools with yAgents yAgents makes it easy to create and execute AI-powered tools. Here's a brief overview of the process: 1. Create a tool: To create a tool, provide a natural language prompt to yAgents. The prompt should clearly describe the tool's purpose and functionality. For example: `create a tool that returns the n-th prime number` 2. Load the tool into the toolkit: To load a tool into yAgents, simply provide a command to yAgents that says so. For example: `load the tool that you just created it into your toolkit` 3. Execute the tool: To run a tool or agent, simply provide a command to yAgents that includes the name of the tool and any required parameters. For example: `generate the 50th prime number` You can see a video of how it works [here]( As you become more familiar with yAgents, you can create more advanced tools and agents to automate your work and enhance your productivity. For more information, see [yAgents' Github]( or our [docs]("}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/youtube.mdx"}, "data": "# YouTube >[YouTube]( is an online video sharing and social media platform by Google. > We download the `YouTube` transcripts and video information. ## Installation and Setup ```bash pip install youtube-transcript-api pip install pytube ``` See a [usage example](/docs/integrations/document_loaders/youtube_transcript). ## Document Loader See a [usage example](/docs/integrations/document_loaders/youtube_transcript). ```python from langchain.document_loaders import YoutubeLoader from langchain.document_loaders import GoogleApiYoutubeLoader ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/zep.mdx"}, "data": "# Zep ## [Fast, Scalable Building Blocks for LLM Apps]( Zep is an open source platform for productionizing LLM apps. Go from a prototype built in LangChain or LlamaIndex, or a custom app, to production in minutes without rewriting code. Key Features: - **Fast!** Zep operates independently of the your chat loop, ensuring a snappy user experience. - **Chat History Memory, Archival, and Enrichment**, populate your prompts with relevant chat history, sumamries, named entities, intent data, and more. - **Vector Search over Chat History and Documents** Automatic embedding of documents, chat histories, and summaries. Use Zep's similarity or native MMR Re-ranked search to find the most relevant. - **Manage Users and their Chat Sessions** Users and their Chat Sessions are first-class citizens in Zep, allowing you to manage user interactions with your bots or agents easily. - **Records Retention and Privacy Compliance** Comply with corporate and regulatory mandates for records retention while ensuring compliance with privacy regulations such as CCPA and GDPR. Fulfill *Right To Be Forgotten* requests with a single API call Zep project: [ Docs: [ ## Installation and Setup 1. Install the Zep service. See the [Zep Quick Start Guide]( 2. Install the Zep Python SDK: ```bash pip install zep_python ``` ## Zep Memory Zep's [Memory API]( persists your app's chat history and metadata to a Session, enriches the memory, automatically generates summaries, and enables vector similarity search over historical chat messages and summaries. There are two approaches to populating your prompt with chat history: 1. Retrieve the most recent N messages (and potentionally a summary) from a Session and use them to construct your prompt. 2. Search over the Session's chat history for messages that are relevant and use them to construct your prompt. Both of these approaches may be useful, with the first providing the LLM with context as to the most recent interactions with a human. The second approach enables you to look back further in the chat history and retrieve messages that are relevant to the current conversation in a token-efficient manner. ```python from langchain.memory import ZepMemory ``` See a [RAG App Example here](/docs/docs/integrations/memory/zep_memory). ## Memory Retriever Zep's Memory Retriever is a LangChain Retriever that enables you to retrieve messages from a Zep Session and use them to construct your prompt. The Retriever supports searching over both individual messages and summaries of conversations. The latter is useful for providing rich, but succinct context to the LLM as to relevant past conversations. Zep's Memory Retriever supports both similarity search and [Maximum Marginal Relevance (MMR) reranking]( MMR search is useful for ensuring that the retrieved messages are diverse and not too similar to each other See a [usage example](/docs/integrations/retrievers/zep_memorystore). ```python from langchain.retrievers import ZepRetriever ``` ## Zep VectorStore Zep's [Document VectorStore API]( enables you to store and retrieve documents using vector similarity search. Zep doesn't require you to understand distance functions, types of embeddings, or indexing best practices. You just pass in your chunked documents, and Zep handles the rest. Zep supports both similarity search and [Maximum Marginal Relevance (MMR) reranking]( MMR search is useful for ensuring that the retrieved documents are diverse and not too similar to each other. ```python from langchain.vectorstores.zep import ZepVectorStore ``` See a [usage example](/docs/integrations/vectorstores/zep)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/providers/zilliz.mdx"}, "data": "# Zilliz >[Zilliz Cloud]( is a fully managed service on cloud for `LF AI Milvus`, ## Installation and Setup Install the Python SDK: ```bash pip install pymilvus ``` ## Vectorstore A wrapper around Zilliz indexes allows you to use it as a vectorstore, whether for semantic search or example selection. ```python from langchain.vectorstores import Milvus ``` For a more detailed walkthrough of the Miluvs wrapper, see [this notebook](/docs/integrations/vectorstores/zilliz)"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/integrations/retrievers/self_query/index.mdx"}, "data": "--- sidebar-position: 0 --- # Self-querying retriever Learn about how the self-querying retriever works [here](/docs/modules/data_connection/retrievers/self_query). import DocCardList from \"@theme/DocCardList\";"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/langsmith/index.md"}, "data": "--- sidebar_class_name: hidden --- # LangSmith [LangSmith]( helps you trace and evaluate your language model applications and intelligent agents to help you move from prototype to production. Check out the [interactive walkthrough](/docs/langsmith/walkthrough) to get started. For more information, please refer to the [LangSmith documentation]( For tutorials and other end-to-end examples demonstrating ways to integrate LangSmith in your workflow, check out the [LangSmith Cookbook]( Some of the guides therein include: - Leveraging user feedback in your JS application ([link]( - Building an automated feedback pipeline ([link]( - How to evaluate and audit your RAG workflows ([link]( - How to fine-tune a LLM on real usage data ([link]( - How to use the [LangChain Hub]( to version your prompts ([link]("}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/agents/agent_types/index.mdx"}, "data": "--- sidebar_position: 0 --- # Agent Types Agents use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning a response to the user. Here are the agents available in LangChain. ## [Zero-shot ReAct](/docs/modules/agents/agent_types/react) This agent uses the [ReAct]( framework to determine which tool to use based solely on the tool's description. Any number of tools can be provided. This agent requires that a description is provided for each tool. **Note**: This is the most general purpose action agent. ## [Structured input ReAct](/docs/modules/agents/agent_types/structured_chat) The structured tool chat agent is capable of using multi-input tools. Older agents are configured to specify an action input as a single string, but this agent can use a tools' argument schema to create a structured action input. This is useful for more complex tool usage, like precisely navigating around a browser. ## [OpenAI Functions](/docs/modules/agents/agent_types/openai_functions_agent) Certain OpenAI models (like gpt-3.5-turbo-0613 and gpt-4-0613) have been explicitly fine-tuned to detect when a function should be called and respond with the inputs that should be passed to the function. The OpenAI Functions Agent is designed to work with these models. ## [Conversational](/docs/modules/agents/agent_types/chat_conversation_agent) This agent is designed to be used in conversational settings. The prompt is designed to make the agent helpful and conversational. It uses the ReAct framework to decide which tool to use, and uses memory to remember the previous conversation interactions. ## [Self-ask with search](/docs/modules/agents/agent_types/self_ask_with_search) This agent utilizes a single tool that should be named `Intermediate Answer`. This tool should be able to look up factual answers to questions. This agent is equivalent to the original [self-ask with search paper]( where a Google search API was provided as the tool. ## [ReAct document store](/docs/modules/agents/agent_types/react_docstore) This agent uses the ReAct framework to interact with a docstore. Two tools must be provided: a `Search` tool and a `Lookup` tool (they must be named exactly as so). The `Search` tool should search for a document, while the `Lookup` tool should look up a term in the most recently found document. This agent is equivalent to the original [ReAct paper]( specifically the Wikipedia example."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/agents/how_to/custom_llm_agent.mdx"}, "data": "# Custom LLM Agent This notebook goes through how to create your own custom LLM agent. An LLM agent consists of three parts: - `PromptTemplate`: This is the prompt template that can be used to instruct the language model on what to do - LLM: This is the language model that powers the agent - `stop` sequence: Instructs the LLM to stop generating as soon as this string is found - `OutputParser`: This determines how to parse the LLM output into an `AgentAction` or `AgentFinish` object The LLM Agent is used in an `AgentExecutor`. This `AgentExecutor` can largely be thought of as a loop that: 1. Passes user input and any previous steps to the Agent (in this case, the LLM Agent) 2. If the Agent returns an `AgentFinish`, then return that directly to the user 3. If the Agent returns an `AgentAction`, then use that to call a tool and get an `Observation` 4. Repeat, passing the `AgentAction` and `Observation` back to the Agent until an `AgentFinish` is emitted. `AgentAction` is a response that consists of `action` and `action_input`. `action` refers to which tool to use, and `action_input` refers to the input to that tool. `log` can also be provided as more context (that can be used for logging, tracing, etc). `AgentFinish` is a response that contains the final message to be sent back to the user. This should be used to end an agent run. In this notebook we walk through how to create a custom LLM agent. ## Set up environment Do necessary imports, etc. ```python from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser from langchain.prompts import StringPromptTemplate from langchain.llms import OpenAI from langchain.utilities import SerpAPIWrapper from langchain.chains import LLMChain from typing import List, Union from langchain.schema import AgentAction, AgentFinish, OutputParserException import re ``` ## Set up tool Set up any tools the agent may want to use. This may be necessary to put in the prompt (so that the agent knows to use these tools). ```python # Define which tools the agent can use to answer user queries search = SerpAPIWrapper() tools = [ Tool( name=\"Search\", func=search.run, description=\"useful for when you need to answer questions about current events\" ) ] ``` ## Prompt template This instructs the agent on what to do. Generally, the template should incorporate: - `tools`: which tools the agent has access and how and when to call them. - `intermediate_steps`: These are tuples of previous (`AgentAction`, `Observation`) pairs. These are generally not passed directly to the model, but the prompt template formats them in a specific way. - `input`: generic user input ```python # Set up the base template template = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools: {tools} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s Question: {input} {agent_scratchpad}\"\"\" ``` ```python # Set up a prompt template class CustomPromptTemplate(StringPromptTemplate): # The template to use template: str # The list of tools available tools: List[Tool] def format(self, **kwargs) -> str: # Get the intermediate steps (AgentAction, Observation tuples) # Format them in a particular way intermediate_steps = kwargs.pop(\"intermediate_steps\") thoughts = \"\" for action, observation in intermediate_steps: thoughts += action.log thoughts += f\"\\nObservation: {observation}\\nThought: \" # Set the agent_scratchpad variable to that value kwargs[\"agent_scratchpad\"] = thoughts # Create a tools variable from the list of tools provided kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools]) # Create a list of tool names for the tools provided kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools]) return self.template.format(**kwargs) ``` ```python prompt = CustomPromptTemplate( template=template, tools=tools, # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically # This includes the `intermediate_steps` variable because that is needed input_variables=[\"input\", \"intermediate_steps\"] ) ``` ## Output parser The output parser is responsible for parsing the LLM output into `AgentAction` and `AgentFinish`. This usually depends heavily on the prompt used. This is where you can change the parsing to do retries, handle whitespace, etc. ```python class CustomOutputParser(AgentOutputParser): def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]: # Check if agent should finish if \"Final Answer:\" in llm_output: return AgentFinish( # Return values is generally always a dictionary with a single `output` key # It is not recommended to try anything else at the moment :) return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()}, log=llm_output, ) # Parse out the action and action input regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\" match = re.search(regex, llm_output, re.DOTALL) if not match: raise OutputParserException(f\"Could not parse LLM output: `{llm_output}`\") action = match.group(1).strip() action_input = match.group(2) # Return the action and action input return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output) ``` ```python output_parser = CustomOutputParser() ``` ## Set up LLM Choose the LLM you want to use! ```python llm = OpenAI(temperature=0) ``` ## Define the stop sequence This is important because it tells the LLM when to stop generation. This depends heavily on the prompt and model you are using. Generally, you want this to be whatever token you use in the prompt to denote the start of an `Observation` (otherwise, the LLM may hallucinate an observation for you). ## Set up the Agent We can now combine everything to set up our agent: ```python # LLM chain consisting of the LLM and a prompt llm_chain = LLMChain(llm=llm, prompt=prompt) ``` ```python tool_names = [tool.name for tool in tools] agent = LLMSingleActionAgent( llm_chain=llm_chain, output_parser=output_parser, stop=[\"\\nObservation:\"], allowed_tools=tool_names ) ``` ## Use the Agent Now we can use it! ```python agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True) ``` ```python agent_executor.run(\"How many people live in canada as of 2023?\") ``` ``` > Entering new AgentExecutor chain... Thought: I need to find out the population of Canada in 2023 Action: Search Action Input: Population of Canada in 2023 Observation:The current population of Canada is 38,658,314 as of Wednesday, April 12, 2023, based on Worldometer elaboration of the latest United Nations data. I now know the final answer Final Answer: Arrr, there be 38,658,314 people livin' in Canada as of 2023! > Finished chain. \"Arrr, there be 38,658,314 people livin' in Canada as of 2023!\" ``` ## Adding Memory If you want to add memory to the agent, you'll need to: 1. Add a place in the custom prompt for the `chat_history` 2. Add a memory object to the agent executor. ```python # Set up the base template template_with_history = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools: {tools} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s Previous conversation history: {history} New question: {input} {agent_scratchpad}\"\"\" ``` ```python prompt_with_history = CustomPromptTemplate( template=template_with_history, tools=tools, # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically # This includes the `intermediate_steps` variable because that is needed input_variables=[\"input\", \"intermediate_steps\", \"history\"] ) ``` ```python llm_chain = LLMChain(llm=llm, prompt=prompt_with_history) ``` ```python tool_names = [tool.name for tool in tools] agent = LLMSingleActionAgent( llm_chain=llm_chain, output_parser=output_parser, stop=[\"\\nObservation:\"], allowed_tools=tool_names ) ``` ```python from langchain.memory import ConversationBufferWindowMemory ``` ```python memory=ConversationBufferWindowMemory(k=2) ``` ```python agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, memory=memory) ``` ```python agent_executor.run(\"How many people live in canada as of 2023?\") ``` ``` > Entering new AgentExecutor chain... Thought: I need to find out the population of Canada in 2023 Action: Search Action Input: Population of Canada in 2023 Observation:The current population of Canada is 38,658,314 as of Wednesday, April 12, 2023, based on Worldometer elaboration of the latest United Nations data. I now know the final answer Final Answer: Arrr, there be 38,658,314 people livin' in Canada as of 2023! > Finished chain. \"Arrr, there be 38,658,314 people livin' in Canada as of 2023!\" ``` ```python agent_executor.run(\"how about in mexico?\") ``` ``` > Entering new AgentExecutor chain... Thought: I need to find out how many people live in Mexico. Action: Search Action Input: How many people live in Mexico as of 2023? Observation:The current population of Mexico is 132,679,922 as of Tuesday, April 11, 2023, based on Worldometer elaboration of the latest United Nations data. Mexico 2020 ... I now know the final answer. Final Answer: Arrr, there be 132,679,922 people livin' in Mexico as of 2023! > Finished chain. \"Arrr, there be 132,679,922 people livin' in Mexico as of 2023!\" ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/agents/how_to/custom_llm_chat_agent.mdx"}, "data": "# Custom LLM Chat Agent This notebook explains how to create your own custom agent based on a chat model. An LLM chat agent consists of four key components: - `PromptTemplate`: This is the prompt template that instructs the language model on what to do. - `ChatModel`: This is the language model that powers the agent. - `stop` sequence: Instructs the LLM to stop generating as soon as this string is found. - `OutputParser`: This determines how to parse the LLM output into an `AgentAction` or `AgentFinish` object. The LLM Agent is used in an `AgentExecutor`. This `AgentExecutor` can largely be thought of as a loop that: 1. Passes user input and any previous steps to the Agent (in this case, the LLM Agent) 2. If the Agent returns an `AgentFinish`, then return that directly to the user 3. If the Agent returns an `AgentAction`, then use that to call a tool and get an `Observation` 4. Repeat, passing the `AgentAction` and `Observation` back to the Agent until an `AgentFinish` is emitted. `AgentAction` is a response that consists of `action` and `action_input`. `action` refers to which tool to use, and `action_input` refers to the input to that tool. `log` can also be provided as more context (that can be used for logging, tracing, etc). `AgentFinish` is a response that contains the final message to be sent back to the user. This should be used to end an agent run. In this notebook we walk through how to create a custom LLM agent. ## Set up environment Do necessary imports, etc. ```bash pip install langchain pip install google-search-results pip install openai ``` ```python from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser from langchain.prompts import BaseChatPromptTemplate from langchain.utilities import SerpAPIWrapper from langchain.chains.llm import LLMChain from langchain.chat_models import ChatOpenAI from typing import List, Union from langchain.schema import AgentAction, AgentFinish, HumanMessage import re from getpass import getpass ``` ## Set up tools Set up any tools the agent may want to use. This may be necessary to put in the prompt (so that the agent knows to use these tools). ```python SERPAPI_API_KEY = getpass() ``` ```python # Define which tools the agent can use to answer user queries search = SerpAPIWrapper(serpapi_api_key=SERPAPI_API_KEY) tools = [ Tool( name=\"Search\", func=search.run, description=\"useful for when you need to answer questions about current events\" ) ] ``` ## Prompt template This instructs the agent on what to do. Generally, the template should incorporate: - `tools`: which tools the agent has access and how and when to call them. - `intermediate_steps`: These are tuples of previous (`AgentAction`, `Observation`) pairs. These are generally not passed directly to the model, but the prompt template formats them in a specific way. - `input`: generic user input ```python # Set up the base template template = \"\"\"Complete the objective as best you can. You have access to the following tools: {tools} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question These were previous tasks you completed: Begin! Question: {input} {agent_scratchpad}\"\"\" ``` ```python # Set up a prompt template class CustomPromptTemplate(BaseChatPromptTemplate): # The template to use template: str # The list of tools available tools: List[Tool] def format_messages(self, **kwargs) -> str: # Get the intermediate steps (AgentAction, Observation tuples) # Format them in a particular way intermediate_steps = kwargs.pop(\"intermediate_steps\") thoughts = \"\" for action, observation in intermediate_steps: thoughts += action.log thoughts += f\"\\nObservation: {observation}\\nThought: \" # Set the agent_scratchpad variable to that value kwargs[\"agent_scratchpad\"] = thoughts # Create a tools variable from the list of tools provided kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools]) # Create a list of tool names for the tools provided kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools]) formatted = self.template.format(**kwargs) return [HumanMessage(content=formatted)] ``` ```python prompt = CustomPromptTemplate( template=template, tools=tools, # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically # This includes the `intermediate_steps` variable because that is needed input_variables=[\"input\", \"intermediate_steps\"] ) ``` ## Output parser The output parser is responsible for parsing the LLM output into `AgentAction` and `AgentFinish`. This usually depends heavily on the prompt used. This is where you can change the parsing to do retries, handle whitespace, etc. ```python class CustomOutputParser(AgentOutputParser): def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]: # Check if agent should finish if \"Final Answer:\" in llm_output: return AgentFinish( # Return values is generally always a dictionary with a single `output` key # It is not recommended to try anything else at the moment :) return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()}, log=llm_output, ) # Parse out the action and action input regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\" match = re.search(regex, llm_output, re.DOTALL) if not match: raise ValueError(f\"Could not parse LLM output: `{llm_output}`\") action = match.group(1).strip() action_input = match.group(2) # Return the action and action input return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output) ``` ```python output_parser = CustomOutputParser() ``` ## Set up LLM Choose the LLM you want to use! ```python OPENAI_API_KEY = getpass() ``` ```python llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0) ``` ## Define the stop sequence This is important because it tells the LLM when to stop generation. This depends heavily on the prompt and model you are using. Generally, you want this to be whatever token you use in the prompt to denote the start of an `Observation` (otherwise, the LLM may hallucinate an observation for you). ## Set up the Agent We can now combine everything to set up our agent: ```python # LLM chain consisting of the LLM and a prompt llm_chain = LLMChain(llm=llm, prompt=prompt) ``` ```python tool_names = [tool.name for tool in tools] agent = LLMSingleActionAgent( llm_chain=llm_chain, output_parser=output_parser, stop=[\"\\nObservation:\"], allowed_tools=tool_names ) ``` ## Use the Agent Now we can use it! ```python agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True) ``` ```python agent_executor.run(\"Search for Leo DiCaprio's girlfriend on the internet.\") ``` ``` > Entering new AgentExecutor chain... Thought: I should use a reliable search engine to get accurate information. Action: Search Action Input: \"Leo DiCaprio girlfriend\" Observation:He went on to date Gisele B\u00fcndchen, Bar Refaeli, Blake Lively, Toni Garrn and Nina Agdal, among others, before finally settling down with current girlfriend Camila Morrone, who is 23 years his junior. I have found the answer to the question. Final Answer: Leo DiCaprio's current girlfriend is Camila Morrone. > Finished chain. \"Leo DiCaprio's current girlfriend is Camila Morrone.\" ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/agents/how_to/mrkl.mdx"}, "data": "# Replicating MRKL This walkthrough demonstrates how to replicate the [MRKL]( system using agents. This uses the example Chinook database. To set it up, follow the instructions on and place the `.db` file in a \"notebooks\" folder at the root of this repository. ```python from langchain.chains import LLMMathChain from langchain.llms import OpenAI from langchain.utilities import SerpAPIWrapper from langchain.utilities import SQLDatabase from langchain_experimental.sql import SQLDatabaseChain from langchain.agents import initialize_agent, Tool from langchain.agents import AgentType ``` ```python llm = OpenAI(temperature=0) search = SerpAPIWrapper() llm_math_chain = LLMMathChain(llm=llm, verbose=True) db = SQLDatabase.from_uri(\"sqlite:///../../../../../notebooks/Chinook.db\") db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True) tools = [ Tool( name=\"Search\", func=search.run, description=\"useful for when you need to answer questions about current events. You should ask targeted questions\" ), Tool( name=\"Calculator\", func=llm_math_chain.run, description=\"useful for when you need to answer questions about math\" ), Tool( name=\"FooBar DB\", func=db_chain.run, description=\"useful for when you need to answer questions about FooBar. Input should be in the form of a question containing full context\" ) ] ``` ```python mrkl = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True) ``` ```python mrkl.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\") ``` ``` > Entering new AgentExecutor chain... I need to find out who Leo DiCaprio's girlfriend is and then calculate her age raised to the 0.43 power. Action: Search Action Input: \"Who is Leo DiCaprio's girlfriend?\" Observation: DiCaprio met actor Camila Morrone in December 2017, when she was 20 and he was 43. They were spotted at Coachella and went on multiple vacations together. Some reports suggested that DiCaprio was ready to ask Morrone to marry him. The couple made their red carpet debut at the 2020 Academy Awards. Thought: I need to calculate Camila Morrone's age raised to the 0.43 power. Action: Calculator Action Input: 21^0.43 > Entering new LLMMathChain chain... 21^0.43 ```text 21**0.43 ``` ...numexpr.evaluate(\"21**0.43\")... Answer: 3.7030049853137306 > Finished chain. Observation: Answer: 3.7030049853137306 Thought: I now know the final answer. Final Answer: Camila Morrone is Leo DiCaprio's girlfriend and her current age raised to the 0.43 power is 3.7030049853137306. > Finished chain. \"Camila Morrone is Leo DiCaprio's girlfriend and her current age raised to the 0.43 power is 3.7030049853137306.\" ``` ```python mrkl.run(\"What is the full name of the artist who recently released an album called 'The Storm Before the Calm' and are they in the FooBar database? If so, what albums of theirs are in the FooBar database?\") ``` ``` > Entering new AgentExecutor chain... I need to find out the artist's full name and then search the FooBar database for their albums. Action: Search Action Input: \"The Storm Before the Calm\" artist Observation: The Storm Before the Calm (stylized in all lowercase) is the tenth (and eighth international) studio album by Canadian-American singer-songwriter Alanis Morissette, released June 17, 2022, via Epiphany Music and Thirty Tigers, as well as by RCA Records in Europe. Thought: I now need to search the FooBar database for Alanis Morissette's albums. Action: FooBar DB Action Input: What albums by Alanis Morissette are in the FooBar database? > Entering new SQLDatabaseChain chain... What albums by Alanis Morissette are in the FooBar database? SQLQuery: /Users/harrisonchase/workplace/langchain/langchain/sql_database.py:191: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage. sample_rows = connection.execute(command) SELECT \"Title\" FROM \"Album\" INNER JOIN \"Artist\" ON \"Album\".\"ArtistId\" = \"Artist\".\"ArtistId\" WHERE \"Name\" = 'Alanis Morissette' LIMIT 5; SQLResult: [('Jagged Little Pill',)] Answer: The albums by Alanis Morissette in the FooBar database are Jagged Little Pill. > Finished chain. Observation: The albums by Alanis Morissette in the FooBar database are Jagged Little Pill. Thought: I now know the final answer. Final Answer: The artist who released the album 'The Storm Before the Calm' is Alanis Morissette and the albums of hers in the FooBar database are Jagged Little Pill. > Finished chain. \"The artist who released the album 'The Storm Before the Calm' is Alanis Morissette and the albums of hers in the FooBar database are Jagged Little Pill.\" ``` ## Using a Chat Model ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(temperature=0) llm1 = OpenAI(temperature=0) search = SerpAPIWrapper() llm_math_chain = LLMMathChain(llm=llm1, verbose=True) db = SQLDatabase.from_uri(\"sqlite:///../../../../../notebooks/Chinook.db\") db_chain = SQLDatabaseChain.from_llm(llm1, db, verbose=True) tools = [ Tool( name=\"Search\", func=search.run, description=\"useful for when you need to answer questions about current events. You should ask targeted questions\" ), Tool( name=\"Calculator\", func=llm_math_chain.run, description=\"useful for when you need to answer questions about math\" ), Tool( name=\"FooBar DB\", func=db_chain.run, description=\"useful for when you need to answer questions about FooBar. Input should be in the form of a question containing full context\" ) ] ``` ```python mrkl = initialize_agent(tools, llm, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True) ``` ```python mrkl.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\") ``` ``` > Entering new AgentExecutor chain... Thought: The first question requires a search, while the second question requires a calculator. Action: ``` { \"action\": \"Search\", \"action_input\": \"Leo DiCaprio girlfriend\" } ``` Observation: Gigi Hadid: 2022 Leo and Gigi were first linked back in September 2022, when a source told Us Weekly that Leo had his \u201csights set\" on her (alarming way to put it, but okay). Thought:For the second question, I need to calculate the age raised to the 0.43 power. I will use the calculator tool. Action: ``` { \"action\": \"Calculator\", \"action_input\": \"((2022-1995)^0.43)\" } ``` > Entering new LLMMathChain chain... ((2022-1995)^0.43) ```text (2022-1995)**0.43 ``` ...numexpr.evaluate(\"(2022-1995)**0.43\")... Answer: 4.125593352125936 > Finished chain. Observation: Answer: 4.125593352125936 Thought:I now know the final answer. Final Answer: Gigi Hadid is Leo DiCaprio's girlfriend and her current age raised to the 0.43 power is approximately 4.13. > Finished chain. \"Gigi Hadid is Leo DiCaprio's girlfriend and her current age raised to the 0.43 power is approximately 4.13.\" ``` ```python mrkl.run(\"What is the full name of the artist who recently released an album called 'The Storm Before the Calm' and are they in the FooBar database? If so, what albums of theirs are in the FooBar database?\") ``` ``` > Entering new AgentExecutor chain... Question: What is the full name of the artist who recently released an album called 'The Storm Before the Calm' and are they in the FooBar database? If so, what albums of theirs are in the FooBar database? Thought: I should use the Search tool to find the answer to the first part of the question and then use the FooBar DB tool to find the answer to the second part. Action: ``` { \"action\": \"Search\", \"action_input\": \"Who recently released an album called 'The Storm Before the Calm'\" } ``` Observation: Alanis Morissette Thought:Now that I know the artist's name, I can use the FooBar DB tool to find out if they are in the database and what albums of theirs are in it. Action: ``` { \"action\": \"FooBar DB\", \"action_input\": \"What albums does Alanis Morissette have in the database?\" } ``` > Entering new SQLDatabaseChain chain... What albums does Alanis Morissette have in the database? SQLQuery: /Users/harrisonchase/workplace/langchain/langchain/sql_database.py:191: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage. sample_rows = connection.execute(command) SELECT \"Title\" FROM \"Album\" WHERE \"ArtistId\" IN (SELECT \"ArtistId\" FROM \"Artist\" WHERE \"Name\" = 'Alanis Morissette') LIMIT 5; SQLResult: [('Jagged Little Pill',)] Answer: Alanis Morissette has the album Jagged Little Pill in the database. > Finished chain. Observation: Alanis Morissette has the album Jagged Little Pill in the database. Thought:The artist Alanis Morissette is in the FooBar database and has the album Jagged Little Pill in it. Final Answer: Alanis Morissette is in the FooBar database and has the album Jagged Little Pill in it. > Finished chain. 'Alanis Morissette is in the FooBar database and has the album Jagged Little Pill in it.' ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/agents/tools/index.mdx"}, "data": "--- sidebar_position: 2 --- # Tools :::info For documentation on built-in tool integrations, visit [Integrations](/docs/integrations/tools/). ::: Tools are interfaces that an agent can use to interact with the world. ## Getting Started Tools are functions that agents can use to interact with the world. These tools can be generic utilities (e.g. search), other chains, or even other agents. Currently, tools can be loaded using the following snippet: ```python from langchain.agents import load_tools tool_names = [...] tools = load_tools(tool_names) ``` Some tools (e.g. chains, agents) may require a base LLM to use to initialize them. In that case, you can pass in an LLM as well: ```python from langchain.agents import load_tools tool_names = [...] llm = ... tools = load_tools(tool_names, llm=llm) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/agents/tools/toolkits.mdx"}, "data": "--- sidebar_position: 3 --- # Toolkits :::info For documentation on built-in toolkit integrations, visit [Integrations](/docs/integrations/toolkits/). ::: Toolkits are collections of tools that are designed to be used together for specific tasks and have convenient loading methods."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/callbacks/custom_chain.mdx"}, "data": "# Callbacks for custom chains When you create a custom chain you can easily set it up to use the same callback system as all the built-in chains. `_call`, `_generate`, `_run`, and equivalent async methods on Chains / LLMs / Chat Models / Agents / Tools now receive a 2nd argument called `run_manager` which is bound to that run, and contains the logging methods that can be used by that object (i.e. `on_llm_new_token`). This is useful when constructing a custom chain. See this guide for more information on how to [create custom chains and use callbacks inside them](/docs/modules/chains/how_to/custom_chain)."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/callbacks/index.mdx"}, "data": "--- sidebar_position: 5 sidebar_class_name: hidden --- # Callbacks :::info Head to [Integrations](/docs/integrations/callbacks/) for documentation on built-in callbacks integrations with 3rd-party tools. ::: LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks. You can subscribe to these events by using the `callbacks` argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail. ## Callback handlers `CallbackHandlers` are objects that implement the `CallbackHandler` interface, which has a method for each event that can be subscribed to. The `CallbackManager` will call the appropriate method on each handler when the event is triggered. ```python class BaseCallbackHandler: \"\"\"Base callback handler that can be used to handle callbacks from langchain.\"\"\" def on_llm_start( self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any ) -> Any: \"\"\"Run when LLM starts running.\"\"\" def on_chat_model_start( self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any ) -> Any: \"\"\"Run when Chat Model starts running.\"\"\" def on_llm_new_token(self, token: str, **kwargs: Any) -> Any: \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\" def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any: \"\"\"Run when LLM ends running.\"\"\" def on_llm_error( self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any ) -> Any: \"\"\"Run when LLM errors.\"\"\" def on_chain_start( self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any ) -> Any: \"\"\"Run when chain starts running.\"\"\" def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any: \"\"\"Run when chain ends running.\"\"\" def on_chain_error( self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any ) -> Any: \"\"\"Run when chain errors.\"\"\" def on_tool_start( self, serialized: Dict[str, Any], input_str: str, **kwargs: Any ) -> Any: \"\"\"Run when tool starts running.\"\"\" def on_tool_end(self, output: str, **kwargs: Any) -> Any: \"\"\"Run when tool ends running.\"\"\" def on_tool_error( self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any ) -> Any: \"\"\"Run when tool errors.\"\"\" def on_text(self, text: str, **kwargs: Any) -> Any: \"\"\"Run on arbitrary text.\"\"\" def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any: \"\"\"Run on agent action.\"\"\" def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any: \"\"\"Run on agent end.\"\"\" ``` ## Get started LangChain provides a few built-in handlers that you can use to get started. These are available in the `langchain/callbacks` module. The most basic handler is the `StdOutCallbackHandler`, which simply logs all events to `stdout`. **Note**: when the `verbose` flag on the object is set to true, the `StdOutCallbackHandler` will be invoked even without being explicitly passed in. ```python from langchain.callbacks import StdOutCallbackHandler from langchain.chains import LLMChain from langchain.llms import OpenAI from langchain.prompts import PromptTemplate handler = StdOutCallbackHandler() llm = OpenAI() prompt = PromptTemplate.from_template(\"1 + {number} = \") # Constructor callback: First, let's explicitly set the StdOutCallbackHandler when initializing our chain chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler]) chain.run(number=2) # Use verbose flag: Then, let's use the `verbose` flag to achieve the same result chain = LLMChain(llm=llm, prompt=prompt, verbose=True) chain.run(number=2) # Request callbacks: Finally, let's use the request `callbacks` to achieve the same result chain = LLMChain(llm=llm, prompt=prompt) chain.run(number=2, callbacks=[handler]) ``` ``` > Entering new LLMChain chain... Prompt after formatting: 1 + 2 = > Finished chain. > Entering new LLMChain chain... Prompt after formatting: 1 + 2 = > Finished chain. > Entering new LLMChain chain... Prompt after formatting: 1 + 2 = > Finished chain. '\\n\\n3' ``` ## Where to pass in callbacks The `callbacks` argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) in two different places: - **Constructor callbacks**: defined in the constructor, e.g. `LLMChain(callbacks=[handler], tags=['a-tag'])`, which will be used for all calls made on that object, and will be scoped to that object only, e.g. if you pass a handler to the `LLMChain` constructor, it will not be used by the Model attached to that chain. - **Request callbacks**: defined in the `run()`/`apply()` methods used for issuing a request, e.g. `chain.run(input, callbacks=[handler])`, which will be used for that specific request only, and all sub-requests that it contains (e.g. a call to an LLMChain triggers a call to a Model, which uses the same handler passed in the `call()` method). The `verbose` argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) as a constructor argument, e.g. `LLMChain(verbose=True)`, and it is equivalent to passing a `ConsoleCallbackHandler` to the `callbacks` argument of that object and all child objects. This is useful for debugging, as it will log all events to the console. ### When do you want to use each of these? - Constructor callbacks are most useful for use cases such as logging, monitoring, etc., which are _not specific to a single request_, but rather to the entire chain. For example, if you want to log all the requests made to an `LLMChain`, you would pass a handler to the constructor. - Request callbacks are most useful for use cases such as streaming, where you want to stream the output of a single request to a specific websocket connection, or other similar use cases. For example, if you want to stream the output of a single request to a websocket, you would pass a handler to the `call()` method"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/callbacks/tags.mdx"}, "data": "# Tags You can add tags to your callbacks by passing a `tags` argument to the `call()`/`run()`/`apply()` methods. This is useful for filtering your logs, e.g. if you want to log all requests made to a specific `LLMChain`, you can add a tag, and then filter your logs by that tag. You can pass tags to both constructor and request callbacks, see the examples above for details. These tags are then passed to the `tags` argument of the \"start\" callback methods, ie. `on_llm_start`, `on_chat_model_start`, `on_chain_start`, `on_tool_start`."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/chains/document/index.mdx"}, "data": "--- sidebar_position: 2 --- # Documents These are the core chains for working with documents. They are useful for summarizing documents, answering questions over documents, extracting information from documents, and more. These chains all implement a common interface: ```python class BaseCombineDocumentsChain(Chain, ABC): \"\"\"Base interface for chains combining documents.\"\"\" @abstractmethod def combine_docs(self, docs: List[Document], **kwargs: Any) -> Tuple[str, dict]: \"\"\"Combine documents into a single string.\"\"\" ``` import DocCardList from \"@theme/DocCardList\";"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/chains/foundational/index.mdx"}, "data": "--- sidebar_position: 1 --- # Foundational import DocCardList from \"@theme/DocCardList\";"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/chains/how_to/index.mdx"}, "data": "--- sidebar_position: 0 --- # How to import DocCardList from \"@theme/DocCardList\";"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/chains/how_to/memory.mdx"}, "data": "# Adding memory (state) Chains can be initialized with a Memory object, which will persist data across calls to the chain. This makes a Chain stateful. ## Get started ```python from langchain.chains import ConversationChain from langchain.memory import ConversationBufferMemory conversation = ConversationChain( llm=chat, memory=ConversationBufferMemory() ) conversation.run(\"Answer briefly. What are the first 3 colors of a rainbow?\") # -> The first three colors of a rainbow are red, orange, and yellow. conversation.run(\"And the next 4?\") # -> The next four colors of a rainbow are green, blue, indigo, and violet. ``` ``` 'The next four colors of a rainbow are green, blue, indigo, and violet.' ``` Essentially, `BaseMemory` defines an interface of how `langchain` stores memory. It allows reading of stored data through `load_memory_variables` method and storing new data through `save_context` method. You can learn more about it in the [Memory](/docs/modules/memory/) section."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/document_loaders/csv.mdx"}, "data": "# CSV >A [comma-separated values (CSV)]( file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas. Load CSV data with a single row per document. ```python from langchain.document_loaders.csv_loader import CSVLoader loader = CSVLoader(file_path='./example_data/mlb_teams_2012.csv') data = loader.load() ``` ```python print(data) ``` ``` [Document(page_content='Team: Nationals\\n\"Payroll (millions)\": 81.34\\n\"Wins\": 98', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 0}, lookup_index=0), Document(page_content='Team: Reds\\n\"Payroll (millions)\": 82.20\\n\"Wins\": 97', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 1}, lookup_index=0), Document(page_content='Team: Yankees\\n\"Payroll (millions)\": 197.96\\n\"Wins\": 95', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 2}, lookup_index=0), Document(page_content='Team: Giants\\n\"Payroll (millions)\": 117.62\\n\"Wins\": 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 3}, lookup_index=0), Document(page_content='Team: Braves\\n\"Payroll (millions)\": 83.31\\n\"Wins\": 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 4}, lookup_index=0), Document(page_content='Team: Athletics\\n\"Payroll (millions)\": 55.37\\n\"Wins\": 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 5}, lookup_index=0), Document(page_content='Team: Rangers\\n\"Payroll (millions)\": 120.51\\n\"Wins\": 93', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 6}, lookup_index=0), Document(page_content='Team: Orioles\\n\"Payroll (millions)\": 81.43\\n\"Wins\": 93', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 7}, lookup_index=0), Document(page_content='Team: Rays\\n\"Payroll (millions)\": 64.17\\n\"Wins\": 90', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 8}, lookup_index=0), Document(page_content='Team: Angels\\n\"Payroll (millions)\": 154.49\\n\"Wins\": 89', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 9}, lookup_index=0), Document(page_content='Team: Tigers\\n\"Payroll (millions)\": 132.30\\n\"Wins\": 88', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 10}, lookup_index=0), Document(page_content='Team: Cardinals\\n\"Payroll (millions)\": 110.30\\n\"Wins\": 88', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 11}, lookup_index=0), Document(page_content='Team: Dodgers\\n\"Payroll (millions)\": 95.14\\n\"Wins\": 86', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 12}, lookup_index=0), Document(page_content='Team: White Sox\\n\"Payroll (millions)\": 96.92\\n\"Wins\": 85', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 13}, lookup_index=0), Document(page_content='Team: Brewers\\n\"Payroll (millions)\": 97.65\\n\"Wins\": 83', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 14}, lookup_index=0), Document(page_content='Team: Phillies\\n\"Payroll (millions)\": 174.54\\n\"Wins\": 81', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 15}, lookup_index=0), Document(page_content='Team: Diamondbacks\\n\"Payroll (millions)\": 74.28\\n\"Wins\": 81', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 16}, lookup_index=0), Document(page_content='Team: Pirates\\n\"Payroll (millions)\": 63.43\\n\"Wins\": 79', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 17}, lookup_index=0), Document(page_content='Team: Padres\\n\"Payroll (millions)\": 55.24\\n\"Wins\": 76', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 18}, lookup_index=0), Document(page_content='Team: Mariners\\n\"Payroll (millions)\": 81.97\\n\"Wins\": 75', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 19}, lookup_index=0), Document(page_content='Team: Mets\\n\"Payroll (millions)\": 93.35\\n\"Wins\": 74', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 20}, lookup_index=0), Document(page_content='Team: Blue Jays\\n\"Payroll (millions)\": 75.48\\n\"Wins\": 73', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 21}, lookup_index=0), Document(page_content='Team: Royals\\n\"Payroll (millions)\": 60.91\\n\"Wins\": 72', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 22}, lookup_index=0), Document(page_content='Team: Marlins\\n\"Payroll (millions)\": 118.07\\n\"Wins\": 69', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 23}, lookup_index=0), Document(page_content='Team: Red Sox\\n\"Payroll (millions)\": 173.18\\n\"Wins\": 69', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 24}, lookup_index=0), Document(page_content='Team: Indians\\n\"Payroll (millions)\": 78.43\\n\"Wins\": 68', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 25}, lookup_index=0), Document(page_content='Team: Twins\\n\"Payroll (millions)\": 94.08\\n\"Wins\": 66', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 26}, lookup_index=0), Document(page_content='Team: Rockies\\n\"Payroll (millions)\": 78.06\\n\"Wins\": 64', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 27}, lookup_index=0), Document(page_content='Team: Cubs\\n\"Payroll (millions)\": 88.19\\n\"Wins\": 61', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 28}, lookup_index=0), Document(page_content='Team: Astros\\n\"Payroll (millions)\": 60.65\\n\"Wins\": 55', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 29}, lookup_index=0)] ``` ## Customizing the CSV parsing and loading See the [csv module]( documentation for more information of what csv args are supported. ```python loader = CSVLoader(file_path='./example_data/mlb_teams_2012.csv', csv_args={ 'delimiter': ',', 'quotechar': '\"', 'fieldnames': ['MLB Team', 'Payroll in millions', 'Wins'] }) data = loader.load() ``` ```python print(data) ``` ``` [Document(page_content='MLB Team: Team\\nPayroll in millions: \"Payroll (millions)\"\\nWins: \"Wins\"', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 0}, lookup_index=0), Document(page_content='MLB Team: Nationals\\nPayroll in millions: 81.34\\nWins: 98', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 1}, lookup_index=0), Document(page_content='MLB Team: Reds\\nPayroll in millions: 82.20\\nWins: 97', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 2}, lookup_index=0), Document(page_content='MLB Team: Yankees\\nPayroll in millions: 197.96\\nWins: 95', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 3}, lookup_index=0), Document(page_content='MLB Team: Giants\\nPayroll in millions: 117.62\\nWins: 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 4}, lookup_index=0), Document(page_content='MLB Team: Braves\\nPayroll in millions: 83.31\\nWins: 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 5}, lookup_index=0), Document(page_content='MLB Team: Athletics\\nPayroll in millions: 55.37\\nWins: 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 6}, lookup_index=0), Document(page_content='MLB Team: Rangers\\nPayroll in millions: 120.51\\nWins: 93', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 7}, lookup_index=0), Document(page_content='MLB Team: Orioles\\nPayroll in millions: 81.43\\nWins: 93', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 8}, lookup_index=0), Document(page_content='MLB Team: Rays\\nPayroll in millions: 64.17\\nWins: 90', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 9}, lookup_index=0), Document(page_content='MLB Team: Angels\\nPayroll in millions: 154.49\\nWins: 89', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 10}, lookup_index=0), Document(page_content='MLB Team: Tigers\\nPayroll in millions: 132.30\\nWins: 88', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 11}, lookup_index=0), Document(page_content='MLB Team: Cardinals\\nPayroll in millions: 110.30\\nWins: 88', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 12}, lookup_index=0), Document(page_content='MLB Team: Dodgers\\nPayroll in millions: 95.14\\nWins: 86', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 13}, lookup_index=0), Document(page_content='MLB Team: White Sox\\nPayroll in millions: 96.92\\nWins: 85', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 14}, lookup_index=0), Document(page_content='MLB Team: Brewers\\nPayroll in millions: 97.65\\nWins: 83', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 15}, lookup_index=0), Document(page_content='MLB Team: Phillies\\nPayroll in millions: 174.54\\nWins: 81', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 16}, lookup_index=0), Document(page_content='MLB Team: Diamondbacks\\nPayroll in millions: 74.28\\nWins: 81', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 17}, lookup_index=0), Document(page_content='MLB Team: Pirates\\nPayroll in millions: 63.43\\nWins: 79', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 18}, lookup_index=0), Document(page_content='MLB Team: Padres\\nPayroll in millions: 55.24\\nWins: 76', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 19}, lookup_index=0), Document(page_content='MLB Team: Mariners\\nPayroll in millions: 81.97\\nWins: 75', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 20}, lookup_index=0), Document(page_content='MLB Team: Mets\\nPayroll in millions: 93.35\\nWins: 74', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 21}, lookup_index=0), Document(page_content='MLB Team: Blue Jays\\nPayroll in millions: 75.48\\nWins: 73', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 22}, lookup_index=0), Document(page_content='MLB Team: Royals\\nPayroll in millions: 60.91\\nWins: 72', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 23}, lookup_index=0), Document(page_content='MLB Team: Marlins\\nPayroll in millions: 118.07\\nWins: 69', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 24}, lookup_index=0), Document(page_content='MLB Team: Red Sox\\nPayroll in millions: 173.18\\nWins: 69', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 25}, lookup_index=0), Document(page_content='MLB Team: Indians\\nPayroll in millions: 78.43\\nWins: 68', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 26}, lookup_index=0), Document(page_content='MLB Team: Twins\\nPayroll in millions: 94.08\\nWins: 66', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 27}, lookup_index=0), Document(page_content='MLB Team: Rockies\\nPayroll in millions: 78.06\\nWins: 64', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 28}, lookup_index=0), Document(page_content='MLB Team: Cubs\\nPayroll in millions: 88.19\\nWins: 61', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 29}, lookup_index=0), Document(page_content='MLB Team: Astros\\nPayroll in millions: 60.65\\nWins: 55', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 30}, lookup_index=0)] ``` ## Specify a column to identify the document source Use the `source_column` argument to specify a source for the document created from each row. Otherwise `file_path` will be used as the source for all documents created from the CSV file. This is useful when using documents loaded from CSV files for chains that answer questions using sources. ```python loader = CSVLoader(file_path='./example_data/mlb_teams_2012.csv', source_column=\"Team\") data = loader.load() ``` ```python print(data) ``` ``` [Document(page_content='Team: Nationals\\n\"Payroll (millions)\": 81.34\\n\"Wins\": 98', lookup_str='', metadata={'source': 'Nationals', 'row': 0}, lookup_index=0), Document(page_content='Team: Reds\\n\"Payroll (millions)\": 82.20\\n\"Wins\": 97', lookup_str='', metadata={'source': 'Reds', 'row': 1}, lookup_index=0), Document(page_content='Team: Yankees\\n\"Payroll (millions)\": 197.96\\n\"Wins\": 95', lookup_str='', metadata={'source': 'Yankees', 'row': 2}, lookup_index=0), Document(page_content='Team: Giants\\n\"Payroll (millions)\": 117.62\\n\"Wins\": 94', lookup_str='', metadata={'source': 'Giants', 'row': 3}, lookup_index=0), Document(page_content='Team: Braves\\n\"Payroll (millions)\": 83.31\\n\"Wins\": 94', lookup_str='', metadata={'source': 'Braves', 'row': 4}, lookup_index=0), Document(page_content='Team: Athletics\\n\"Payroll (millions)\": 55.37\\n\"Wins\": 94', lookup_str='', metadata={'source': 'Athletics', 'row': 5}, lookup_index=0), Document(page_content='Team: Rangers\\n\"Payroll (millions)\": 120.51\\n\"Wins\": 93', lookup_str='', metadata={'source': 'Rangers', 'row': 6}, lookup_index=0), Document(page_content='Team: Orioles\\n\"Payroll (millions)\": 81.43\\n\"Wins\": 93', lookup_str='', metadata={'source': 'Orioles', 'row': 7}, lookup_index=0), Document(page_content='Team: Rays\\n\"Payroll (millions)\": 64.17\\n\"Wins\": 90', lookup_str='', metadata={'source': 'Rays', 'row': 8}, lookup_index=0), Document(page_content='Team: Angels\\n\"Payroll (millions)\": 154.49\\n\"Wins\": 89', lookup_str='', metadata={'source': 'Angels', 'row': 9}, lookup_index=0), Document(page_content='Team: Tigers\\n\"Payroll (millions)\": 132.30\\n\"Wins\": 88', lookup_str='', metadata={'source': 'Tigers', 'row': 10}, lookup_index=0), Document(page_content='Team: Cardinals\\n\"Payroll (millions)\": 110.30\\n\"Wins\": 88', lookup_str='', metadata={'source': 'Cardinals', 'row': 11}, lookup_index=0), Document(page_content='Team: Dodgers\\n\"Payroll (millions)\": 95.14\\n\"Wins\": 86', lookup_str='', metadata={'source': 'Dodgers', 'row': 12}, lookup_index=0), Document(page_content='Team: White Sox\\n\"Payroll (millions)\": 96.92\\n\"Wins\": 85', lookup_str='', metadata={'source': 'White Sox', 'row': 13}, lookup_index=0), Document(page_content='Team: Brewers\\n\"Payroll (millions)\": 97.65\\n\"Wins\": 83', lookup_str='', metadata={'source': 'Brewers', 'row': 14}, lookup_index=0), Document(page_content='Team: Phillies\\n\"Payroll (millions)\": 174.54\\n\"Wins\": 81', lookup_str='', metadata={'source': 'Phillies', 'row': 15}, lookup_index=0), Document(page_content='Team: Diamondbacks\\n\"Payroll (millions)\": 74.28\\n\"Wins\": 81', lookup_str='', metadata={'source': 'Diamondbacks', 'row': 16}, lookup_index=0), Document(page_content='Team: Pirates\\n\"Payroll (millions)\": 63.43\\n\"Wins\": 79', lookup_str='', metadata={'source': 'Pirates', 'row': 17}, lookup_index=0), Document(page_content='Team: Padres\\n\"Payroll (millions)\": 55.24\\n\"Wins\": 76', lookup_str='', metadata={'source': 'Padres', 'row': 18}, lookup_index=0), Document(page_content='Team: Mariners\\n\"Payroll (millions)\": 81.97\\n\"Wins\": 75', lookup_str='', metadata={'source': 'Mariners', 'row': 19}, lookup_index=0), Document(page_content='Team: Mets\\n\"Payroll (millions)\": 93.35\\n\"Wins\": 74', lookup_str='', metadata={'source': 'Mets', 'row': 20}, lookup_index=0), Document(page_content='Team: Blue Jays\\n\"Payroll (millions)\": 75.48\\n\"Wins\": 73', lookup_str='', metadata={'source': 'Blue Jays', 'row': 21}, lookup_index=0), Document(page_content='Team: Royals\\n\"Payroll (millions)\": 60.91\\n\"Wins\": 72', lookup_str='', metadata={'source': 'Royals', 'row': 22}, lookup_index=0), Document(page_content='Team: Marlins\\n\"Payroll (millions)\": 118.07\\n\"Wins\": 69', lookup_str='', metadata={'source': 'Marlins', 'row': 23}, lookup_index=0), Document(page_content='Team: Red Sox\\n\"Payroll (millions)\": 173.18\\n\"Wins\": 69', lookup_str='', metadata={'source': 'Red Sox', 'row': 24}, lookup_index=0), Document(page_content='Team: Indians\\n\"Payroll (millions)\": 78.43\\n\"Wins\": 68', lookup_str='', metadata={'source': 'Indians', 'row': 25}, lookup_index=0), Document(page_content='Team: Twins\\n\"Payroll (millions)\": 94.08\\n\"Wins\": 66', lookup_str='', metadata={'source': 'Twins', 'row': 26}, lookup_index=0), Document(page_content='Team: Rockies\\n\"Payroll (millions)\": 78.06\\n\"Wins\": 64', lookup_str='', metadata={'source': 'Rockies', 'row': 27}, lookup_index=0), Document(page_content='Team: Cubs\\n\"Payroll (millions)\": 88.19\\n\"Wins\": 61', lookup_str='', metadata={'source': 'Cubs', 'row': 28}, lookup_index=0), Document(page_content='Team: Astros\\n\"Payroll (millions)\": 60.65\\n\"Wins\": 55', lookup_str='', metadata={'source': 'Astros', 'row': 29}, lookup_index=0)] ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/document_loaders/file_directory.mdx"}, "data": "# File Directory This covers how to load all documents in a directory. Under the hood, by default this uses the [UnstructuredLoader](/docs/integrations/document_loaders/unstructured_file). ```python from langchain.document_loaders import DirectoryLoader ``` We can use the `glob` parameter to control which files to load. Note that here it doesn't load the `.rst` file or the `.html` files. ```python loader = DirectoryLoader('../', glob=\"**/*.md\") ``` ```python docs = loader.load() ``` ```python len(docs) ``` ``` 1 ``` ## Show a progress bar By default a progress bar will not be shown. To show a progress bar, install the `tqdm` library (e.g. `pip install tqdm`), and set the `show_progress` parameter to `True`. ```python loader = DirectoryLoader('../', glob=\"**/*.md\", show_progress=True) docs = loader.load() ``` ``` Requirement already satisfied: tqdm in /Users/jon/.pyenv/versions/3.9.16/envs/microbiome-app/lib/python3.9/site-packages (4.65.0) 0it [00:00, ?it/s] ``` ## Use multithreading By default the loading happens in one thread. In order to utilize several threads set the `use_multithreading` flag to true. ```python loader = DirectoryLoader('../', glob=\"**/*.md\", use_multithreading=True) docs = loader.load() ``` ## Change loader class By default this uses the `UnstructuredLoader` class. However, you can change up the type of loader pretty easily. ```python from langchain.document_loaders import TextLoader ``` ```python loader = DirectoryLoader('../', glob=\"**/*.md\", loader_cls=TextLoader) ``` ```python docs = loader.load() ``` ```python len(docs) ``` ``` 1 ``` If you need to load Python source code files, use the `PythonLoader`. ```python from langchain.document_loaders import PythonLoader ``` ```python loader = DirectoryLoader('../../../../../', glob=\"**/*.py\", loader_cls=PythonLoader) ``` ```python docs = loader.load() ``` ```python len(docs) ``` ``` 691 ``` ## Auto-detect file encodings with TextLoader In this example we will see some strategies that can be useful when loading a large list of arbitrary files from a directory using the `TextLoader` class. First to illustrate the problem, let's try to load multiple texts with arbitrary encodings. ```python path = '../../../../../tests/integration_tests/examples' loader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader) ``` ### A. Default Behavior ```python loader.load() ``` ```html \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 /data/source/langchain/langchain/document_loaders/text.py:29 in load \u2502 \u2502 \u2502 \u2502 26 \u2502 \u2502 text = \"\" \u2502 \u2502 27 \u2502 \u2502 with open(self.file_path, encoding=self.encoding) as f: \u2502 \u2502 28 \u2502 \u2502 \u2502 try: \u2502 \u2502 \u2771 29 \u2502 \u2502 \u2502 \u2502 text = f.read() \u2502 \u2502 30 \u2502 \u2502 \u2502 except UnicodeDecodeError as e: \u2502 \u2502 31 \u2502 \u2502 \u2502 \u2502 if self.autodetect_encoding: \u2502 \u2502 32 \u2502 \u2502 \u2502 \u2502 \u2502 detected_encodings = self.detect_file_encodings() \u2502 \u2502 \u2502 \u2502 /home/spike/.pyenv/versions/3.9.11/lib/python3.9/codecs.py:322 in decode \u2502 \u2502 \u2502 \u2502 319 \u2502 def decode(self, input, final=False): \u2502 \u2502 320 \u2502 \u2502 # decode input (taking the buffer into account) \u2502 \u2502 321 \u2502 \u2502 data = self.buffer + input \u2502 \u2502 \u2771 322 \u2502 \u2502 (result, consumed) = self._buffer_decode(data, self.errors, final) \u2502 \u2502 323 \u2502 \u2502 # keep undecoded input until the next call \u2502 \u2502 324 \u2502 \u2502 self.buffer = data[consumed:] \u2502 \u2502 325 \u2502 \u2502 return result \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f UnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte The above exception was the direct cause of the following exception: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 in &lt;module&gt;:1 \u2502 \u2502 \u2502 \u2502 \u2771 1 loader.load() \u2502 \u2502 2 \u2502 \u2502 \u2502 \u2502 /data/source/langchain/langchain/document_loaders/directory.py:84 in load \u2502 \u2502 \u2502 \u2502 81 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 if self.silent_errors: \u2502 \u2502 82 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 logger.warning(e) \u2502 \u2502 83 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 else: \u2502 \u2502 \u2771 84 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 raise e \u2502 \u2502 85 \u2502 \u2502 \u2502 \u2502 \u2502 finally: \u2502 \u2502 86 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 if pbar: \u2502 \u2502 87 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 pbar.update(1) \u2502 \u2502 \u2502 \u2502 /data/source/langchain/langchain/document_loaders/directory.py:78 in load \u2502 \u2502 \u2502 \u2502 75 \u2502 \u2502 \u2502 if i.is_file(): \u2502 \u2502 76 \u2502 \u2502 \u2502 \u2502 if _is_visible(i.relative_to(p)) or self.load_hidden: \u2502 \u2502 77 \u2502 \u2502 \u2502 \u2502 \u2502 try: \u2502 \u2502 \u2771 78 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 sub_docs = self.loader_cls(str(i), **self.loader_kwargs).load() \u2502 \u2502 79 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 docs.extend(sub_docs) \u2502 \u2502 80 \u2502 \u2502 \u2502 \u2502 \u2502 except Exception as e: \u2502 \u2502 81 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 if self.silent_errors: \u2502 \u2502 \u2502 \u2502 /data/source/langchain/langchain/document_loaders/text.py:44 in load \u2502 \u2502 \u2502 \u2502 41 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 except UnicodeDecodeError: \u2502 \u2502 42 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 continue \u2502 \u2502 43 \u2502 \u2502 \u2502 \u2502 else: \u2502 \u2502 \u2771 44 \u2502 \u2502 \u2502 \u2502 \u2502 raise RuntimeError(f\"Error loading {self.file_path}\") from e \u2502 \u2502 45 \u2502 \u2502 \u2502 except Exception as e: \u2502 \u2502 46 \u2502 \u2502 \u2502 \u2502 raise RuntimeError(f\"Error loading {self.file_path}\") from e \u2502 \u2502 47 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f RuntimeError: Error loading ../../../../../tests/integration_tests/examples/example-non-utf8.txt ``` The file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding. With the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded. ### B. Silent fail We can pass the parameter `silent_errors` to the `DirectoryLoader` to skip the files which could not be loaded and continue the load process. ```python loader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader, silent_errors=True) docs = loader.load() ``` ``` Error loading ../../../../../tests/integration_tests/examples/example-non-utf8.txt ``` ```python doc_sources = [doc.metadata['source'] for doc in docs] doc_sources ``` ``` ['../../../../../tests/integration_tests/examples/whatsapp_chat.txt', '../../../../../tests/integration_tests/examples/example-utf8.txt'] ``` ### C. Auto detect encodings We can also ask `TextLoader` to auto detect the file encoding before failing, by passing the `autodetect_encoding` to the loader class. ```python text_loader_kwargs={'autodetect_encoding': True} loader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs) docs = loader.load() ``` ```python doc_sources = [doc.metadata['source'] for doc in docs] doc_sources ``` ``` ['../../../../../tests/integration_tests/examples/example-non-utf8.txt', '../../../../../tests/integration_tests/examples/whatsapp_chat.txt', '../../../../../tests/integration_tests/examples/example-utf8.txt'] ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/document_loaders/html.mdx"}, "data": "# HTML >[The HyperText Markup Language or HTML]( is the standard markup language for documents designed to be displayed in a web browser. This covers how to load `HTML` documents into a document format that we can use downstream. ```python from langchain.document_loaders import UnstructuredHTMLLoader ``` ```python loader = UnstructuredHTMLLoader(\"example_data/fake-content.html\") ``` ```python data = loader.load() ``` ```python data ``` ``` [Document(page_content='My First Heading\\n\\nMy first paragraph.', lookup_str='', metadata={'source': 'example_data/fake-content.html'}, lookup_index=0)] ``` ## Loading HTML with BeautifulSoup4 We can also use `BeautifulSoup4` to load HTML documents using the `BSHTMLLoader`. This will extract the text from the HTML into `page_content`, and the page title as `title` into `metadata`. ```python from langchain.document_loaders import BSHTMLLoader ``` ```python loader = BSHTMLLoader(\"example_data/fake-content.html\") data = loader.load() data ``` ``` [Document(page_content='\\n\\nTest Title\\n\\n\\nMy First Heading\\nMy first paragraph.\\n\\n\\n', metadata={'source': 'example_data/fake-content.html', 'title': 'Test Title'})] ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/document_loaders/index.mdx"}, "data": "--- sidebar_position: 0 --- # Document loaders :::info Head to [Integrations](/docs/integrations/document_loaders/) for documentation on built-in document loader integrations with 3rd-party tools. ::: Use document loaders to load data from a source as `Document`'s. A `Document` is a piece of text and associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video. Document loaders provide a \"load\" method for loading data as documents from a configured source. They optionally implement a \"lazy load\" as well for lazily loading data into memory. ## Get started The simplest loader reads in a file as text and places it all into one document. ```python from langchain.document_loaders import TextLoader loader = TextLoader(\"./index.md\") loader.load() ``` ``` [ Document(page_content='---\\nsidebar_position: 0\\n---\\n# Document loaders\\n\\nUse document loaders to load data from a source as `Document`\\'s. A `Document` is a piece of text\\nand associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text\\ncontents of any web page, or even for loading a transcript of a YouTube video.\\n\\nEvery document loader exposes two methods:\\n1. \"Load\": load documents from the configured source\\n2. \"Load and split\": load documents from the configured source and split them using the passed in text splitter\\n\\nThey optionally implement:\\n\\n3. \"Lazy load\": load documents into memory lazily\\n', metadata={'source': '../docs/docs/modules/data_connection/document_loaders/index.md'}) ] ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/document_loaders/json.mdx"}, "data": "# JSON >[JSON (JavaScript Object Notation)]( is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute\u2013value pairs and arrays (or other serializable values). >[JSON Lines]( is a file format where each line is a valid JSON value. >The `JSONLoader` uses a specified [jq schema]( to parse the JSON files. It uses the `jq` python package. Check this [manual]( for a detailed documentation of the `jq` syntax. ```python #!pip install jq ``` ```python from langchain.document_loaders import JSONLoader ``` ```python import json from pathlib import Path from pprint import pprint file_path='./example_data/facebook_chat.json' data = json.loads(Path(file_path).read_text()) ``` ```python pprint(data) ``` ``` {'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_chat.jpg'}, 'is_still_participant': True, 'joinable_mode': {'link': '', 'mode': 1}, 'magic_words': [], 'messages': [{'content': 'Bye!', 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}, {'content': 'Oh no worries! Bye', 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}, {'content': 'No Im sorry it was my mistake, the blue one is not ' 'for sale', 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}, {'content': 'I thought you were selling the blue one!', 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}, {'content': 'Im not interested in this bag. Im interested in the ' 'blue one!', 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}, {'content': 'Here is $129', 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}, {'photos': [{'creation_timestamp': 1675595059, 'uri': 'url_of_some_picture.jpg'}], 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}, {'content': 'Online is at least $100', 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}, {'content': 'How much do you want?', 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}, {'content': 'Goodmorning! $50 is too low.', 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}, {'content': 'Hi! Im interested in your bag. Im offering $50. Let ' 'me know if you are interested. Thanks!', 'sender_name': 'User 1', 'timestamp_ms': 1675549022673}], 'participants': [{'name': 'User 1'}, {'name': 'User 2'}], 'thread_path': 'inbox/User 1 and User 2 chat', 'title': 'User 1 and User 2 chat'} ``` ## Using `JSONLoader` Suppose we are interested in extracting the values under the `content` field within the `messages` key of the JSON data. This can easily be done through the `JSONLoader` as shown below. ### JSON file ```python loader = JSONLoader( file_path='./example_data/facebook_chat.json', jq_schema='.messages[].content', text_content=False) data = loader.load() ``` ```python pprint(data) ``` ``` [Document(page_content='Bye!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1}), Document(page_content='Oh no worries! Bye', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2}), Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3}), Document(page_content='I thought you were selling the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4}), Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5}), Document(page_content='Here is $129', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6}), Document(page_content='', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7}), Document(page_content='Online is at least $100', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8}), Document(page_content='How much do you want?', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9}), Document(page_content='Goodmorning! $50 is too low.', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10}), Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11})] ``` ### JSON Lines file If you want to load documents from a JSON Lines file, you pass `json_lines=True` and specify `jq_schema` to extract `page_content` from a single JSON object. ```python file_path = './example_data/facebook_chat_messages.jsonl' pprint(Path(file_path).read_text()) ``` ``` ('{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675597571851, \"content\": \"Bye!\"}\\n' '{\"sender_name\": \"User 1\", \"timestamp_ms\": 1675597435669, \"content\": \"Oh no ' 'worries! Bye\"}\\n' '{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675596277579, \"content\": \"No Im ' 'sorry it was my mistake, the blue one is not for sale\"}\\n') ``` ```python loader = JSONLoader( file_path='./example_data/facebook_chat_messages.jsonl', jq_schema='.content', text_content=False, json_lines=True) data = loader.load() ``` ```python pprint(data) ``` ``` [Document(page_content='Bye!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 1}), Document(page_content='Oh no worries! Bye', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 2}), Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 3})] ``` Another option is set `jq_schema='.'` and provide `content_key`: ```python loader = JSONLoader( file_path='./example_data/facebook_chat_messages.jsonl', jq_schema='.', content_key='sender_name', json_lines=True) data = loader.load() ``` ```python pprint(data) ``` ``` [Document(page_content='User 2', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 1}), Document(page_content='User 1', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 2}), Document(page_content='User 2', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 3})] ``` ## Extracting metadata Generally, we want to include metadata available in the JSON file into the documents that we create from the content. The following demonstrates how metadata can be extracted using the `JSONLoader`. There are some key changes to be noted. In the previous example where we didn't collect the metadata, we managed to directly specify in the schema where the value for the `page_content` can be extracted from. ``` .messages[].content ``` In the current example, we have to tell the loader to iterate over the records in the `messages` field. The jq_schema then has to be: ``` .messages[] ``` This allows us to pass the records (dict) into the `metadata_func` that has to be implemented. The `metadata_func` is responsible for identifying which pieces of information in the record should be included in the metadata stored in the final `Document` object. Additionally, we now have to explicitly specify in the loader, via the `content_key` argument, the key from the record where the value for the `page_content` needs to be extracted from. ```python # Define the metadata extraction function. def metadata_func(record: dict, metadata: dict) -> dict: metadata[\"sender_name\"] = record.get(\"sender_name\") metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\") return metadata loader = JSONLoader( file_path='./example_data/facebook_chat.json', jq_schema='.messages[]', content_key=\"content\", metadata_func=metadata_func ) data = loader.load() ``` ```python pprint(data) ``` ``` [Document(page_content='Bye!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}), Document(page_content='Oh no worries! Bye', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}), Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}), Document(page_content='I thought you were selling the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}), Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}), Document(page_content='Here is $129', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}), Document(page_content='', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}), Document(page_content='Online is at least $100', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}), Document(page_content='How much do you want?', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}), Document(page_content='Goodmorning! $50 is too low.', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}), Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673})] ``` Now, you will see that the documents contain the metadata associated with the content we extracted. ## The `metadata_func` As shown above, the `metadata_func` accepts the default metadata generated by the `JSONLoader`. This allows full control to the user with respect to how the metadata is formatted. For example, the default metadata contains the `source` and the `seq_num` keys. However, it is possible that the JSON data contain these keys as well. The user can then exploit the `metadata_func` to rename the default keys and use the ones from the JSON data. The example below shows how we can modify the `source` to only contain information of the file source relative to the `langchain` directory. ```python # Define the metadata extraction function. def metadata_func(record: dict, metadata: dict) -> dict: metadata[\"sender_name\"] = record.get(\"sender_name\") metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\") if \"source\" in metadata: source = metadata[\"source\"].split(\"/\") source = source[source.index(\"langchain\"):] metadata[\"source\"] = \"/\".join(source) return metadata loader = JSONLoader( file_path='./example_data/facebook_chat.json', jq_schema='.messages[]', content_key=\"content\", metadata_func=metadata_func ) data = loader.load() ``` ```python pprint(data) ``` ``` [Document(page_content='Bye!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}), Document(page_content='Oh no worries! Bye', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}), Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}), Document(page_content='I thought you were selling the blue one!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}), Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}), Document(page_content='Here is $129', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}), Document(page_content='', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}), Document(page_content='Online is at least $100', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}), Document(page_content='How much do you want?', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}), Document(page_content='Goodmorning! $50 is too low.', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}), Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673})] ``` ## Common JSON structures with jq schema The list below provides a reference to the possible `jq_schema` the user can use to extract content from the JSON data depending on the structure. ``` JSON -> [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}] jq_schema -> \".[].text\" JSON -> {\"key\": [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]} jq_schema -> \".key[].text\" JSON -> [\"...\", \"...\", \"...\"] jq_schema -> \".[]\" ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/document_loaders/markdown.mdx"}, "data": "# Markdown >[Markdown]( is a lightweight markup language for creating formatted text using a plain-text editor. This covers how to load `Markdown` documents into a document format that we can use downstream. ```python # !pip install unstructured > /dev/null ``` ```python from langchain.document_loaders import UnstructuredMarkdownLoader ``` ```python markdown_path = \"../../../../../README.md\" loader = UnstructuredMarkdownLoader(markdown_path) ``` ```python data = loader.load() ``` ```python data ``` ``` [Document(page_content=\"\u00f0\\x9f\u00a6\\x9c\u00ef\u00b8\\x8f\u00f0\\x9f\u201d\\x97 LangChain\\n\\n\u00e2\\x9a\u00a1 Building applications with LLMs through composability \u00e2\\x9a\u00a1\\n\\nLooking for the JS/TS version? Check out LangChain.js.\\n\\nProduction Support: As you move your LangChains into production, we'd love to offer more comprehensive support.\\nPlease fill out this form and we'll set up a dedicated support Slack channel.\\n\\nQuick Install\\n\\npip install langchain\\nor\\nconda install langchain -c conda-forge\\n\\n\u00f0\\x9f\u00a4\u201d What is this?\\n\\nLarge language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. However, using these LLMs in isolation is often insufficient for creating a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.\\n\\nThis library aims to assist in the development of those types of applications. Common examples of these applications include:\\n\\n\u00e2\\x9d\u201c Question Answering over specific documents\\n\\nDocumentation\\n\\nEnd-to-end Example: Question Answering over Notion Database\\n\\n\u00f0\\x9f\u2019\u00ac Chatbots\\n\\nDocumentation\\n\\nEnd-to-end Example: Chat-LangChain\\n\\n\u00f0\\x9f\u00a4\\x96 Agents\\n\\nDocumentation\\n\\nEnd-to-end Example: GPT+WolframAlpha\\n\\n\u00f0\\x9f\u201c\\x96 Documentation\\n\\nPlease see here for full documentation on:\\n\\nGetting started (installation, setting up the environment, simple examples)\\n\\nHow-To examples (demos, integrations, helper functions)\\n\\nReference (full API docs)\\n\\nResources (high-level explanation of core concepts)\\n\\n\u00f0\\x9f\\x9a\\x80 What can this help with?\\n\\nThere are six main areas that LangChain is designed to help with.\\nThese are, in increasing order of complexity:\\n\\n\u00f0\\x9f\u201c\\x83 LLMs and Prompts:\\n\\nThis includes prompt management, prompt optimization, a generic interface for all LLMs, and common utilities for working with LLMs.\\n\\n\u00f0\\x9f\u201d\\x97 Chains:\\n\\nChains go beyond a single LLM call and involve sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\n\\n\u00f0\\x9f\u201c\\x9a Data Augmented Generation:\\n\\nData Augmented Generation involves specific types of chains that first interact with an external data source to fetch data for use in the generation step. Examples include summarization of long pieces of text and question/answering over specific data sources.\\n\\n\u00f0\\x9f\u00a4\\x96 Agents:\\n\\nAgents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end-to-end agents.\\n\\n\u00f0\\x9f\u00a7\\xa0 Memory:\\n\\nMemory refers to persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\n\\n\u00f0\\x9f\u00a7\\x90 Evaluation:\\n\\n[BETA] Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\\n\\nFor more information on these concepts, please see our full documentation.\\n\\n\u00f0\\x9f\u2019\\x81 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see here.\", metadata={'source': '../../../../../README.md'})] ``` ## Retain Elements Under the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying `mode=\"elements\"`. ```python loader = UnstructuredMarkdownLoader(markdown_path, mode=\"elements\") ``` ```python data = loader.load() ``` ```python data[0] ``` ``` Document(page_content='\u00f0\\x9f\u00a6\\x9c\u00ef\u00b8\\x8f\u00f0\\x9f\u201d\\x97 LangChain', metadata={'source': '../../../../../README.md', 'page_number': 1, 'category': 'Title'}) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/document_loaders/pdf.mdx"}, "data": "# PDF >[Portable Document Format (PDF)]( standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems. This covers how to load `PDF` documents into the Document format that we use downstream. ## Using PyPDF Load PDF using `pypdf` into array of documents, where each document contains the page content and metadata with `page` number. ```bash pip install pypdf ``` ```python from langchain.document_loaders import PyPDFLoader loader = PyPDFLoader(\"example_data/layout-parser-paper.pdf\") pages = loader.load_and_split() ``` ```python pages[0] ``` ``` Document(page_content='LayoutParser : A Uni\\x0ced Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1( \\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1Allen Institute for AI\\nshannons@allenai.org\\n2Brown University\\nruochen zhang@brown.edu\\n3Harvard University\\nfmelissadell,jacob carlson g@fas.harvard.edu\\n4University of Washington\\nbcgl@cs.washington.edu\\n5University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model con\\x0cgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\ne\\x0borts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser , an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at .\\nKeywords: Document Image Analysis \u00b7Deep Learning \u00b7Layout Analysis\\n\u00b7Character Recognition \u00b7Open Source library \u00b7Toolkit.\\n1 Introduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classi\\x0ccation [ 11,arXiv:2103.15348v2 [cs.CV] 21 Jun 2021', metadata={'source': 'example_data/layout-parser-paper.pdf', 'page': 0}) ``` An advantage of this approach is that documents can be retrieved with page numbers. We want to use `OpenAIEmbeddings` so we have to get the OpenAI API Key. ```python import os import getpass os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:') ``` ``` OpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7 ``` ```python from langchain.vectorstores import FAISS from langchain.embeddings.openai import OpenAIEmbeddings faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings()) docs = faiss_index.similarity_search(\"How will the community be engaged?\", k=2) for doc in docs: print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300]) ``` ``` 9: 10 Z. Shen et al. Fig. 4: Illustration of (a) the original historical Japanese document with layout detection results and (b) a recreated version of the document image that achieves much better character recognition recall. The reorganization algorithm rearranges the tokens based on the their detect 3: 4 Z. Shen et al. Efficient Data AnnotationC u s t o m i z e d M o d e l T r a i n i n gModel Cust omizationDI A Model HubDI A Pipeline SharingCommunity PlatformLa y out Detection ModelsDocument Images T h e C o r e L a y o u t P a r s e r L i b r a r yOCR ModuleSt or age & VisualizationLa y ou ``` ### Extracting images Using the `rapidocr-onnxruntime` package we can extract images as text as well: ```bash pip install rapidocr-onnxruntime ``` ```python loader = PyPDFLoader(\" extract_images=True) pages = loader.load() pages[4].page_content ``` ``` 'LayoutParser : A Uni\ufb01ed Toolkit for DL-Based DIA 5\\nTable 1: Current layout detection models in the LayoutParser model zoo\\nDataset Base Model1Large Model Notes\\nPubLayNet [38] F / M M Layouts of modern scienti\ufb01c documents\\nPRImA [3] M - Layouts of scanned modern magazines and scienti\ufb01c reports\\nNewspaper [17] F - Layouts of scanned US newspapers from the 20th century\\nTableBank [18] F F Table region on modern scienti\ufb01c and business document\\nHJDataset [31] F / M - Layouts of history Japanese documents\\n1For each dataset, we train several models of di\ufb00erent sizes for di\ufb00erent needs (the trade-o\ufb00 between accuracy\\nvs. computational cost). For \u201cbase model\u201d and \u201clarge model\u201d, we refer to using the ResNet 50 or ResNet 101\\nbackbones [ 13], respectively. One can train models of di\ufb00erent architectures, like Faster R-CNN [ 28] (F) and Mask\\nR-CNN [ 12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\\nusing the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\\nzoo in coming months.\\nlayout data structures , which are optimized for e\ufb03ciency and versatility. 3) When\\nnecessary, users can employ existing or customized OCR models via the uni\ufb01ed\\nAPI provided in the OCR module . 4)LayoutParser comes with a set of utility\\nfunctions for the visualization and storage of the layout data. 5) LayoutParser\\nis also highly customizable, via its integration with functions for layout data\\nannotation and model training . We now provide detailed descriptions for each\\ncomponent.\\n3.1 Layout Detection Models\\nInLayoutParser , a layout model takes a document image as an input and\\ngenerates a list of rectangular boxes for the target content regions. Di\ufb00erent\\nfrom traditional methods, it relies on deep convolutional neural networks rather\\nthan manually curated rules to identify content regions. It is formulated as an\\nobject detection problem and state-of-the-art models like Faster R-CNN [ 28] and\\nMask R-CNN [ 12] are used. This yields prediction results of high accuracy and\\nmakes it possible to build a concise, generalized interface for layout detection.\\nLayoutParser , built upon Detectron2 [ 35], provides a minimal API that can\\nperform layout detection with only four lines of code in Python:\\n1import layoutparser as lp\\n2image = cv2. imread (\" image_file \") # load images\\n3model = lp. Detectron2LayoutModel (\\n4 \"lp :// PubLayNet / faster_rcnn_R_50_FPN_3x / config \")\\n5layout = model . detect ( image )\\nLayoutParser provides a wealth of pre-trained model weights using various\\ndatasets covering di\ufb00erent languages, time periods, and document types. Due to\\ndomain shift [ 7], the prediction performance can notably drop when models are ap-\\nplied to target samples that are signi\ufb01cantly di\ufb00erent from the training dataset. As\\ndocument structures and layouts vary greatly in di\ufb00erent domains, it is important\\nto select models trained on a dataset similar to the test samples. A semantic syntax\\nis used for initializing the model weights in LayoutParser , using both the dataset\\nname and model name lp:/// .' ``` ## Using MathPix Inspired by Daniel Gross's [ ```python from langchain.document_loaders import MathpixPDFLoader ``` ```python loader = MathpixPDFLoader(\"example_data/layout-parser-paper.pdf\") ``` ```python data = loader.load() ``` ## Using Unstructured ```python from langchain.document_loaders import UnstructuredPDFLoader ``` ```python loader = UnstructuredPDFLoader(\"example_data/layout-parser-paper.pdf\") ``` ```python data = loader.load() ``` ### Retain Elements Under the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying `mode=\"elements\"`. ```python loader = UnstructuredPDFLoader(\"example_data/layout-parser-paper.pdf\", mode=\"elements\") ``` ```python data = loader.load() ``` ```python data[0] ``` ``` Document(page_content='LayoutParser: A Uni\ufb01ed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1 (\ufffd), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1 Allen Institute for AI\\nshannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\n{melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n5 University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model con\ufb01gurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\ne\ufb00orts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at Document Image Analysis \u00b7 Deep Learning \u00b7 Layout Analysis\\n\u00b7 Character Recognition \u00b7 Open Source library \u00b7 Toolkit.\\n1\\nIntroduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classi\ufb01cation [11,\\narXiv:2103.15348v2 [cs.CV] 21 Jun 2021\\n', lookup_str='', metadata={'file_path': 'example_data/layout-parser-paper.pdf', 'page_number': 1, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': '', 'encryption': None}, lookup_index=0) ``` ### Fetching remote PDFs using Unstructured This covers how to load online PDFs into a document format that we can use downstream. This can be used for various online PDF sites such as and Note: all other PDF loaders can also be used to fetch remote PDFs, but `OnlinePDFLoader` is a legacy function, and works specifically with `UnstructuredPDFLoader`. ```python from langchain.document_loaders import OnlinePDFLoader ``` ```python loader = OnlinePDFLoader(\" ``` ```python data = loader.load() ``` ```python print(data) ``` ``` [Document(page_content='A WEAK ( k, k ) -LEFSCHETZ THEOREM FOR PROJECTIVE TORIC ORBIFOLDS\\n\\nWilliam D. Montoya\\n\\nInstituto de Matem\u00b4atica, Estat\u00b4\u0131stica e Computa\u00b8c\u02dcao Cient\u00b4\u0131\ufb01ca,\\n\\nIn [3] we proved that, under suitable conditions, on a very general codimension s quasi- smooth intersection subvariety X in a projective toric orbifold P d \u03a3 with d + s = 2 ( k + 1 ) the Hodge conjecture holds, that is, every ( p, p ) -cohomology class, under the Poincar\u00b4e duality is a rational linear combination of fundamental classes of algebraic subvarieties of X . The proof of the above-mentioned result relies, for p \u2260 d + 1 \u2212 s , on a Lefschetz\\n\\nKeywords: (1,1)- Lefschetz theorem, Hodge conjecture, toric varieties, complete intersection Email: wmontoya@ime.unicamp.br\\n\\ntheorem ([7]) and the Hard Lefschetz theorem for projective orbifolds ([11]). When p = d + 1 \u2212 s the proof relies on the Cayley trick, a trick which associates to X a quasi-smooth hypersurface Y in a projective vector bundle, and the Cayley Proposition (4.3) which gives an isomorphism of some primitive cohomologies (4.2) of X and Y . The Cayley trick, following the philosophy of Mavlyutov in [7], reduces results known for quasi-smooth hypersurfaces to quasi-smooth intersection subvarieties. The idea in this paper goes the other way around, we translate some results for quasi-smooth intersection subvarieties to\\n\\nAcknowledgement. I thank Prof. Ugo Bruzzo and Tiago Fonseca for useful discus- sions. I also acknowledge support from FAPESP postdoctoral grant No. 2019/23499-7.\\n\\nLet M be a free abelian group of rank d , let N = Hom ( M, Z ) , and N R = N \u2297 Z R .\\n\\nif there exist k linearly independent primitive elements e\\n\\n, . . . , e k \u2208 N such that \u03c3 = { \u00b5\\n\\ne\\n\\n+ \u22ef + \u00b5 k e k } . \u2022 The generators e i are integral if for every i and any nonnegative rational number \u00b5 the product \u00b5e i is in N only if \u00b5 is an integer. \u2022 Given two rational simplicial cones \u03c3 , \u03c3 \u2032 one says that \u03c3 \u2032 is a face of \u03c3 ( \u03c3 \u2032 and the zero locus Z ( \u03a3 ) \u2236 = V ( B \u03a3 ) in the a\ufb03ne space A d \u2236 = Spec ( S ) is the irrelevant locus.\\n\\nProposition 2.3 (Theorem 5.1.11 [5]) . The toric variety P d \u03a3 is a categorical quotient A d \u2216 Z ( \u03a3 ) by the group Hom ( Cl ( \u03a3 ) , C \u2217 ) and the group action is induced by the Cl ( \u03a3 ) - grading of S .\\n\\nNow we give a brief introduction to complex orbifolds and we mention the needed theorems for the next section. Namely: de Rham theorem and Dolbeault theorem for complex orbifolds.\\n\\nDe\ufb01nition 2.4. A complex orbifold of complex dimension d is a singular complex space whose singularities are locally isomorphic to quotient singularities C d / G , for \ufb01nite sub- groups G \u2282 Gl ( d, C ) .\\n\\nDe\ufb01nition 2.5. A di\ufb00erential form on a complex orbifold Z is de\ufb01ned locally at z \u2208 Z as a G -invariant di\ufb00erential form on C d where G \u2282 Gl ( d, C ) and Z is locally isomorphic to d\\n\\nRoughly speaking the local geometry of orbifolds reduces to local G -invariant geometry.\\n\\nWe have a complex of di\ufb00erential forms ( A \u25cf ( Z ) , d ) and a double complex ( A \u25cf , \u25cf ( Z ) , \u2202, \u00af \u2202 ) of bigraded di\ufb00erential forms which de\ufb01ne the de Rham and the Dolbeault cohomology groups (for a \ufb01xed p \u2208 N ) respectively:\\n\\n(1,1)-Lefschetz theorem for projective toric orbifolds\\n\\nDe\ufb01nition 3.1. A subvariety X \u2282 P d \u03a3 is quasi-smooth if V ( I X ) \u2282 A #\u03a3 ( 1 ) is smooth outside\\n\\nExample 3.2 . Quasi-smooth hypersurfaces or more generally quasi-smooth intersection sub-\\n\\nExample 3.2 . Quasi-smooth hypersurfaces or more generally quasi-smooth intersection sub- varieties are quasi-smooth subvarieties (see [2] or [7] for more details).\\n\\nRemark 3.3 . Quasi-smooth subvarieties are suborbifolds of P d \u03a3 in the sense of Satake in [8]. Intuitively speaking they are subvarieties whose only singularities come from the ambient\\n\\nProof. From the exponential short exact sequence\\n\\nwe have a long exact sequence in cohomology\\n\\nH 1 (O \u2217 X ) \u2192 H 2 ( X, Z ) \u2192 H 2 (O X ) \u2243 H 0 , 2 ( X )\\n\\nwhere the last isomorphisms is due to Steenbrink in [9]. Now, it is enough to prove the commutativity of the next diagram\\n\\nwhere the last isomorphisms is due to Steenbrink in [9]. Now,\\n\\nH 2 ( X, Z ) / / H 2 ( X, O X ) \u2243 Dolbeault H 2 ( X, C ) deRham \u2243 H 2 dR ( X, C ) / / H 0 , 2 \u00af \u2202 ( X )\\n\\nof the proof follows as the ( 1 , 1 ) -Lefschetz theorem in [6].\\n\\nRemark 3.5 . For k = 1 and P d \u03a3 as the projective space, we recover the classical ( 1 , 1 ) - Lefschetz theorem.\\n\\nBy the Hard Lefschetz Theorem for projective orbifolds (see [11] for details) we\\n\\nBy the Hard Lefschetz Theorem for projective orbifolds (see [11] for details) we get an isomorphism of cohomologies :\\n\\ngiven by the Lefschetz morphism and since it is a morphism of Hodge structures, we have:\\n\\nH 1 , 1 ( X, Q ) \u2243 H dim X \u2212 1 , dim X \u2212 1 ( X, Q )\\n\\nCorollary 3.6. If the dimension of X is 1 , 2 or 3 . The Hodge conjecture holds on X\\n\\nProof. If the dim C X = 1 the result is clear by the Hard Lefschetz theorem for projective orbifolds. The dimension 2 and 3 cases are covered by Theorem 3.5 and the Hard Lefschetz.\\n\\nCayley trick and Cayley proposition\\n\\nThe Cayley trick is a way to associate to a quasi-smooth intersection subvariety a quasi- smooth hypersurface. Let L 1 , . . . , L s be line bundles on P d \u03a3 and let \u03c0 \u2236 P ( E ) \u2192 P d \u03a3 be the projective space bundle associated to the vector bundle E = L 1 \u2295 \u22ef \u2295 L s . It is known that P ( E ) is a ( d + s \u2212 1 ) -dimensional simplicial toric variety whose fan depends on the degrees of the line bundles and the fan \u03a3. Furthermore, if the Cox ring, without considering the grading, of P d \u03a3 is C [ x 1 , . . . , x m ] then the Cox ring of P ( E ) is\\n\\nMoreover for X a quasi-smooth intersection subvariety cut o\ufb00 by f 1 , . . . , f s with deg ( f i ) = [ L i ] we relate the hypersurface Y cut o\ufb00 by F = y 1 f 1 + \u22c5 \u22c5 \u22c5 + y s f s which turns out to be quasi-smooth. For more details see Section 2 in [7].\\n\\nWe will denote P ( E ) as P d + s \u2212 1 \u03a3 ,X to keep track of its relation with X and P d \u03a3 .\\n\\nThe following is a key remark.\\n\\nRemark 4.1 . There is a morphism \u03b9 \u2236 X \u2192 Y \u2282 P d + s \u2212 1 \u03a3 ,X . Moreover every point z \u2236 = ( x, y ) \u2208 Y with y \u2260 0 has a preimage. Hence for any subvariety W = V ( I W ) \u2282 X \u2282 P d \u03a3 there exists W \u2032 \u2282 Y \u2282 P d + s \u2212 1 \u03a3 ,X such that \u03c0 ( W \u2032 ) = W , i.e., W \u2032 = { z = ( x, y ) \u2223 x \u2208 W } .\\n\\nFor X \u2282 P d \u03a3 a quasi-smooth intersection variety the morphism in cohomology induced by the inclusion i \u2217 \u2236 H d \u2212 s ( P d \u03a3 , C ) \u2192 H d \u2212 s ( X, C ) is injective by Proposition 1.4 in [7].\\n\\nDe\ufb01nition 4.2. The primitive cohomology of H d \u2212 s prim ( X ) is the quotient H d \u2212 s ( X, C )/ i \u2217 ( H d \u2212 s ( P d \u03a3 , C )) and H d \u2212 s prim ( X, Q ) with rational coe\ufb03cients.\\n\\nH d \u2212 s ( P d \u03a3 , C ) and H d \u2212 s ( X, C ) have pure Hodge structures, and the morphism i \u2217 is com- patible with them, so that H d \u2212 s prim ( X ) gets a pure Hodge structure.\\n\\nThe next Proposition is the Cayley proposition.\\n\\nProposition 4.3. [Proposition 2.3 in [3] ] Let X = X 1 \u2229\u22c5 \u22c5 \u22c5\u2229 X s be a quasi-smooth intersec- tion subvariety in P d \u03a3 cut o\ufb00 by homogeneous polynomials f 1 . . . f s . Then for p \u2260 d + s \u2212 1 2 , d + s \u2212 3 2\\n\\nRemark 4.5 . The above isomorphisms are also true with rational coe\ufb03cients since H \u25cf ( X, C ) = H \u25cf ( X, Q ) \u2297 Q C . See the beginning of Section 7.1 in [10] for more details.\\n\\nTheorem 5.1. Let Y = { F = y 1 f 1 + \u22ef + y k f k = 0 } \u2282 P 2 k + 1 \u03a3 ,X be the quasi-smooth hypersurface associated to the quasi-smooth intersection surface X = X f 1 \u2229 \u22c5 \u22c5 \u22c5 \u2229 X f k \u2282 P k + 2 \u03a3 . Then on Y the Hodge conjecture holds.\\n\\nthe Hodge conjecture holds.\\n\\nProof. If H k,k prim ( X, Q ) = 0 we are done. So let us assume H k,k prim ( X, Q ) \u2260 0. By the Cayley proposition H k,k prim ( Y, Q ) \u2243 H 1 , 1 prim ( X, Q ) and by the ( 1 , 1 ) -Lefschetz theorem for projective\\n\\ntoric orbifolds there is a non-zero algebraic basis \u03bb C 1 , . . . , \u03bb C n with rational coe\ufb03cients of H 1 , 1 prim ( X, Q ) , that is, there are n \u2236 = h 1 , 1 prim ( X, Q ) algebraic curves C 1 , . . . , C n in X such that under the Poincar\u00b4e duality the class in homology [ C i ] goes to \u03bb C i , [ C i ] \u21a6 \u03bb C i . Recall that the Cox ring of P k + 2 is contained in the Cox ring of P 2 k + 1 \u03a3 ,X without considering the grading. Considering the grading we have that if \u03b1 \u2208 Cl ( P k + 2 \u03a3 ) then ( \u03b1, 0 ) \u2208 Cl ( P 2 k + 1 \u03a3 ,X ) . So the polynomials de\ufb01ning C i \u2282 P k + 2 \u03a3 can be interpreted in P 2 k + 1 X, \u03a3 but with di\ufb00erent degree. Moreover, by Remark 4.1 each C i is contained in Y = { F = y 1 f 1 + \u22ef + y k f k = 0 } and\\n\\nfurthermore it has codimension k .\\n\\nClaim: { C i } ni = 1 is a basis of prim ( ) . It is enough to prove that \u03bb C i is di\ufb00erent from zero in H k,k prim ( Y, Q ) or equivalently that the cohomology classes { \u03bb C i } ni = 1 do not come from the ambient space. By contradiction, let us assume that there exists a j and C \u2282 P 2 k + 1 \u03a3 ,X such that \u03bb C \u2208 H k,k ( P 2 k + 1 \u03a3 ,X , Q ) with i \u2217 ( \u03bb C ) = \u03bb C j or in terms of homology there exists a ( k + 2 ) -dimensional algebraic subvariety V \u2282 P 2 k + 1 \u03a3 ,X such that V \u2229 Y = C j so they are equal as a homology class of P 2 k + 1 \u03a3 ,X ,i.e., [ V \u2229 Y ] = [ C j ] . It is easy to check that \u03c0 ( V ) \u2229 X = C j as a subvariety of P k + 2 \u03a3 where \u03c0 \u2236 ( x, y ) \u21a6 x . Hence [ \u03c0 ( V ) \u2229 X ] = [ C j ] which is equivalent to say that \u03bb C j comes from P k + 2 \u03a3 which contradicts the choice of [ C j ] .\\n\\nRemark 5.2 . Into the proof of the previous theorem, the key fact was that on X the Hodge conjecture holds and we translate it to Y by contradiction. So, using an analogous argument we have:\\n\\nargument we have:\\n\\nProposition 5.3. Let Y = { F = y 1 f s +\u22ef+ y s f s = 0 } \u2282 P 2 k + 1 \u03a3 ,X be the quasi-smooth hypersurface associated to a quasi-smooth intersection subvariety X = X f 1 \u2229 \u22c5 \u22c5 \u22c5 \u2229 X f s \u2282 P d \u03a3 such that d + s = 2 ( k + 1 ) . If the Hodge conjecture holds on X then it holds as well on Y .\\n\\nCorollary 5.4. If the dimension of Y is 2 s \u2212 1 , 2 s or 2 s + 1 then the Hodge conjecture holds on Y .\\n\\nProof. By Proposition 5.3 and Corollary 3.6.\\n\\n[\\n\\n] Angella, D. Cohomologies of certain orbifolds. Journal of Geometry and Physics\\n\\n(\\n\\n),\\n\\n\u2013\\n\\n[\\n\\n] Batyrev, V. V., and Cox, D. A. On the Hodge structure of projective hypersur- faces in toric varieties. Duke Mathematical Journal\\n\\n,\\n\\n(Aug\\n\\n). [\\n\\n] Bruzzo, U., and Montoya, W. On the Hodge conjecture for quasi-smooth in- tersections in toric varieties. S\u02dcao Paulo J. Math. Sci. Special Section: Geometry in Algebra and Algebra in Geometry (\\n\\n). [\\n\\n] Caramello Jr, F. C. Introduction to orbifolds. a\\n\\niv:\\n\\nv\\n\\n(\\n\\n). [\\n\\n] Cox, D., Little, J., and Schenck, H. Toric varieties, vol.\\n\\nAmerican Math- ematical Soc.,\\n\\n[\\n\\n] Griffiths, P., and Harris, J. Principles of Algebraic Geometry. John Wiley & Sons, Ltd,\\n\\n[\\n\\n] Mavlyutov, A. R. Cohomology of complete intersections in toric varieties. Pub- lished in Paci\ufb01c J. of Math.\\n\\nNo.\\n\\n(\\n\\n),\\n\\n\u2013\\n\\n[\\n\\n] Satake, I. On a Generalization of the Notion of Manifold. Proceedings of the National Academy of Sciences of the United States of America\\n\\n,\\n\\n(\\n\\n),\\n\\n\u2013\\n\\n[\\n\\n] Steenbrink, J. H. M. Intersection form for quasi-homogeneous singularities. Com- positio Mathematica\\n\\n,\\n\\n(\\n\\n),\\n\\n\u2013\\n\\n[\\n\\n] Voisin, C. Hodge Theory and Complex Algebraic Geometry I, vol.\\n\\nof Cambridge Studies in Advanced Mathematics . Cambridge University Press,\\n\\n[\\n\\n] Wang, Z. Z., and Zaffran, D. A remark on the Hard Lefschetz theorem for K\u00a8ahler orbifolds. Proceedings of the American Mathematical Society\\n\\n,\\n\\n(Aug\\n\\n).\\n\\n[2] Batyrev, V. V., and Cox, D. A. On the Hodge structure of projective hypersur- faces in toric varieties. Duke Mathematical Journal 75, 2 (Aug 1994).\\n\\n[\\n\\n] Bruzzo, U., and Montoya, W. On the Hodge conjecture for quasi-smooth in- tersections in toric varieties. S\u02dcao Paulo J. Math. Sci. Special Section: Geometry in Algebra and Algebra in Geometry (\\n\\n).\\n\\n[3] Bruzzo, U., and Montoya, W. On the Hodge conjecture for quasi-smooth in- tersections in toric varieties. S\u02dcao Paulo J. Math. Sci. Special Section: Geometry in Algebra and Algebra in Geometry (2021).\\n\\nA. R. Cohomology of complete intersections in toric varieties. Pub-', lookup_str='', metadata={'source': '/var/folders/ph/hhm7_zyx4l13k3v8z02dwp1w0000gn/T/tmpgq0ckaja/online_file.pdf'}, lookup_index=0)] ``` ## Using PyPDFium2 ```python from langchain.document_loaders import PyPDFium2Loader ``` ```python loader = PyPDFium2Loader(\"example_data/layout-parser-paper.pdf\") ``` ```python data = loader.load() ``` ## Using PDFMiner ```python from langchain.document_loaders import PDFMinerLoader ``` ```python loader = PDFMinerLoader(\"example_data/layout-parser-paper.pdf\") ``` ```python data = loader.load() ``` ### Using PDFMiner to generate HTML text This can be helpful for chunking texts semantically into sections as the output html content can be parsed via `BeautifulSoup` to get more structured and rich information about font size, page numbers, PDF headers/footers, etc. ```python from langchain.document_loaders import PDFMinerPDFasHTMLLoader ``` ```python loader = PDFMinerPDFasHTMLLoader(\"example_data/layout-parser-paper.pdf\") ``` ```python data = loader.load()[0] # entire PDF is loaded as a single Document ``` ```python from bs4 import BeautifulSoup soup = BeautifulSoup(data.page_content,'html.parser') content = soup.find_all('div') ``` ```python import re cur_fs = None cur_text = '' snippets = [] # first collect all snippets that have the same font size for c in content: sp = c.find('span') if not sp: continue st = sp.get('style') if not st: continue fs = re.findall('font-size:(\\d+)px',st) if not fs: continue fs = int(fs[0]) if not cur_fs: cur_fs = fs if fs == cur_fs: cur_text += c.text else: snippets.append((cur_text,cur_fs)) cur_fs = fs cur_text = c.text snippets.append((cur_text,cur_fs)) # Note: The above logic is very straightforward. One can also add more strategies such as removing duplicate snippets (as # headers/footers in a PDF appear on multiple pages so if we find duplicates it's safe to assume that it is redundant info) ``` ```python from langchain.docstore.document import Document cur_idx = -1 semantic_snippets = [] # Assumption: headings have higher font size than their respective content for s in snippets: # if current snippet's font size > previous section's heading => it is a new heading if not semantic_snippets or s[1] > semantic_snippets[cur_idx].metadata['heading_font']: metadata={'heading':s[0], 'content_font': 0, 'heading_font': s[1]} metadata.update(data.metadata) semantic_snippets.append(Document(page_content='',metadata=metadata)) cur_idx += 1 continue # if current snippet's font size content belongs to the same section (one can also create # a tree like structure for sub sections if needed but that may require some more thinking and may be data specific) if not semantic_snippets[cur_idx].metadata['content_font'] or s[1] previous section's content but less than previous section's heading than also make a new # section (e.g. title of a PDF will have the highest font size but we don't want it to subsume all sections) metadata={'heading':s[0], 'content_font': 0, 'heading_font': s[1]} metadata.update(data.metadata) semantic_snippets.append(Document(page_content='',metadata=metadata)) cur_idx += 1 ``` ```python semantic_snippets[4] ``` ``` Document(page_content='Recently, various DL models and datasets have been developed for layout analysis\\ntasks. The dhSegment [22] utilizes fully convolutional networks [20] for segmen-\\ntation tasks on historical documents. Object detection-based methods like Faster\\nR-CNN [28] and Mask R-CNN [12] are used for identifying document elements [38]\\nand detecting tables [30, 26]. Most recently, Graph Neural Networks [29] have also\\nbeen used in table detection [27]. However, these models are usually implemented\\nindividually and there is no uni\ufb01ed framework to load and use such models.\\nThere has been a surge of interest in creating open-source tools for document\\nimage processing: a search of document image analysis in Github leads to 5M\\nrelevant code pieces 6; yet most of them rely on traditional rule-based methods\\nor provide limited functionalities. The closest prior research to our work is the\\nOCR-D project7, which also tries to build a complete toolkit for DIA. However,\\nsimilar to the platform developed by Neudecker et al. [21], it is designed for\\nanalyzing historical documents, and provides no supports for recent DL models.\\nThe DocumentLayoutAnalysis project8 focuses on processing born-digital PDF\\ndocuments via analyzing the stored PDF data. Repositories like DeepLayout9\\nand Detectron2-PubLayNet10 are individual deep learning models trained on\\nlayout analysis datasets without support for the full DIA pipeline. The Document\\nAnalysis and Exploitation (DAE) platform [15] and the DeepDIVA project [2]\\naim to improve the reproducibility of DIA methods (or DL models), yet they\\nare not actively maintained. OCR engines like Tesseract [14], easyOCR11 and\\npaddleOCR12 usually do not come with comprehensive functionalities for other\\nDIA tasks like layout analysis.\\nRecent years have also seen numerous e\ufb00orts to create libraries for promoting\\nreproducibility and reusability in the \ufb01eld of DL. Libraries like Dectectron2 [35],\\n6 The number shown is obtained by specifying the search type as \u2018code\u2019.\\n7 Shen et al.\\nFig. 1: The overall architecture of LayoutParser. For an input document image,\\nthe core LayoutParser library provides a set of o\ufb00-the-shelf tools for layout\\ndetection, OCR, visualization, and storage, backed by a carefully designed layout\\ndata structure. LayoutParser also supports high level customization via e\ufb03cient\\nlayout annotation and model training functions. These improve model accuracy\\non the target samples. The community platform enables the easy sharing of DIA\\nmodels and whole digitization pipelines to promote reusability and reproducibility.\\nA collection of detailed documentation, tutorials and exemplar projects make\\nLayoutParser easy to learn and use.\\nAllenNLP [8] and transformers [34] have provided the community with complete\\nDL-based support for developing and deploying models for general computer\\nvision and natural language processing problems. LayoutParser, on the other\\nhand, specializes speci\ufb01cally in DIA tasks. LayoutParser is also equipped with a\\ncommunity platform inspired by established model hubs such as Torch Hub [23]\\nand TensorFlow Hub [1]. It enables the sharing of pretrained models as well as\\nfull document processing pipelines that are unique to DIA tasks.\\nThere have been a variety of document data collections to facilitate the\\ndevelopment of DL models. Some examples include PRImA [3](magazine layouts),\\nPubLayNet [38](academic paper layouts), Table Bank [18](tables in academic\\npapers), Newspaper Navigator Dataset [16, 17](newspaper \ufb01gure layouts) and\\nHJDataset [31](historical Japanese document layouts). A spectrum of models\\ntrained on these datasets are currently available in the LayoutParser model zoo\\nto support di\ufb00erent use cases.\\n', metadata={'heading': '2 Related Work\\n', 'content_font': 9, 'heading_font': 11, 'source': 'example_data/layout-parser-paper.pdf'}) ``` ## Using PyMuPDF This is the fastest of the PDF parsing options, and contains detailed metadata about the PDF and its pages, as well as returns one document per page. ```python from langchain.document_loaders import PyMuPDFLoader ``` ```python loader = PyMuPDFLoader(\"example_data/layout-parser-paper.pdf\") ``` ```python data = loader.load() ``` ```python data[0] ``` ``` Document(page_content='LayoutParser: A Uni\ufb01ed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1 (\ufffd), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1 Allen Institute for AI\\nshannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\n{melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n5 University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model con\ufb01gurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\ne\ufb00orts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at Document Image Analysis \u00b7 Deep Learning \u00b7 Layout Analysis\\n\u00b7 Character Recognition \u00b7 Open Source library \u00b7 Toolkit.\\n1\\nIntroduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classi\ufb01cation [11,\\narXiv:2103.15348v2 [cs.CV] 21 Jun 2021\\n', lookup_str='', metadata={'file_path': 'example_data/layout-parser-paper.pdf', 'page_number': 1, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': '', 'encryption': None}, lookup_index=0) ``` Additionally, you can pass along any of the options from the [PyMuPDF documentation]( as keyword arguments in the `load` call, and it will be pass along to the `get_text()` call. ## PyPDF Directory Load PDFs from directory ```python from langchain.document_loaders import PyPDFDirectoryLoader ``` ```python loader = PyPDFDirectoryLoader(\"example_data/\") ``` ```python docs = loader.load() ``` ## Using PDFPlumber Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page. ```python from langchain.document_loaders import PDFPlumberLoader ``` ```python loader = PDFPlumberLoader(\"example_data/layout-parser-paper.pdf\") ``` ```python data = loader.load() ``` ```python data[0] ``` ``` Document(page_content='LayoutParser: A Unified Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1 ((cid:0)), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1 Allen Institute for AI\\n1202 shannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\nnuJ {melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n12 5 University of Waterloo\\nw422li@uwaterloo.ca\\n]VC.sc[\\nAbstract. Recentadvancesindocumentimageanalysis(DIA)havebeen\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomescouldbeeasilydeployedinproductionandextendedforfurther\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model configurations complicate the easy reuse of im-\\n2v84351.3012:viXra portantinnovationsbyawideaudience.Thoughtherehavebeenon-going\\nefforts to improve reusability and simplify deep learning (DL) model\\ndevelopmentindisciplineslikenaturallanguageprocessingandcomputer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademicresearchacross awiderangeof disciplinesinthesocialsciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitiveinterfacesforapplyingandcustomizingDLmodelsforlayoutde-\\ntection,characterrecognition,andmanyotherdocumentprocessingtasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at DocumentImageAnalysis\u00b7DeepLearning\u00b7LayoutAnalysis\\n\u00b7 Character Recognition \u00b7 Open Source library \u00b7 Toolkit.\\n1 Introduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocumentimageanalysis(DIA)tasksincludingdocumentimageclassification[11,', metadata={'source': 'example_data/layout-parser-paper.pdf', 'file_path': 'example_data/layout-parser-paper.pdf', 'page': 1, 'total_pages': 16, 'Author': '', 'CreationDate': 'D:20210622012710Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210622012710Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}) ``` ## Using AmazonTextractPDFParser The AmazonTextractPDFLoader calls the [Amazon Textract Service]( to convert PDFs into a Document structure. The loader does pure OCR at the moment, with more features like layout support planned, depending on demand. Single and multi-page documents are supported with up to 3000 pages and 512 MB of size. For the call to be successful an AWS account is required, similar to the [AWS CLI]( requirements. Besides the AWS configuration, it is very similar to the other PDF loaders, while also supporting JPEG, PNG and TIFF and non-native PDF formats. ```python from langchain.document_loaders import AmazonTextractPDFLoader loader = AmazonTextractPDFLoader(\"example_data/alejandro_rosalez_sample-small.jpeg\") documents = loader.load() ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/document_transformers/index.mdx"}, "data": "--- sidebar_position: 1 --- # Document transformers :::info Head to [Integrations](/docs/integrations/document_transformers/) for documentation on built-in document transformer integrations with 3rd-party tools. ::: Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents. ## Text splitters When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that. At a high level, text splitters work as following: 1. Split the text up into small, semantically meaningful chunks (often sentences). 2. Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function). 3. Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks). That means there are two different axes along which you can customize your text splitter: 1. How the text is split 2. How the chunk size is measured ### Get started with text splitters The default recommended text splitter is the RecursiveCharacterTextSplitter. This text splitter takes a list of characters. It tries to create chunks based on splitting on the first character, but if any chunks are too large it then moves onto the next character, and so forth. By default the characters it tries to split on are `[\"\\n\\n\", \"\\n\", \" \", \"\"]` In addition to controlling which characters you can split on, you can also control a few other things: - `length_function`: how the length of chunks is calculated. Defaults to just counting number of characters, but it's pretty common to pass a token counter here. - `chunk_size`: the maximum size of your chunks (as measured by the length function). - `chunk_overlap`: the maximum overlap between chunks. It can be nice to have some overlap to maintain some continuity between chunks (e.g. do a sliding window). - `add_start_index`: whether to include the starting position of each chunk within the original document in the metadata. ```python # This is a long document we can split up. with open('../../state_of_the_union.txt') as f: state_of_the_union = f.read() ``` ```python from langchain.text_splitter import RecursiveCharacterTextSplitter ``` ```python text_splitter = RecursiveCharacterTextSplitter( # Set a really small chunk size, just to show. chunk_size = 100, chunk_overlap = 20, length_function = len, add_start_index = True, ) ``` ```python texts = text_splitter.create_documents([state_of_the_union]) print(texts[0]) print(texts[1]) ``` ``` page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and' metadata={'start_index': 0} page_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.' metadata={'start_index': 82} ``` ## Other transformations: ### Filter redundant docs, translate docs, extract metadata, and more We can do perform a number of transformations on docs which are not simply splitting the text. With the `EmbeddingsRedundantFilter` we can identify similar documents and filter out redundancies. With integrations like [doctran]( we can do things like translate documents from one language to another, extract desired properties and add them to metadata, and convert conversational dialogue into a Q/A format set of documents."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter.mdx"}, "data": "# Split by character This is the simplest method. This splits based on characters (by default \"\\n\\n\") and measure chunk length by number of characters. 1. How the text is split: by single character. 2. How the chunk size is measured: by number of characters. ```python # This is a long document we can split up. with open('../../../state_of_the_union.txt') as f: state_of_the_union = f.read() ``` ```python from langchain.text_splitter import CharacterTextSplitter text_splitter = CharacterTextSplitter( separator = \"\\n\\n\", chunk_size = 1000, chunk_overlap = 200, length_function = len, is_separator_regex = False, ) ``` ```python texts = text_splitter.create_documents([state_of_the_union]) print(texts[0]) ``` ``` page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia\u2019s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.' lookup_str='' metadata={} lookup_index=0 ``` Here's an example of passing metadata along with the documents, notice that it is split along with the documents. ```python metadatas = [{\"document\": 1}, {\"document\": 2}] documents = text_splitter.create_documents([state_of_the_union, state_of_the_union], metadatas=metadatas) print(documents[0]) ``` ``` page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia\u2019s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.' lookup_str='' metadata={'document': 1} lookup_index=0 ``` ```python text_splitter.split_text(state_of_the_union)[0] ``` ``` 'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia\u2019s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.' ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/document_transformers/text_splitters/code_splitter.mdx"}, "data": "# Split code CodeTextSplitter allows you to split your code with multiple languages supported. Import enum `Language` and specify the language. ```python from langchain.text_splitter import ( RecursiveCharacterTextSplitter, Language, ) ``` ```python # Full list of support languages [e.value for e in Language] ``` ``` ['cpp', 'go', 'java', 'kotlin', 'js', 'ts', 'php', 'proto', 'python', 'rst', 'ruby', 'rust', 'scala', 'swift', 'markdown', 'latex', 'html', 'sol', 'csharp'] ``` ```python # You can also see the separators used for a given language RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON) ``` ``` ['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', ''] ``` ## Python Here's an example using the PythonTextSplitter: ```python PYTHON_CODE = \"\"\" def hello_world(): print(\"Hello, World!\") # Call the function hello_world() \"\"\" python_splitter = RecursiveCharacterTextSplitter.from_language( language=Language.PYTHON, chunk_size=50, chunk_overlap=0 ) python_docs = python_splitter.create_documents([PYTHON_CODE]) python_docs ``` ``` [Document(page_content='def hello_world():\\n print(\"Hello, World!\")', metadata={}), Document(page_content='# Call the function\\nhello_world()', metadata={})] ``` ## JS Here's an example using the JS text splitter: ```python JS_CODE = \"\"\" function helloWorld() { console.log(\"Hello, World!\"); } // Call the function helloWorld(); \"\"\" js_splitter = RecursiveCharacterTextSplitter.from_language( language=Language.JS, chunk_size=60, chunk_overlap=0 ) js_docs = js_splitter.create_documents([JS_CODE]) js_docs ``` ``` [Document(page_content='function helloWorld() {\\n console.log(\"Hello, World!\");\\n}', metadata={}), Document(page_content='// Call the function\\nhelloWorld();', metadata={})] ``` ## TS Here's an example using the TS text splitter: ```python TS_CODE = \"\"\" function helloWorld(): void { console.log(\"Hello, World!\"); } // Call the function helloWorld(); \"\"\" ts_splitter = RecursiveCharacterTextSplitter.from_language( language=Language.TS, chunk_size=60, chunk_overlap=0 ) ts_docs = ts_splitter.create_documents([TS_CODE]) ts_docs ``` ``` [Document(page_content='function helloWorld(): void {\\n console.log(\"Hello, World!\");\\n}', metadata={}), Document(page_content='// Call the function\\nhelloWorld();', metadata={})] ``` ## Markdown Here's an example using the Markdown text splitter: ````python markdown_text = \"\"\" # LangChain Building applications with LLMs through composability ## Quick Install ```bash # Hopefully this code block isn't split pip install langchain ``` As an open-source project in a rapidly developing field, we are extremely open to contributions. \"\"\" ```` ```python md_splitter = RecursiveCharacterTextSplitter.from_language( language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0 ) md_docs = md_splitter.create_documents([markdown_text]) md_docs ``` ``` [Document(page_content='# LangChain', metadata={}), Document(page_content=' Building applications with LLMs through composability ', metadata={}), Document(page_content='## Quick Install', metadata={}), Document(page_content=\"```bash\\n# Hopefully this code block isn't split\", metadata={}), Document(page_content='pip install langchain', metadata={}), Document(page_content='```', metadata={}), Document(page_content='As an open-source project in a rapidly developing field, we', metadata={}), Document(page_content='are extremely open to contributions.', metadata={})] ``` ## Latex Here's an example on Latex text: ```python latex_text = \"\"\" \\documentclass{article} \\begin{document} \\maketitle \\section{Introduction} Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in a variety of natural language processing tasks, including language translation, text generation, and sentiment analysis. \\subsection{History of LLMs} The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance. \\subsection{Applications of LLMs} LLMs have many applications in industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics. \\end{document} \"\"\" ``` ```python latex_splitter = RecursiveCharacterTextSplitter.from_language( language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0 ) latex_docs = latex_splitter.create_documents([latex_text]) latex_docs ``` ``` [Document(page_content='\\\\documentclass{article}\\n\\n\\x08egin{document}\\n\\n\\\\maketitle', metadata={}), Document(page_content='\\\\section{Introduction}', metadata={}), Document(page_content='Large language models (LLMs) are a type of machine learning', metadata={}), Document(page_content='model that can be trained on vast amounts of text data to', metadata={}), Document(page_content='generate human-like language. In recent years, LLMs have', metadata={}), Document(page_content='made significant advances in a variety of natural language', metadata={}), Document(page_content='processing tasks, including language translation, text', metadata={}), Document(page_content='generation, and sentiment analysis.', metadata={}), Document(page_content='\\\\subsection{History of LLMs}', metadata={}), Document(page_content='The earliest LLMs were developed in the 1980s and 1990s,', metadata={}), Document(page_content='but they were limited by the amount of data that could be', metadata={}), Document(page_content='processed and the computational power available at the', metadata={}), Document(page_content='time. In the past decade, however, advances in hardware and', metadata={}), Document(page_content='software have made it possible to train LLMs on massive', metadata={}), Document(page_content='datasets, leading to significant improvements in', metadata={}), Document(page_content='performance.', metadata={}), Document(page_content='\\\\subsection{Applications of LLMs}', metadata={}), Document(page_content='LLMs have many applications in industry, including', metadata={}), Document(page_content='chatbots, content creation, and virtual assistants. They', metadata={}), Document(page_content='can also be used in academia for research in linguistics,', metadata={}), Document(page_content='psychology, and computational linguistics.', metadata={}), Document(page_content='\\\\end{document}', metadata={})] ``` ## HTML Here's an example using an HTML text splitter: ```python html_text = \"\"\" LangChain body { font-family: Arial, sans-serif; } h1 { color: darkblue; } LangChain Building applications with LLMs through composability As an open-source project in a rapidly developing field, we are extremely open to contributions. \"\"\" ``` ```python html_splitter = RecursiveCharacterTextSplitter.from_language( language=Language.HTML, chunk_size=60, chunk_overlap=0 ) html_docs = html_splitter.create_documents([html_text]) html_docs ``` ``` [Document(page_content='\\n', metadata={}), Document(page_content='\\n LangChain', metadata={}), Document(page_content='\\n body {\\n font-family: Aria', metadata={}), Document(page_content='l, sans-serif;\\n }\\n h1 {', metadata={}), Document(page_content='color: darkblue;\\n }\\n \\n ', metadata={}), Document(page_content='', metadata={}), Document(page_content='\\n LangChain', metadata={}), Document(page_content=' Building applications with LLMs through composability ', metadata={}), Document(page_content='\\n ', metadata={}), Document(page_content='\\n As an open-source project in a rapidly dev', metadata={}), Document(page_content='eloping field, we are extremely open to contributions.', metadata={}), Document(page_content='\\n \\n', metadata={})] ``` ## Solidity Here's an example using the Solidity text splitter: ```python SOL_CODE = \"\"\" pragma solidity ^0.8.20; contract HelloWorld { function add(uint a, uint b) pure public returns(uint) { return a + b; } } \"\"\" sol_splitter = RecursiveCharacterTextSplitter.from_language( language=Language.SOL, chunk_size=128, chunk_overlap=0 ) sol_docs = sol_splitter.create_documents([SOL_CODE]) sol_docs ``` ``` [ Document(page_content='pragma solidity ^0.8.20;', metadata={}), Document(page_content='contract HelloWorld {\\n function add(uint a, uint b) pure public returns(uint) {\\n return a + b;\\n }\\n}', metadata={}) ] ``` ## C# Here's an example using the C# text splitter: ```csharp using System; class Program { static void Main() { int age = 30; // Change the age value as needed // Categorize the age without any console output if (age = 18 && age ``` [Document(page_content='using System;', metadata={}), Document(page_content='class Program\\n{', metadata={}), Document(page_content='static void', metadata={}), Document(page_content='Main()', metadata={}), Document(page_content='{', metadata={}), Document(page_content='int age', metadata={}), Document(page_content='= 30; // Change', metadata={}), Document(page_content='the age value', metadata={}), Document(page_content='as needed', metadata={}), Document(page_content='//', metadata={}), Document(page_content='Categorize the', metadata={}), Document(page_content='age without any', metadata={}), Document(page_content='console output', metadata={}), Document(page_content='if (age', metadata={}), Document(page_content='= 18 &&', metadata={}), Document(page_content='age"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter.mdx"}, "data": "# Recursively split by character This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text. 1. How the text is split: by list of characters. 2. How the chunk size is measured: by number of characters. ```python # This is a long document we can split up. with open('../../../state_of_the_union.txt') as f: state_of_the_union = f.read() ``` ```python from langchain.text_splitter import RecursiveCharacterTextSplitter ``` ```python text_splitter = RecursiveCharacterTextSplitter( # Set a really small chunk size, just to show. chunk_size = 100, chunk_overlap = 20, length_function = len, is_separator_regex = False, ) ``` ```python texts = text_splitter.create_documents([state_of_the_union]) print(texts[0]) print(texts[1]) ``` ``` page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and' lookup_str='' metadata={} lookup_index=0 page_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.' lookup_str='' metadata={} lookup_index=0 ``` ```python text_splitter.split_text(state_of_the_union)[:2] ``` ``` ['Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and', 'of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.'] ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/index.mdx"}, "data": "--- sidebar_position: 1 sidebar_class_name: hidden --- # Retrieval Many LLM applications require user-specific data that is not part of the model's training set. The primary way of accomplishing this is through Retrieval Augmented Generation (RAG). In this process, external data is *retrieved* and then passed to the LLM when doing the *generation* step. LangChain provides all the building blocks for RAG applications - from simple to complex. This section of the documentation covers everything related to the *retrieval* step - e.g. the fetching of the data. Although this sounds simple, it can be subtly complex. This encompasses several key modules. ![data_connection_diagram](/img/data_connection.jpg) **[Document loaders](/docs/modules/data_connection/document_loaders/)** Load documents from many different sources. LangChain provides over 100 different document loaders as well as integrations with other major providers in the space, like AirByte and Unstructured. We provide integrations to load all types of documents (HTML, PDF, code) from all types of locations (private s3 buckets, public websites). **[Document transformers](/docs/modules/data_connection/document_transformers/)** A key part of retrieval is fetching only the relevant parts of documents. This involves several transformation steps in order to best prepare the documents for retrieval. One of the primary ones here is splitting (or chunking) a large document into smaller chunks. LangChain provides several different algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc). **[Text embedding models](/docs/modules/data_connection/text_embedding/)** Another key part of retrieval has become creating embeddings for documents. Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of text that are similar. LangChain provides integrations with over 25 different embedding providers and methods, from open-source to proprietary API, allowing you to choose the one best suited for your needs. LangChain provides a standard interface, allowing you to easily swap between models. **[Vector stores](/docs/modules/data_connection/vectorstores/)** With the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings. LangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones, allowing you to choose the one best suited for your needs. LangChain exposes a standard interface, allowing you to easily swap between vector stores. **[Retrievers](/docs/modules/data_connection/retrievers/)** Once the data is in the database, you still need to retrieve it. LangChain supports many different retrieval algorithms and is one of the places where we add the most value. We support basic methods that are easy to get started - namely simple semantic search. However, we have also added a collection of algorithms on top of this to increase performance. These include: - [Parent Document Retriever](/docs/modules/data_connection/retrievers/parent_document_retriever): This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context. - [Self Query Retriever](/docs/modules/data_connection/retrievers/self_query): User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the *semantic* part of a query from other *metadata filters* present in the query. - [Ensemble Retriever](/docs/modules/data_connection/retrievers/ensemble): Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this. - And more!"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/retrievers/contextual_compression/index.mdx"}, "data": "# Contextual compression One challenge with retrieval is that usually you don't know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses. Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. \u201cCompressing\u201d here refers to both compressing the contents of an individual document and filtering out documents wholesale. To use the Contextual Compression Retriever, you'll need: - a base retriever - a Document Compressor The Contextual Compression Retriever passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether. ![]( ## Get started ```python # Helper function for printing docs def pretty_print_docs(docs): print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)])) ``` ## Using a vanilla vector store retriever Let's start by initializing a simple vector store retriever and storing the 2023 State of the Union speech (in chunks). We can see that given an example question our retriever returns one or two relevant docs and a few irrelevant docs. And even the relevant docs have a lot of irrelevant information in them. ```python from langchain.text_splitter import CharacterTextSplitter from langchain.embeddings import OpenAIEmbeddings from langchain.document_loaders import TextLoader from langchain.vectorstores import FAISS documents = TextLoader('../../../state_of_the_union.txt').load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_documents(documents) retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever() docs = retriever.get_relevant_documents(\"What did the president say about Ketanji Brown Jackson\") pretty_print_docs(docs) ``` ``` Document 1: Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence. ---------------------------------------------------------------------------------------------------- Document 2: A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. We can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling. We\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers. We\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. We\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders. ---------------------------------------------------------------------------------------------------- Document 3: And for our LGBTQ+ Americans, let\u2019s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. While it often appears that we never agree, that isn\u2019t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. And soon, we\u2019ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. So tonight I\u2019m offering a Unity Agenda for the Nation. Four big things we can do together. First, beat the opioid epidemic. ---------------------------------------------------------------------------------------------------- Document 4: Tonight, I\u2019m announcing a crackdown on these companies overcharging American businesses and consumers. And as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up. That ends on my watch. Medicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. We\u2019ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. Let\u2019s pass the Paycheck Fairness Act and paid leave. Raise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. Let\u2019s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill\u2014our First Lady who teaches full-time\u2014calls America\u2019s best-kept secret: community colleges. ``` ## Adding contextual compression with an `LLMChainExtractor` Now let's wrap our base retriever with a `ContextualCompressionRetriever`. We'll add an `LLMChainExtractor`, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query. ```python from langchain.llms import OpenAI from langchain.retrievers import ContextualCompressionRetriever from langchain.retrievers.document_compressors import LLMChainExtractor llm = OpenAI(temperature=0) compressor = LLMChainExtractor.from_llm(llm) compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever) compressed_docs = compression_retriever.get_relevant_documents(\"What did the president say about Ketanji Jackson Brown\") pretty_print_docs(compressed_docs) ``` ``` Document 1: \"One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\" ---------------------------------------------------------------------------------------------------- Document 2: \"A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\" ``` ## More built-in compressors: filters ### `LLMChainFilter` The `LLMChainFilter` is slightly simpler but more robust compressor that uses an LLM chain to decide which of the initially retrieved documents to filter out and which ones to return, without manipulating the document contents. ```python from langchain.retrievers.document_compressors import LLMChainFilter _filter = LLMChainFilter.from_llm(llm) compression_retriever = ContextualCompressionRetriever(base_compressor=_filter, base_retriever=retriever) compressed_docs = compression_retriever.get_relevant_documents(\"What did the president say about Ketanji Jackson Brown\") pretty_print_docs(compressed_docs) ``` ``` Document 1: Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence. ``` ### `EmbeddingsFilter` Making an extra LLM call over each retrieved document is expensive and slow. The `EmbeddingsFilter` provides a cheaper and faster option by embedding the documents and query and only returning those documents which have sufficiently similar embeddings to the query. ```python from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers.document_compressors import EmbeddingsFilter embeddings = OpenAIEmbeddings() embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76) compression_retriever = ContextualCompressionRetriever(base_compressor=embeddings_filter, base_retriever=retriever) compressed_docs = compression_retriever.get_relevant_documents(\"What did the president say about Ketanji Jackson Brown\") pretty_print_docs(compressed_docs) ``` ``` Document 1: Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence. ---------------------------------------------------------------------------------------------------- Document 2: A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. We can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling. We\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers. We\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. We\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders. ---------------------------------------------------------------------------------------------------- Document 3: And for our LGBTQ+ Americans, let\u2019s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. While it often appears that we never agree, that isn\u2019t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. And soon, we\u2019ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. So tonight I\u2019m offering a Unity Agenda for the Nation. Four big things we can do together. First, beat the opioid epidemic. ``` # Stringing compressors and document transformers together Using the `DocumentCompressorPipeline` we can also easily combine multiple compressors in sequence. Along with compressors we can add `BaseDocumentTransformer`s to our pipeline, which don't perform any contextual compression but simply perform some transformation on a set of documents. For example `TextSplitter`s can be used as document transformers to split documents into smaller pieces, and the `EmbeddingsRedundantFilter` can be used to filter out redundant documents based on embedding similarity between documents. Below we create a compressor pipeline by first splitting our docs into smaller chunks, then removing redundant documents, and then filtering based on relevance to the query. ```python from langchain.document_transformers import EmbeddingsRedundantFilter from langchain.retrievers.document_compressors import DocumentCompressorPipeline from langchain.text_splitter import CharacterTextSplitter splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=\". \") redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings) relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76) pipeline_compressor = DocumentCompressorPipeline( transformers=[splitter, redundant_filter, relevant_filter] ) ``` ```python compression_retriever = ContextualCompressionRetriever(base_compressor=pipeline_compressor, base_retriever=retriever) compressed_docs = compression_retriever.get_relevant_documents(\"What did the president say about Ketanji Jackson Brown\") pretty_print_docs(compressed_docs) ``` ``` Document 1: One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson ---------------------------------------------------------------------------------------------------- Document 2: As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. While it often appears that we never agree, that isn\u2019t true. I signed 80 bipartisan bills into law last year ---------------------------------------------------------------------------------------------------- Document 3: A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/retrievers/time_weighted_vectorstore.mdx"}, "data": "# Time-weighted vector store retriever This retriever uses a combination of semantic similarity and a time decay. The algorithm for scoring them is: ``` semantic_similarity + (1.0 - decay_rate) ^ hours_passed ``` Notably, `hours_passed` refers to the hours passed since the object in the retriever **was last accessed**, not since it was created. This means that frequently accessed objects remain \"fresh\". ```python import faiss from datetime import datetime, timedelta from langchain.docstore import InMemoryDocstore from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers import TimeWeightedVectorStoreRetriever from langchain.schema import Document from langchain.vectorstores import FAISS ``` ## Low decay rate A low `decay rate` (in this, to be extreme, we will set it close to 0) means memories will be \"remembered\" for longer. A `decay rate` of 0 means memories never be forgotten, making this retriever equivalent to the vector lookup. ```python # Define your embedding model embeddings_model = OpenAIEmbeddings() # Initialize the vectorstore as empty embedding_size = 1536 index = faiss.IndexFlatL2(embedding_size) vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {}) retriever = TimeWeightedVectorStoreRetriever(vectorstore=vectorstore, decay_rate=.0000000000000000000000001, k=1) ``` ```python yesterday = datetime.now() - timedelta(days=1) retriever.add_documents([Document(page_content=\"hello world\", metadata={\"last_accessed_at\": yesterday})]) retriever.add_documents([Document(page_content=\"hello foo\")]) ``` ``` ['d7f85756-2371-4bdf-9140-052780a0f9b3'] ``` ```python # \"Hello World\" is returned first because it is most salient, and the decay rate is close to 0., meaning it's still recent enough retriever.get_relevant_documents(\"hello world\") ``` ``` [Document(page_content='hello world', metadata={'last_accessed_at': datetime.datetime(2023, 5, 13, 21, 0, 27, 678341), 'created_at': datetime.datetime(2023, 5, 13, 21, 0, 27, 279596), 'buffer_idx': 0})] ``` ## High decay rate With a high `decay rate` (e.g., several 9's), the `recency score` quickly goes to 0! If you set this all the way to 1, `recency` is 0 for all objects, once again making this equivalent to a vector lookup. ```python # Define your embedding model embeddings_model = OpenAIEmbeddings() # Initialize the vectorstore as empty embedding_size = 1536 index = faiss.IndexFlatL2(embedding_size) vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {}) retriever = TimeWeightedVectorStoreRetriever(vectorstore=vectorstore, decay_rate=.999, k=1) ``` ```python yesterday = datetime.now() - timedelta(days=1) retriever.add_documents([Document(page_content=\"hello world\", metadata={\"last_accessed_at\": yesterday})]) retriever.add_documents([Document(page_content=\"hello foo\")]) ``` ``` ['40011466-5bbe-4101-bfd1-e22e7f505de2'] ``` ```python # \"Hello Foo\" is returned first because \"hello world\" is mostly forgotten retriever.get_relevant_documents(\"hello world\") ``` ``` [Document(page_content='hello foo', metadata={'last_accessed_at': datetime.datetime(2023, 4, 16, 22, 9, 2, 494798), 'created_at': datetime.datetime(2023, 4, 16, 22, 9, 2, 178722), 'buffer_idx': 1})] ``` ## Virtual time Using some utils in LangChain, you can mock out the time component. ```python from langchain.utils import mock_now import datetime ``` ```python # Notice the last access time is that date time with mock_now(datetime.datetime(2011, 2, 3, 10, 11)): print(retriever.get_relevant_documents(\"hello world\")) ``` ``` [Document(page_content='hello world', metadata={'last_accessed_at': MockDateTime(2011, 2, 3, 10, 11), 'created_at': datetime.datetime(2023, 5, 13, 21, 0, 27, 279596), 'buffer_idx': 0})] ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/retrievers/vectorstore.mdx"}, "data": "# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it's very easy to construct a retriever. Let's walk through an example. ```python from langchain.document_loaders import TextLoader loader = TextLoader('../../../state_of_the_union.txt') ``` ```python from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import FAISS from langchain.embeddings import OpenAIEmbeddings documents = loader.load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_documents(documents) embeddings = OpenAIEmbeddings() db = FAISS.from_documents(texts, embeddings) ``` ``` Exiting: Cleaning up .chroma directory ``` ```python retriever = db.as_retriever() ``` ```python docs = retriever.get_relevant_documents(\"what did he say about ketanji brown jackson\") ``` ## Maximum marginal relevance retrieval By default, the vector store retriever uses similarity search. If the underlying vector store supports maximum marginal relevance search, you can specify that as the search type. ```python retriever = db.as_retriever(search_type=\"mmr\") ``` ```python docs = retriever.get_relevant_documents(\"what did he say about ketanji brown jackson\") ``` ## Similarity score threshold retrieval You can also set a retrieval method that sets a similarity score threshold and only returns documents with a score above that threshold. ```python retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .5}) ``` ```python docs = retriever.get_relevant_documents(\"what did he say about ketanji brown jackson\") ``` ## Specifying top k You can also specify search kwargs like `k` to use when doing retrieval. ```python retriever = db.as_retriever(search_kwargs={\"k\": 1}) ``` ```python docs = retriever.get_relevant_documents(\"what did he say about ketanji brown jackson\") ``` ```python len(docs) ``` ``` 1 ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/text_embedding/index.mdx"}, "data": "--- sidebar_position: 2 --- # Text embedding models :::info Head to [Integrations](/docs/integrations/text_embedding/) for documentation on built-in integrations with text embedding model providers. ::: The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them. Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space. The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself). ## Get started ### Setup To start we'll need to install the OpenAI Python package: ```bash pip install openai ``` Accessing the API requires an API key, which you can get by creating an account and heading [here]( Once we have a key we'll want to set it as an environment variable by running: ```bash export OPENAI_API_KEY=\"...\" ``` If you'd prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class: ```python from langchain.embeddings import OpenAIEmbeddings embeddings_model = OpenAIEmbeddings(openai_api_key=\"...\") ``` Otherwise you can initialize without any params: ```python from langchain.embeddings import OpenAIEmbeddings embeddings_model = OpenAIEmbeddings() ``` ### `embed_documents` #### Embed list of texts ```python embeddings = embeddings_model.embed_documents( [ \"Hi there!\", \"Oh, hello!\", \"What's your name?\", \"My friends call me World\", \"Hello World!\" ] ) len(embeddings), len(embeddings[0]) ``` ``` (5, 1536) ``` ### `embed_query` #### Embed single query Embed a single piece of text for the purpose of comparing to other embedded pieces of texts. ```python embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\") embedded_query[:5] ``` ``` [0.0053587136790156364, -0.0004999046213924885, 0.038883671164512634, -0.003001077566295862, -0.00900818221271038] ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/data_connection/vectorstores/index.mdx"}, "data": "--- sidebar_position: 3 --- # Vector stores :::info Head to [Integrations](/docs/integrations/vectorstores/) for documentation on built-in integrations with 3rd-party vector stores. ::: One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you. ![vector store diagram](/img/vector_stores.jpg) ## Get started This walkthrough showcases basic functionality related to vector stores. A key part of working with vector stores is creating the vector to put in them, which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the [text embedding model](/docs/modules/data_connection/text_embedding/) interfaces before diving into this. import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; There are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings. This walkthrough uses the `chroma` vector database, which runs on your local machine as a library. ```bash pip install chromadb ``` We want to use OpenAIEmbeddings so we have to get the OpenAI API Key. ```python import os import getpass os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:') ``` ```python from langchain.document_loaders import TextLoader from langchain.embeddings.openai import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import Chroma # Load the document, split it into chunks, embed each chunk and load it into the vector store. raw_documents = TextLoader('../../../state_of_the_union.txt').load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) documents = text_splitter.split_documents(raw_documents) db = Chroma.from_documents(documents, OpenAIEmbeddings()) ``` This walkthrough uses the `FAISS` vector database, which makes use of the Facebook AI Similarity Search (FAISS) library. ```bash pip install faiss-cpu ``` We want to use OpenAIEmbeddings so we have to get the OpenAI API Key. ```python import os import getpass os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:') ``` ```python from langchain.document_loaders import TextLoader from langchain.embeddings.openai import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import FAISS # Load the document, split it into chunks, embed each chunk and load it into the vector store. raw_documents = TextLoader('../../../state_of_the_union.txt').load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) documents = text_splitter.split_documents(raw_documents) db = FAISS.from_documents(documents, OpenAIEmbeddings()) ``` This notebook shows how to use functionality related to the LanceDB vector database based on the Lance data format. ```bash pip install lancedb ``` We want to use OpenAIEmbeddings so we have to get the OpenAI API Key. ```python import os import getpass os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:') ``` ```python from langchain.document_loaders import TextLoader from langchain.embeddings.openai import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import LanceDB import lancedb db = lancedb.connect(\"/tmp/lancedb\") table = db.create_table( \"my_table\", data=[ { \"vector\": embeddings.embed_query(\"Hello World\"), \"text\": \"Hello World\", \"id\": \"1\", } ], mode=\"overwrite\", ) # Load the document, split it into chunks, embed each chunk and load it into the vector store. raw_documents = TextLoader('../../../state_of_the_union.txt').load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) documents = text_splitter.split_documents(raw_documents) db = LanceDB.from_documents(documents, OpenAIEmbeddings(), connection=table) ``` ### Similarity search ```python query = \"What did the president say about Ketanji Brown Jackson\" docs = db.similarity_search(query) print(docs[0].page_content) ``` ``` Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence. ``` ### Similarity search by vector It is also possible to do a search for documents similar to a given embedding vector using `similarity_search_by_vector` which accepts an embedding vector as a parameter instead of a string. ```python embedding_vector = OpenAIEmbeddings().embed_query(query) docs = db.similarity_search_by_vector(embedding_vector) print(docs[0].page_content) ``` The query is the same, and so the result is also the same. ``` Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence. ``` ## Asynchronous operations Vector stores are usually run as a separate service that requires some IO operations, and therefore they might be called asynchronously. That gives performance benefits as you don't waste time waiting for responses from external services. That might also be important if you work with an asynchronous framework, such as [FastAPI]( LangChain supports async operation on vector stores. All the methods might be called using their async counterparts, with the prefix `a`, meaning `async`. `Qdrant` is a vector store, which supports all the async operations, thus it will be used in this walkthrough. ```bash pip install qdrant-client ``` ```python from langchain.vectorstores import Qdrant ``` ### Create a vector store asynchronously ```python db = await Qdrant.afrom_documents(documents, embeddings, \" ``` ### Similarity search ```python query = \"What did the president say about Ketanji Brown Jackson\" docs = await db.asimilarity_search(query) print(docs[0].page_content) ``` ``` Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence. ``` ### Similarity search by vector ```python embedding_vector = embeddings.embed_query(query) docs = await db.asimilarity_search_by_vector(embedding_vector) ``` ## Maximum marginal relevance search (MMR) Maximal marginal relevance optimizes for similarity to query **and** diversity among selected documents. It is also supported in async API. ```python query = \"What did the president say about Ketanji Brown Jackson\" found_docs = await qdrant.amax_marginal_relevance_search(query, k=2, fetch_k=10) for i, doc in enumerate(found_docs): print(f\"{i + 1}.\", doc.page_content, \"\\n\") ``` ``` 1. Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence. 2. We can\u2019t change how divided we\u2019ve been. But we can change how we move forward\u2014on COVID-19 and other issues we must face together. I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. Officer Mora was 27 years old. Officer Rivera was 22. Both Dominican Americans who\u2019d grown up on the same streets they later chose to patrol as police officers. I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. I\u2019ve worked on these issues a long time. I know what works: Investing in crime prevention and community police officers who\u2019ll walk the beat, who\u2019ll know the neighborhood, and who can restore trust and safety. ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/index.mdx"}, "data": "--- sidebar_class_name: hidden --- # Modules LangChain provides standard, extendable interfaces and external integrations for the following main modules: #### [Model I/O](/docs/modules/model_io/) Interface with language models #### [Retrieval](/docs/modules/data_connection/) Interface with application-specific data #### [Agents](/docs/modules/agents/) Let chains choose which tools to use given high-level directives ## Additional #### [Chains](/docs/modules/chains/) Common, building block compositions #### [Memory](/docs/modules/memory/) Persist application state between runs of a chain #### [Callbacks](/docs/modules/callbacks/) Log and stream intermediate steps of any chain"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/memory/chat_messages/index.mdx"}, "data": "--- sidebar_position: 1 --- # Chat Messages :::info Head to [Integrations](/docs/integrations/memory/) for documentation on built-in memory integrations with 3rd-party databases and tools. ::: One of the core utility classes underpinning most (if not all) memory modules is the `ChatMessageHistory` class. This is a super lightweight wrapper that provides convenience methods for saving HumanMessages, AIMessages, and then fetching them all. You may want to use this class directly if you are managing memory outside of a chain. ```python from langchain.memory import ChatMessageHistory history = ChatMessageHistory() history.add_user_message(\"hi!\") history.add_ai_message(\"whats up?\") ``` ```python history.messages ``` ``` [HumanMessage(content='hi!', additional_kwargs={}), AIMessage(content='whats up?', additional_kwargs={})] ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/memory/index.mdx"}, "data": "--- sidebar_position: 3 sidebar_class_name: hidden --- # Memory Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly. A more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships. We call this ability to store information about past interactions \"memory\". LangChain provides a lot of utilities for adding memory to a system. These utilities can be used by themselves or incorporated seamlessly into a chain. A memory system needs to support two basic actions: reading and writing. Recall that every chain defines some core execution logic that expects certain inputs. Some of these inputs come directly from the user, but some of these inputs can come from memory. A chain will interact with its memory system twice in a given run. 1. AFTER receiving the initial user inputs but BEFORE executing the core logic, a chain will READ from its memory system and augment the user inputs. 2. AFTER executing the core logic but BEFORE returning the answer, a chain will WRITE the inputs and outputs of the current run to memory, so that they can be referred to in future runs. ![memory-diagram](/img/memory_diagram.png) ## Building memory into a system The two core design decisions in any memory system are: - How state is stored - How state is queried ### Storing: List of chat messages Underlying any memory is a history of all chat interactions. Even if these are not all used directly, they need to be stored in some form. One of the key parts of the LangChain memory module is a series of integrations for storing these chat messages, from in-memory lists to persistent databases. - [Chat message storage](/docs/modules/memory/chat_messages/): How to work with Chat Messages, and the various integrations offered. ### Querying: Data structures and algorithms on top of chat messages Keeping a list of chat messages is fairly straight-forward. What is less straight-forward are the data structures and algorithms built on top of chat messages that serve a view of those messages that is most useful. A very simply memory system might just return the most recent messages each run. A slightly more complex memory system might return a succinct summary of the past K messages. An even more sophisticated system might extract entities from stored messages and only return information about entities referenced in the current run. Each application can have different requirements for how memory is queried. The memory module should make it easy to both get started with simple memory systems and write your own custom systems if needed. - [Memory types](/docs/modules/memory/types/): The various data structures and algorithms that make up the memory types LangChain supports ## Get started Let's take a look at what Memory actually looks like in LangChain. Here we'll cover the basics of interacting with an arbitrary memory class. Let's take a look at how to use `ConversationBufferMemory` in chains. `ConversationBufferMemory` is an extremely simple form of memory that just keeps a list of chat messages in a buffer and passes those into the prompt template. ```python from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory() memory.chat_memory.add_user_message(\"hi!\") memory.chat_memory.add_ai_message(\"what's up?\") ``` When using memory in a chain, there are a few key concepts to understand. Note that here we cover general concepts that are useful for most types of memory. Each individual memory type may very well have its own parameters and concepts that are necessary to understand. ### What variables get returned from memory Before going into the chain, various variables are read from memory. These have specific names which need to align with the variables the chain expects. You can see what these variables are by calling `memory.load_memory_variables({})`. Note that the empty dictionary that we pass in is just a placeholder for real variables. If the memory type you are using is dependent upon the input variables, you may need to pass some in. ```python memory.load_memory_variables({}) ``` ``` {'history': \"Human: hi!\\nAI: what's up?\"} ``` In this case, you can see that `load_memory_variables` returns a single key, `history`. This means that your chain (and likely your prompt) should expect an input named `history`. You can usually control this variable through parameters on the memory class. For example, if you want the memory variables to be returned in the key `chat_history` you can do: ```python memory = ConversationBufferMemory(memory_key=\"chat_history\") memory.chat_memory.add_user_message(\"hi!\") memory.chat_memory.add_ai_message(\"what's up?\") ``` ``` {'chat_history': \"Human: hi!\\nAI: what's up?\"} ``` The parameter name to control these keys may vary per memory type, but it's important to understand that (1) this is controllable, and (2) how to control it. ### Whether memory is a string or a list of messages One of the most common types of memory involves returning a list of chat messages. These can either be returned as a single string, all concatenated together (useful when they will be passed into LLMs) or a list of ChatMessages (useful when passed into ChatModels). By default, they are returned as a single string. In order to return as a list of messages, you can set `return_messages=True` ```python memory = ConversationBufferMemory(return_messages=True) memory.chat_memory.add_user_message(\"hi!\") memory.chat_memory.add_ai_message(\"what's up?\") ``` ``` {'history': [HumanMessage(content='hi!', additional_kwargs={}, example=False), AIMessage(content='what's up?', additional_kwargs={}, example=False)]} ``` ### What keys are saved to memory Often times chains take in or return multiple input/output keys. In these cases, how can we know which keys we want to save to the chat message history? This is generally controllable by `input_key` and `output_key` parameters on the memory types. These default to `None` - and if there is only one input/output key it is known to just use that. However, if there are multiple input/output keys then you MUST specify the name of which one to use. ### End to end example Finally, let's take a look at using this in a chain. We'll use an `LLMChain`, and show working with both an LLM and a ChatModel. #### Using an LLM ```python from langchain.llms import OpenAI from langchain.prompts import PromptTemplate from langchain.chains import LLMChain from langchain.memory import ConversationBufferMemory llm = OpenAI(temperature=0) # Notice that \"chat_history\" is present in the prompt template template = \"\"\"You are a nice chatbot having a conversation with a human. Previous conversation: {chat_history} New human question: {question} Response:\"\"\" prompt = PromptTemplate.from_template(template) # Notice that we need to align the `memory_key` memory = ConversationBufferMemory(memory_key=\"chat_history\") conversation = LLMChain( llm=llm, prompt=prompt, verbose=True, memory=memory ) ``` ```python # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation({\"question\": \"hi\"}) ``` #### Using a ChatModel ```python from langchain.chat_models import ChatOpenAI from langchain.prompts import ( ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate, ) from langchain.chains import LLMChain from langchain.memory import ConversationBufferMemory llm = ChatOpenAI() prompt = ChatPromptTemplate( messages=[ SystemMessagePromptTemplate.from_template( \"You are a nice chatbot having a conversation with a human.\" ), # The `variable_name` here is what must align with memory MessagesPlaceholder(variable_name=\"chat_history\"), HumanMessagePromptTemplate.from_template(\"{question}\") ] ) # Notice that we `return_messages=True` to fit into the MessagesPlaceholder # Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name. memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True) conversation = LLMChain( llm=llm, prompt=prompt, verbose=True, memory=memory ) ``` ```python # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation({\"question\": \"hi\"}) ``` ## Next steps And that's it for getting started! Please see the other sections for walkthroughs of more advanced topics, like custom memory, multiple memories, and more."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/memory/types/buffer.mdx"}, "data": "# Conversation Buffer This notebook shows how to use `ConversationBufferMemory`. This memory allows for storing messages and then extracts the messages in a variable. We can first extract it as a string. ```python from langchain.memory import ConversationBufferMemory ``` ```python memory = ConversationBufferMemory() memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"}) ``` ```python memory.load_memory_variables({}) ``` ``` {'history': 'Human: hi\\nAI: whats up'} ``` We can also get the history as a list of messages (this is useful if you are using this with a chat model). ```python memory = ConversationBufferMemory(return_messages=True) memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"}) ``` ```python memory.load_memory_variables({}) ``` ``` {'history': [HumanMessage(content='hi', additional_kwargs={}), AIMessage(content='whats up', additional_kwargs={})]} ``` ## Using in a chain Finally, let's take a look at using this in a chain (setting `verbose=True` so we can see the prompt). ```python from langchain.llms import OpenAI from langchain.chains import ConversationChain llm = OpenAI(temperature=0) conversation = ConversationChain( llm=llm, verbose=True, memory=ConversationBufferMemory() ) ``` ```python conversation.predict(input=\"Hi there!\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: > Finished chain. \" Hi there! It's nice to meet you. How can I help you today?\" ``` ```python conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: Hi there! It's nice to meet you. How can I help you today? Human: I'm doing well! Just having a conversation with an AI. AI: > Finished chain. \" That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\" ``` ```python conversation.predict(input=\"Tell me about yourself.\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: Hi there! It's nice to meet you. How can I help you today? Human: I'm doing well! Just having a conversation with an AI. AI: That's great! It's always nice to have a conversation with someone new. What would you like to talk about? Human: Tell me about yourself. AI: > Finished chain. \" Sure! I'm an AI created to help people with their everyday tasks. I'm programmed to understand natural language and provide helpful information. I'm also constantly learning and updating my knowledge base so I can provide more accurate and helpful answers.\" ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/memory/types/buffer_window.mdx"}, "data": "# Conversation Buffer Window `ConversationBufferWindowMemory` keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large. Let's first explore the basic functionality of this type of memory. ```python from langchain.memory import ConversationBufferWindowMemory ``` ```python memory = ConversationBufferWindowMemory( k=1) memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"}) memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"}) ``` ```python memory.load_memory_variables({}) ``` ``` {'history': 'Human: not much you\\nAI: not much'} ``` We can also get the history as a list of messages (this is useful if you are using this with a chat model). ```python memory = ConversationBufferWindowMemory( k=1, return_messages=True) memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"}) memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"}) ``` ```python memory.load_memory_variables({}) ``` ``` {'history': [HumanMessage(content='not much you', additional_kwargs={}), AIMessage(content='not much', additional_kwargs={})]} ``` ## Using in a chain Let's walk through an example, again setting `verbose=True` so we can see the prompt. ```python from langchain.llms import OpenAI from langchain.chains import ConversationChain conversation_with_summary = ConversationChain( llm=OpenAI(temperature=0), # We set a low k=2, to only keep the last 2 interactions in memory memory=ConversationBufferWindowMemory(k=2), verbose=True ) conversation_with_summary.predict(input=\"Hi, what's up?\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi, what's up? AI: > Finished chain. \" Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?\" ``` ```python conversation_with_summary.predict(input=\"What's their issues?\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi, what's up? AI: Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you? Human: What's their issues? AI: > Finished chain. \" The customer is having trouble connecting to their Wi-Fi network. I'm helping them troubleshoot the issue and get them connected.\" ``` ```python conversation_with_summary.predict(input=\"Is it going well?\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi, what's up? AI: Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you? Human: What's their issues? AI: The customer is having trouble connecting to their Wi-Fi network. I'm helping them troubleshoot the issue and get them connected. Human: Is it going well? AI: > Finished chain. \" Yes, it's going well so far. We've already identified the problem and are now working on a solution.\" ``` ```python # Notice here that the first interaction does not appear. conversation_with_summary.predict(input=\"What's the solution?\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: What's their issues? AI: The customer is having trouble connecting to their Wi-Fi network. I'm helping them troubleshoot the issue and get them connected. Human: Is it going well? AI: Yes, it's going well so far. We've already identified the problem and are now working on a solution. Human: What's the solution? AI: > Finished chain. \" The solution is to reset the router and reconfigure the settings. We're currently in the process of doing that.\" ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/memory/types/entity_summary_memory.mdx"}, "data": "# Entity Entity memory remembers given facts about specific entities in a conversation. It extracts information on entities (using an LLM) and builds up its knowledge about that entity over time (also using an LLM). Let's first walk through using this functionality. ```python from langchain.llms import OpenAI from langchain.memory import ConversationEntityMemory llm = OpenAI(temperature=0) ``` ```python memory = ConversationEntityMemory(llm=llm) _input = {\"input\": \"Deven & Sam are working on a hackathon project\"} memory.load_memory_variables(_input) memory.save_context( _input, {\"output\": \" That sounds like a great project! What kind of project are they working on?\"} ) ``` ```python memory.load_memory_variables({\"input\": 'who is Sam'}) ``` ``` {'history': 'Human: Deven & Sam are working on a hackathon project\\nAI: That sounds like a great project! What kind of project are they working on?', 'entities': {'Sam': 'Sam is working on a hackathon project with Deven.'}} ``` ```python memory = ConversationEntityMemory(llm=llm, return_messages=True) _input = {\"input\": \"Deven & Sam are working on a hackathon project\"} memory.load_memory_variables(_input) memory.save_context( _input, {\"output\": \" That sounds like a great project! What kind of project are they working on?\"} ) ``` ```python memory.load_memory_variables({\"input\": 'who is Sam'}) ``` ``` {'history': [HumanMessage(content='Deven & Sam are working on a hackathon project', additional_kwargs={}), AIMessage(content=' That sounds like a great project! What kind of project are they working on?', additional_kwargs={})], 'entities': {'Sam': 'Sam is working on a hackathon project with Deven.'}} ``` ## Using in a chain Let's now use it in a chain! ```python from langchain.chains import ConversationChain from langchain.memory import ConversationEntityMemory from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE from pydantic import BaseModel from typing import List, Dict, Any ``` ```python conversation = ConversationChain( llm=llm, verbose=True, prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE, memory=ConversationEntityMemory(llm=llm) ) ``` ```python conversation.predict(input=\"Deven & Sam are working on a hackathon project\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: You are an assistant to a human, powered by a large language model trained by OpenAI. You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand. You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics. Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist. Context: {'Deven': 'Deven is working on a hackathon project with Sam.', 'Sam': 'Sam is working on a hackathon project with Deven.'} Current conversation: Last line: Human: Deven & Sam are working on a hackathon project You: > Finished chain. ' That sounds like a great project! What kind of project are they working on?' ``` ```python conversation.memory.entity_store.store ``` ``` {'Deven': 'Deven is working on a hackathon project with Sam, which they are entering into a hackathon.', 'Sam': 'Sam is working on a hackathon project with Deven.'} ``` ```python conversation.predict(input=\"They are trying to add more complex memory structures to Langchain\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: You are an assistant to a human, powered by a large language model trained by OpenAI. You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand. You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics. Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist. Context: {'Deven': 'Deven is working on a hackathon project with Sam, which they are entering into a hackathon.', 'Sam': 'Sam is working on a hackathon project with Deven.', 'Langchain': ''} Current conversation: Human: Deven & Sam are working on a hackathon project AI: That sounds like a great project! What kind of project are they working on? Last line: Human: They are trying to add more complex memory structures to Langchain You: > Finished chain. ' That sounds like an interesting project! What kind of memory structures are they trying to add?' ``` ```python conversation.predict(input=\"They are adding in a key-value store for entities mentioned so far in the conversation.\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: You are an assistant to a human, powered by a large language model trained by OpenAI. You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand. You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics. Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist. Context: {'Deven': 'Deven is working on a hackathon project with Sam, which they are entering into a hackathon. They are trying to add more complex memory structures to Langchain.', 'Sam': 'Sam is working on a hackathon project with Deven, trying to add more complex memory structures to Langchain.', 'Langchain': 'Langchain is a project that is trying to add more complex memory structures.', 'Key-Value Store': ''} Current conversation: Human: Deven & Sam are working on a hackathon project AI: That sounds like a great project! What kind of project are they working on? Human: They are trying to add more complex memory structures to Langchain AI: That sounds like an interesting project! What kind of memory structures are they trying to add? Last line: Human: They are adding in a key-value store for entities mentioned so far in the conversation. You: > Finished chain. ' That sounds like a great idea! How will the key-value store help with the project?' ``` ```python conversation.predict(input=\"What do you know about Deven & Sam?\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: You are an assistant to a human, powered by a large language model trained by OpenAI. You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand. You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics. Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist. Context: {'Deven': 'Deven is working on a hackathon project with Sam, which they are entering into a hackathon. They are trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation.', 'Sam': 'Sam is working on a hackathon project with Deven, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation.'} Current conversation: Human: Deven & Sam are working on a hackathon project AI: That sounds like a great project! What kind of project are they working on? Human: They are trying to add more complex memory structures to Langchain AI: That sounds like an interesting project! What kind of memory structures are they trying to add? Human: They are adding in a key-value store for entities mentioned so far in the conversation. AI: That sounds like a great idea! How will the key-value store help with the project? Last line: Human: What do you know about Deven & Sam? You: > Finished chain. ' Deven and Sam are working on a hackathon project together, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation. They seem to be working hard on this project and have a great idea for how the key-value store can help.' ``` ## Inspecting the memory store We can also inspect the memory store directly. In the following examples, we look at it directly, and then go through some examples of adding information and watch how it changes. ```python from pprint import pprint pprint(conversation.memory.entity_store.store) ``` ``` {'Daimon': 'Daimon is a company founded by Sam, a successful entrepreneur.', 'Deven': 'Deven is working on a hackathon project with Sam, which they are ' 'entering into a hackathon. They are trying to add more complex ' 'memory structures to Langchain, including a key-value store for ' 'entities mentioned so far in the conversation, and seem to be ' 'working hard on this project with a great idea for how the ' 'key-value store can help.', 'Key-Value Store': 'A key-value store is being added to the project to store ' 'entities mentioned in the conversation.', 'Langchain': 'Langchain is a project that is trying to add more complex ' 'memory structures, including a key-value store for entities ' 'mentioned so far in the conversation.', 'Sam': 'Sam is working on a hackathon project with Deven, trying to add more ' 'complex memory structures to Langchain, including a key-value store ' 'for entities mentioned so far in the conversation. They seem to have ' 'a great idea for how the key-value store can help, and Sam is also ' 'the founder of a company called Daimon.'} ``` ```python conversation.predict(input=\"Sam is the founder of a company called Daimon.\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: You are an assistant to a human, powered by a large language model trained by OpenAI. You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand. You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics. Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist. Context: {'Daimon': 'Daimon is a company founded by Sam, a successful entrepreneur.', 'Sam': 'Sam is working on a hackathon project with Deven, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation. They seem to have a great idea for how the key-value store can help, and Sam is also the founder of a company called Daimon.'} Current conversation: Human: They are adding in a key-value store for entities mentioned so far in the conversation. AI: That sounds like a great idea! How will the key-value store help with the project? Human: What do you know about Deven & Sam? AI: Deven and Sam are working on a hackathon project together, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation. They seem to be working hard on this project and have a great idea for how the key-value store can help. Human: Sam is the founder of a company called Daimon. AI: That's impressive! It sounds like Sam is a very successful entrepreneur. What kind of company is Daimon? Last line: Human: Sam is the founder of a company called Daimon. You: > Finished chain. \" That's impressive! It sounds like Sam is a very successful entrepreneur. What kind of company is Daimon?\" ``` ```python from pprint import pprint pprint(conversation.memory.entity_store.store) ``` ``` {'Daimon': 'Daimon is a company founded by Sam, a successful entrepreneur, who ' 'is working on a hackathon project with Deven to add more complex ' 'memory structures to Langchain.', 'Deven': 'Deven is working on a hackathon project with Sam, which they are ' 'entering into a hackathon. They are trying to add more complex ' 'memory structures to Langchain, including a key-value store for ' 'entities mentioned so far in the conversation, and seem to be ' 'working hard on this project with a great idea for how the ' 'key-value store can help.', 'Key-Value Store': 'A key-value store is being added to the project to store ' 'entities mentioned in the conversation.', 'Langchain': 'Langchain is a project that is trying to add more complex ' 'memory structures, including a key-value store for entities ' 'mentioned so far in the conversation.', 'Sam': 'Sam is working on a hackathon project with Deven, trying to add more ' 'complex memory structures to Langchain, including a key-value store ' 'for entities mentioned so far in the conversation. They seem to have ' 'a great idea for how the key-value store can help, and Sam is also ' 'the founder of a successful company called Daimon.'} ``` ```python conversation.predict(input=\"What do you know about Sam?\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: You are an assistant to a human, powered by a large language model trained by OpenAI. You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand. You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics. Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist. Context: {'Deven': 'Deven is working on a hackathon project with Sam, which they are entering into a hackathon. They are trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation, and seem to be working hard on this project with a great idea for how the key-value store can help.', 'Sam': 'Sam is working on a hackathon project with Deven, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation. They seem to have a great idea for how the key-value store can help, and Sam is also the founder of a successful company called Daimon.', 'Langchain': 'Langchain is a project that is trying to add more complex memory structures, including a key-value store for entities mentioned so far in the conversation.', 'Daimon': 'Daimon is a company founded by Sam, a successful entrepreneur, who is working on a hackathon project with Deven to add more complex memory structures to Langchain.'} Current conversation: Human: What do you know about Deven & Sam? AI: Deven and Sam are working on a hackathon project together, trying to add more complex memory structures to Langchain, including a key-value store for entities mentioned so far in the conversation. They seem to be working hard on this project and have a great idea for how the key-value store can help. Human: Sam is the founder of a company called Daimon. AI: That's impressive! It sounds like Sam is a very successful entrepreneur. What kind of company is Daimon? Human: Sam is the founder of a company called Daimon. AI: That's impressive! It sounds like Sam is a very successful entrepreneur. What kind of company is Daimon? Last line: Human: What do you know about Sam? You: > Finished chain. ' Sam is the founder of a successful company called Daimon. He is also working on a hackathon project with Deven to add more complex memory structures to Langchain. They seem to have a great idea for how the key-value store can help.' ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/memory/types/index.mdx"}, "data": "--- sidebar_position: 2 --- # Memory types There are many different types of memory. Each has their own parameters, their own return types, and is useful in different scenarios. Please see their individual page for more detail on each one."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/memory/types/summary.mdx"}, "data": "# Conversation Summary Now let's take a look at using a slightly more complex type of memory - `ConversationSummaryMemory`. This type of memory creates a summary of the conversation over time. This can be useful for condensing information from the conversation over time. Conversation summary memory summarizes the conversation as it happens and stores the current summary in memory. This memory can then be used to inject the summary of the conversation so far into a prompt/chain. This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens. Let's first explore the basic functionality of this type of memory. ```python from langchain.memory import ConversationSummaryMemory, ChatMessageHistory from langchain.llms import OpenAI ``` ```python memory = ConversationSummaryMemory(llm=OpenAI(temperature=0)) memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"}) ``` ```python memory.load_memory_variables({}) ``` ``` {'history': '\\nThe human greets the AI, to which the AI responds.'} ``` We can also get the history as a list of messages (this is useful if you are using this with a chat model). ```python memory = ConversationSummaryMemory(llm=OpenAI(temperature=0), return_messages=True) memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"}) ``` ```python memory.load_memory_variables({}) ``` ``` {'history': [SystemMessage(content='\\nThe human greets the AI, to which the AI responds.', additional_kwargs={})]} ``` We can also utilize the `predict_new_summary` method directly. ```python messages = memory.chat_memory.messages previous_summary = \"\" memory.predict_new_summary(messages, previous_summary) ``` ``` '\\nThe human greets the AI, to which the AI responds.' ``` ## Initializing with messages/existing summary If you have messages outside this class, you can easily initialize the class with `ChatMessageHistory`. During loading, a summary will be calculated. ```python history = ChatMessageHistory() history.add_user_message(\"hi\") history.add_ai_message(\"hi there!\") ``` ```python memory = ConversationSummaryMemory.from_messages( llm=OpenAI(temperature=0), chat_memory=history, return_messages=True ) ``` ```python memory.buffer ``` ``` '\\nThe human greets the AI, to which the AI responds with a friendly greeting.' ``` Optionally you can speed up initialization using a previously generated summary, and avoid regenerating the summary by just initializing directly. ```python memory = ConversationSummaryMemory( llm=OpenAI(temperature=0), buffer=\"The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\", chat_memory=history, return_messages=True ) ``` ## Using in a chain Let's walk through an example of using this in a chain, again setting `verbose=True` so we can see the prompt. ```python from langchain.llms import OpenAI from langchain.chains import ConversationChain llm = OpenAI(temperature=0) conversation_with_summary = ConversationChain( llm=llm, memory=ConversationSummaryMemory(llm=OpenAI()), verbose=True ) conversation_with_summary.predict(input=\"Hi, what's up?\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi, what's up? AI: > Finished chain. \" Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?\" ``` ```python conversation_with_summary.predict(input=\"Tell me more about it!\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: The human greeted the AI and asked how it was doing. The AI replied that it was doing great and was currently helping a customer with a technical issue. Human: Tell me more about it! AI: > Finished chain. \" Sure! The customer is having trouble with their computer not connecting to the internet. I'm helping them troubleshoot the issue and figure out what the problem is. So far, we've tried resetting the router and checking the network settings, but the issue still persists. We're currently looking into other possible solutions.\" ``` ```python conversation_with_summary.predict(input=\"Very cool -- what is the scope of the project?\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: The human greeted the AI and asked how it was doing. The AI replied that it was doing great and was currently helping a customer with a technical issue where their computer was not connecting to the internet. The AI was troubleshooting the issue and had already tried resetting the router and checking the network settings, but the issue still persisted and they were looking into other possible solutions. Human: Very cool -- what is the scope of the project? AI: > Finished chain. \" The scope of the project is to troubleshoot the customer's computer issue and find a solution that will allow them to connect to the internet. We are currently exploring different possibilities and have already tried resetting the router and checking the network settings, but the issue still persists.\" ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/memory/types/vectorstore_retriever_memory.mdx"}, "data": "# Backed by a Vector Store `VectorStoreRetrieverMemory` stores memories in a vector store and queries the top-K most \"salient\" docs every time it is called. This differs from most of the other Memory classes in that it doesn't explicitly track the order of interactions. In this case, the \"docs\" are previous conversation snippets. This can be useful to refer to relevant pieces of information that the AI was told earlier in the conversation. ```python from datetime import datetime from langchain.embeddings.openai import OpenAIEmbeddings from langchain.llms import OpenAI from langchain.memory import VectorStoreRetrieverMemory from langchain.chains import ConversationChain from langchain.prompts import PromptTemplate ``` ### Initialize your vector store Depending on the store you choose, this step may look different. Consult the relevant vector store documentation for more details. ```python import faiss from langchain.docstore import InMemoryDocstore from langchain.vectorstores import FAISS embedding_size = 1536 # Dimensions of the OpenAIEmbeddings index = faiss.IndexFlatL2(embedding_size) embedding_fn = OpenAIEmbeddings().embed_query vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {}) ``` ### Create your `VectorStoreRetrieverMemory` The memory object is instantiated from any vector store retriever. ```python # In actual usage, you would set `k` to be a higher value, but we use k=1 to show that # the vector lookup still returns the semantically relevant information retriever = vectorstore.as_retriever(search_kwargs=dict(k=1)) memory = VectorStoreRetrieverMemory(retriever=retriever) # When added to an agent, the memory object can save pertinent information from conversations or used tools memory.save_context({\"input\": \"My favorite food is pizza\"}, {\"output\": \"that's good to know\"}) memory.save_context({\"input\": \"My favorite sport is soccer\"}, {\"output\": \"...\"}) memory.save_context({\"input\": \"I don't the Celtics\"}, {\"output\": \"ok\"}) # ``` ```python # Notice the first result returned is the memory pertaining to tax help, which the language model deems more semantically relevant # to a 1099 than the other documents, despite them both containing numbers. print(memory.load_memory_variables({\"prompt\": \"what sport should i watch?\"})[\"history\"]) ``` ``` input: My favorite sport is soccer output: ... ``` ## Using in a chain Let's walk through an example, again setting `verbose=True` so we can see the prompt. ```python llm = OpenAI(temperature=0) # Can be any valid LLM _DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: {history} (You do not need to use these pieces of information if not relevant) Current conversation: Human: {input} AI:\"\"\" PROMPT = PromptTemplate( input_variables=[\"history\", \"input\"], template=_DEFAULT_TEMPLATE ) conversation_with_summary = ConversationChain( llm=llm, prompt=PROMPT, # We set a very low max_token_limit for the purposes of testing. memory=memory, verbose=True ) conversation_with_summary.predict(input=\"Hi, my name is Perry, what's up?\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: My favorite food is pizza output: that's good to know (You do not need to use these pieces of information if not relevant) Current conversation: Human: Hi, my name is Perry, what's up? AI: > Finished chain. \" Hi Perry, I'm doing well. How about you?\" ``` ```python # Here, the basketball related content is surfaced conversation_with_summary.predict(input=\"what's my favorite sport?\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: My favorite sport is soccer output: ... (You do not need to use these pieces of information if not relevant) Current conversation: Human: what's my favorite sport? AI: > Finished chain. ' You told me earlier that your favorite sport is soccer.' ``` ```python # Even though the language model is stateless, since relevant memory is fetched, it can \"reason\" about the time. # Timestamping memories and data is useful in general to let the agent determine temporal relevance conversation_with_summary.predict(input=\"Whats my favorite food\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: My favorite food is pizza output: that's good to know (You do not need to use these pieces of information if not relevant) Current conversation: Human: Whats my favorite food AI: > Finished chain. ' You said your favorite food is pizza.' ``` ```python # The memories from the conversation are automatically stored, # since this query best matches the introduction chat above, # the agent is able to 'remember' the user's name. conversation_with_summary.predict(input=\"What's my name?\") ``` ``` > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: Hi, my name is Perry, what's up? response: Hi Perry, I'm doing well. How about you? (You do not need to use these pieces of information if not relevant) Current conversation: Human: What's my name? AI: > Finished chain. ' Your name is Perry.' ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/chat/chat_model_caching.mdx"}, "data": "# Caching LangChain provides an optional caching layer for chat models. This is useful for two reasons: It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times. It can speed up your application by reducing the number of API calls you make to the LLM provider. ```python from langchain.globals import set_llm_cache from langchain.chat_models import ChatOpenAI llm = ChatOpenAI() ``` ## In Memory Cache ```python from langchain.cache import InMemoryCache set_llm_cache(InMemoryCache()) # The first time, it is not yet in cache, so it should take longer llm.predict(\"Tell me a joke\") ``` ``` CPU times: user 35.9 ms, sys: 28.6 ms, total: 64.6 ms Wall time: 4.83 s \"\\n\\nWhy couldn't the bicycle stand up by itself? It was...two tired!\" ``` ```python # The second time it is, so it goes faster llm.predict(\"Tell me a joke\") ``` ``` CPU times: user 238 \u00b5s, sys: 143 \u00b5s, total: 381 \u00b5s Wall time: 1.76 ms '\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.' ``` ## SQLite Cache ```bash rm .langchain.db ``` ```python # We can do the same thing with a SQLite cache from langchain.cache import SQLiteCache set_llm_cache(SQLiteCache(database_path=\".langchain.db\")) ``` ```python # The first time, it is not yet in cache, so it should take longer llm.predict(\"Tell me a joke\") ``` ``` CPU times: user 17 ms, sys: 9.76 ms, total: 26.7 ms Wall time: 825 ms '\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.' ``` ```python # The second time it is, so it goes faster llm.predict(\"Tell me a joke\") ``` ``` CPU times: user 2.46 ms, sys: 1.23 ms, total: 3.7 ms Wall time: 2.67 ms '\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.' ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/chat/prompts.mdx"}, "data": "# Prompts Prompts for chat models are built around messages, instead of just plain text. You can make use of templating by using a `MessagePromptTemplate`. You can build a `ChatPromptTemplate` from one or more `MessagePromptTemplates`. You can use `ChatPromptTemplate`'s `format_prompt` -- this returns a `PromptValue`, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model. For convenience, there is a `from_template` method defined on the template. If you were to use this template, this is what it would look like: ```python from langchain.prompts import PromptTemplate from langchain.prompts.chat import ( ChatPromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate, HumanMessagePromptTemplate, ) template=\"You are a helpful assistant that translates {input_language} to {output_language}.\" system_message_prompt = SystemMessagePromptTemplate.from_template(template) human_template=\"{text}\" human_message_prompt = HumanMessagePromptTemplate.from_template(human_template) ``` ```python chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt]) # get a chat completion from the formatted messages chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages()) ``` ``` AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}) ``` If you wanted to construct the MessagePromptTemplate more directly, you could create a PromptTemplate outside and then pass it in, e.g.: ```python prompt=PromptTemplate( template=\"You are a helpful assistant that translates {input_language} to {output_language}.\", input_variables=[\"input_language\", \"output_language\"], ) system_message_prompt = SystemMessagePromptTemplate(prompt=prompt) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/index.mdx"}, "data": "--- sidebar_position: 0 sidebar_custom_props: description: Interface with language models sidebar_class_name: hidden --- # Model I/O The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model. - [Prompts](/docs/modules/model_io/prompts/): Templatize, dynamically select, and manage model inputs - [Chat models](/docs/modules/model_io/chat/): Models that are backed by a language model but take a list of Chat Messages as input and return a Chat Message - [LLMs](/docs/modules/model_io/llms/): Models that take a text string as input and return a text string - [Output parsers](/docs/modules/model_io/output_parsers/): Extract information from model outputs ![model_io_diagram](/img/model_io.jpg) ## LLMs vs Chat models LLMs and chat models are subtly but importantly different. LLMs in LangChain refer to pure text completion models. The APIs they wrap take a string prompt as input and output a string completion. OpenAI's GPT-3 is implemented as an LLM. Chat models are often backed by LLMs but tuned specifically for having conversations. And, crucially, their provider APIs use a different interface than pure text completion models. Instead of a single string, they take a list of chat messages as input. Usually these messages are labeled with the speaker (usually one of \"System\", \"AI\", and \"Human\"). And they return an AI chat message as output. GPT-4 and Anthropic's Claude-2 are both implemented as chat models."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/llms/llm_caching.mdx"}, "data": "# Caching LangChain provides an optional caching layer for LLMs. This is useful for two reasons: It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times. It can speed up your application by reducing the number of API calls you make to the LLM provider. ```python from langchain.globals import set_llm_cache from langchain.llms import OpenAI # To make the caching really obvious, lets use a slower model. llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2) ``` ## In Memory Cache ```python from langchain.cache import InMemoryCache set_llm_cache(InMemoryCache()) # The first time, it is not yet in cache, so it should take longer llm.predict(\"Tell me a joke\") ``` ``` CPU times: user 35.9 ms, sys: 28.6 ms, total: 64.6 ms Wall time: 4.83 s \"\\n\\nWhy couldn't the bicycle stand up by itself? It was...two tired!\" ``` ```python # The second time it is, so it goes faster llm.predict(\"Tell me a joke\") ``` ``` CPU times: user 238 \u00b5s, sys: 143 \u00b5s, total: 381 \u00b5s Wall time: 1.76 ms '\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.' ``` ## SQLite Cache ```bash rm .langchain.db ``` ```python # We can do the same thing with a SQLite cache from langchain.cache import SQLiteCache set_llm_cache(SQLiteCache(database_path=\".langchain.db\")) ``` ```python # The first time, it is not yet in cache, so it should take longer llm.predict(\"Tell me a joke\") ``` ``` CPU times: user 17 ms, sys: 9.76 ms, total: 26.7 ms Wall time: 825 ms '\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.' ``` ```python # The second time it is, so it goes faster llm.predict(\"Tell me a joke\") ``` ``` CPU times: user 2.46 ms, sys: 1.23 ms, total: 3.7 ms Wall time: 2.67 ms '\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.' ``` ## Optional caching in chains You can also turn off caching for particular nodes in chains. Note that because of certain interfaces, it's often easier to construct the chain first, and then edit the LLM afterwards. As an example, we will load a summarizer map-reduce chain. We will cache results for the map-step, but then not freeze it for the combine step. ```python llm = OpenAI(model_name=\"text-davinci-002\") no_cache_llm = OpenAI(model_name=\"text-davinci-002\", cache=False) ``` ```python from langchain.text_splitter import CharacterTextSplitter from langchain.chains.mapreduce import MapReduceChain text_splitter = CharacterTextSplitter() ``` ```python with open('../../../state_of_the_union.txt') as f: state_of_the_union = f.read() texts = text_splitter.split_text(state_of_the_union) ``` ```python from langchain.docstore.document import Document docs = [Document(page_content=t) for t in texts[:3]] from langchain.chains.summarize import load_summarize_chain ``` ```python chain = load_summarize_chain(llm, chain_type=\"map_reduce\", reduce_llm=no_cache_llm) ``` ```python chain.run(docs) ``` ``` CPU times: user 452 ms, sys: 60.3 ms, total: 512 ms Wall time: 5.09 s '\\n\\nPresident Biden is discussing the American Rescue Plan and the Bipartisan Infrastructure Law, which will create jobs and help Americans. He also talks about his vision for America, which includes investing in education and infrastructure. In response to Russian aggression in Ukraine, the United States is joining with European allies to impose sanctions and isolate Russia. American forces are being mobilized to protect NATO countries in the event that Putin decides to keep moving west. The Ukrainians are bravely fighting back, but the next few weeks will be hard for them. Putin will pay a high price for his actions in the long run. Americans should not be alarmed, as the United States is taking action to protect its interests and allies.' ``` When we run it again, we see that it runs substantially faster but the final answer is different. This is due to caching at the map steps, but not at the reduce step. ```python chain.run(docs) ``` ``` CPU times: user 11.5 ms, sys: 4.33 ms, total: 15.8 ms Wall time: 1.04 s '\\n\\nPresident Biden is discussing the American Rescue Plan and the Bipartisan Infrastructure Law, which will create jobs and help Americans. He also talks about his vision for America, which includes investing in education and infrastructure.' ``` ```bash rm .langchain.db sqlite.db ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/output_parsers/comma_separated.mdx"}, "data": "# List parser This output parser can be used when you want to return a list of comma-separated items. ```python from langchain.output_parsers import CommaSeparatedListOutputParser from langchain.prompts import PromptTemplate from langchain.llms import OpenAI output_parser = CommaSeparatedListOutputParser() format_instructions = output_parser.get_format_instructions() prompt = PromptTemplate( template=\"List five {subject}.\\n{format_instructions}\", input_variables=[\"subject\"], partial_variables={\"format_instructions\": format_instructions} ) model = OpenAI(temperature=0) _input = prompt.format(subject=\"ice cream flavors\") output = model(_input) output_parser.parse(output) ``` The resulting output will be: ``` ['Vanilla', 'Chocolate', 'Strawberry', 'Mint Chocolate Chip', 'Cookies and Cream'] ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/output_parsers/output_fixing_parser.mdx"}, "data": "# Auto-fixing parser This output parser wraps another output parser, and in the event that the first one fails it calls out to another LLM to fix any errors. But we can do other things besides throw errors. Specifically, we can pass the misformatted output, along with the formatted instructions, to the model and ask it to fix it. For this example, we'll use the above Pydantic output parser. Here's what happens if we pass it a result that does not comply with the schema: ```python from langchain.chat_models import ChatOpenAI from langchain.output_parsers import PydanticOutputParser from langchain.pydantic_v1 import BaseModel, Field from typing import List ``` ```python class Actor(BaseModel): name: str = Field(description=\"name of an actor\") film_names: List[str] = Field(description=\"list of names of films they starred in\") actor_query = \"Generate the filmography for a random actor.\" parser = PydanticOutputParser(pydantic_object=Actor) ``` ```python misformatted = \"{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}\" ``` ```python parser.parse(misformatted) ``` ``` --------------------------------------------------------------------------- JSONDecodeError Traceback (most recent call last) File ~/workplace/langchain/langchain/output_parsers/pydantic.py:23, in PydanticOutputParser.parse(self, text) 22 json_str = match.group() ---> 23 json_object = json.loads(json_str) 24 return self.pydantic_object.parse_obj(json_object) File ~/.pyenv/versions/3.9.1/lib/python3.9/json/__init__.py:346, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw) 343 if (cls is None and object_hook is None and 344 parse_int is None and parse_float is None and 345 parse_constant is None and object_pairs_hook is None and not kw): --> 346 return _default_decoder.decode(s) 347 if cls is None: File ~/.pyenv/versions/3.9.1/lib/python3.9/json/decoder.py:337, in JSONDecoder.decode(self, s, _w) 333 \"\"\"Return the Python representation of ``s`` (a ``str`` instance 334 containing a JSON document). 335 336 \"\"\" --> 337 obj, end = self.raw_decode(s, idx=_w(s, 0).end()) 338 end = _w(s, end).end() File ~/.pyenv/versions/3.9.1/lib/python3.9/json/decoder.py:353, in JSONDecoder.raw_decode(self, s, idx) 352 try: --> 353 obj, end = self.scan_once(s, idx) 354 except StopIteration as err: JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1) During handling of the above exception, another exception occurred: OutputParserException Traceback (most recent call last) Cell In[6], line 1 ----> 1 parser.parse(misformatted) File ~/workplace/langchain/langchain/output_parsers/pydantic.py:29, in PydanticOutputParser.parse(self, text) 27 name = self.pydantic_object.__name__ 28 msg = f\"Failed to parse {name} from completion {text}. Got: {e}\" ---> 29 raise OutputParserException(msg) OutputParserException: Failed to parse Actor from completion {'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}. Got: Expecting property name enclosed in double quotes: line 1 column 2 (char 1) ``` Now we can construct and use a `OutputFixingParser`. This output parser takes as an argument another output parser but also an LLM with which to try to correct any formatting mistakes. ```python from langchain.output_parsers import OutputFixingParser new_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI()) ``` ```python new_parser.parse(misformatted) ``` ``` Actor(name='Tom Hanks', film_names=['Forrest Gump']) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/output_parsers/structured.mdx"}, "data": "# Structured output parser This output parser can be used when you want to return multiple fields. While the Pydantic/JSON parser is more powerful, we initially experimented with data structures having text fields only. ```python from langchain.output_parsers import StructuredOutputParser, ResponseSchema from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate from langchain.llms import OpenAI from langchain.chat_models import ChatOpenAI ``` Here we define the response schema we want to receive. ```python response_schemas = [ ResponseSchema(name=\"answer\", description=\"answer to the user's question\"), ResponseSchema(name=\"source\", description=\"source used to answer the user's question, should be a website.\") ] output_parser = StructuredOutputParser.from_response_schemas(response_schemas) ``` We now get a string that contains instructions for how the response should be formatted, and we then insert that into our prompt. ```python format_instructions = output_parser.get_format_instructions() prompt = PromptTemplate( template=\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\", input_variables=[\"question\"], partial_variables={\"format_instructions\": format_instructions} ) ``` We can now use this to format a prompt to send to the language model, and then parse the returned result. ```python model = OpenAI(temperature=0) ``` ```python _input = prompt.format_prompt(question=\"what's the capital of france?\") output = model(_input.to_string()) ``` ```python output_parser.parse(output) ``` ``` {'answer': 'Paris', 'source': ' ``` And here's an example of using this in a chat model ```python chat_model = ChatOpenAI(temperature=0) ``` ```python prompt = ChatPromptTemplate( messages=[ HumanMessagePromptTemplate.from_template(\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\") ], input_variables=[\"question\"], partial_variables={\"format_instructions\": format_instructions} ) ``` ```python _input = prompt.format_prompt(question=\"what's the capital of france?\") output = chat_model(_input.to_messages()) ``` ```python output_parser.parse(output.content) ``` ``` {'answer': 'Paris', 'source': ' ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/prompts/example_selectors/custom_example_selector.md"}, "data": "# Custom example selector In this tutorial, we'll create a custom example selector that selects examples randomly from a given list of examples. An `ExampleSelector` must implement two methods: 1. An `add_example` method which takes in an example and adds it into the ExampleSelector 2. A `select_examples` method which takes in input variables (which are meant to be user input) and returns a list of examples to use in the few-shot prompt. Let's implement a custom `ExampleSelector` that just selects two examples at random. **Note:** Take a look at the current set of example selector implementations supported in LangChain [here](/docs/modules/model_io/prompts/example_selectors/). ## Implement custom example selector ```python from langchain.prompts.example_selector.base import BaseExampleSelector from typing import Dict, List import numpy as np class CustomExampleSelector(BaseExampleSelector): def __init__(self, examples: List[Dict[str, str]]): self.examples = examples def add_example(self, example: Dict[str, str]) -> None: \"\"\"Add new example to store for a key.\"\"\" self.examples.append(example) def select_examples(self, input_variables: Dict[str, str]) -> List[dict]: \"\"\"Select which examples to use based on the inputs.\"\"\" return np.random.choice(self.examples, size=2, replace=False) ``` ## Use custom example selector ```python examples = [ {\"foo\": \"1\"}, {\"foo\": \"2\"}, {\"foo\": \"3\"} ] # Initialize example selector. example_selector = CustomExampleSelector(examples) # Select examples example_selector.select_examples({\"foo\": \"foo\"}) # -> array([{'foo': '2'}, {'foo': '3'}], dtype=object) # Add new example to the set of examples example_selector.add_example({\"foo\": \"4\"}) example_selector.examples # -> [{'foo': '1'}, {'foo': '2'}, {'foo': '3'}, {'foo': '4'}] # Select examples example_selector.select_examples({\"foo\": \"foo\"}) # -> array([{'foo': '1'}, {'foo': '4'}], dtype=object) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/prompts/example_selectors/index.mdx"}, "data": "# Example selectors If you have a large number of examples, you may need to select which ones to include in the prompt. The Example Selector is the class responsible for doing so. The base interface is defined as below: ```python class BaseExampleSelector(ABC): \"\"\"Interface for selecting examples to include in prompts.\"\"\" @abstractmethod def select_examples(self, input_variables: Dict[str, str]) -> List[dict]: \"\"\"Select which examples to use based on the inputs.\"\"\" ``` The only method it needs to define is a ``select_examples`` method. This takes in the input variables and then returns a list of examples. It is up to each specific implementation as to how those examples are selected."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/prompts/example_selectors/length_based.mdx"}, "data": "# Select by length This example selector selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more. ```python from langchain.prompts import PromptTemplate from langchain.prompts import FewShotPromptTemplate from langchain.prompts.example_selector import LengthBasedExampleSelector # Examples of a pretend task of creating antonyms. examples = [ {\"input\": \"happy\", \"output\": \"sad\"}, {\"input\": \"tall\", \"output\": \"short\"}, {\"input\": \"energetic\", \"output\": \"lethargic\"}, {\"input\": \"sunny\", \"output\": \"gloomy\"}, {\"input\": \"windy\", \"output\": \"calm\"}, ] example_prompt = PromptTemplate( input_variables=[\"input\", \"output\"], template=\"Input: {input}\\nOutput: {output}\", ) example_selector = LengthBasedExampleSelector( # The examples it has available to choose from. examples=examples, # The PromptTemplate being used to format the examples. example_prompt=example_prompt, # The maximum length that the formatted examples should be. # Length is measured by the get_text_length function below. max_length=25, # The function used to get the length of a string, which is used # to determine which examples to include. It is commented out because # it is provided as a default value if none is specified. # get_text_length: Callable[[str], int] = lambda x: len(re.split(\"\\n| \", x)) ) dynamic_prompt = FewShotPromptTemplate( # We provide an ExampleSelector instead of examples. example_selector=example_selector, example_prompt=example_prompt, prefix=\"Give the antonym of every input\", suffix=\"Input: {adjective}\\nOutput:\", input_variables=[\"adjective\"], ) ``` ```python # An example with small input, so it selects all examples. print(dynamic_prompt.format(adjective=\"big\")) ``` ``` Give the antonym of every input Input: happy Output: sad Input: tall Output: short Input: energetic Output: lethargic Input: sunny Output: gloomy Input: windy Output: calm Input: big Output: ``` ```python # An example with long input, so it selects only one example. long_string = \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\" print(dynamic_prompt.format(adjective=long_string)) ``` ``` Give the antonym of every input Input: happy Output: sad Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else Output: ``` ```python # You can add an example to an example selector as well. new_example = {\"input\": \"big\", \"output\": \"small\"} dynamic_prompt.example_selector.add_example(new_example) print(dynamic_prompt.format(adjective=\"enthusiastic\")) ``` ``` Give the antonym of every input Input: happy Output: sad Input: tall Output: short Input: energetic Output: lethargic Input: sunny Output: gloomy Input: windy Output: calm Input: big Output: small Input: enthusiastic Output: ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/prompts/example_selectors/similarity.mdx"}, "data": "# Select by similarity This object selects examples based on similarity to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs. ```python from langchain.prompts.example_selector import SemanticSimilarityExampleSelector from langchain.vectorstores import Chroma from langchain.embeddings import OpenAIEmbeddings from langchain.prompts import FewShotPromptTemplate, PromptTemplate example_prompt = PromptTemplate( input_variables=[\"input\", \"output\"], template=\"Input: {input}\\nOutput: {output}\", ) # Examples of a pretend task of creating antonyms. examples = [ {\"input\": \"happy\", \"output\": \"sad\"}, {\"input\": \"tall\", \"output\": \"short\"}, {\"input\": \"energetic\", \"output\": \"lethargic\"}, {\"input\": \"sunny\", \"output\": \"gloomy\"}, {\"input\": \"windy\", \"output\": \"calm\"}, ] ``` ```python example_selector = SemanticSimilarityExampleSelector.from_examples( # The list of examples available to select from. examples, # The embedding class used to produce embeddings which are used to measure semantic similarity. OpenAIEmbeddings(), # The VectorStore class that is used to store the embeddings and do a similarity search over. Chroma, # The number of examples to produce. k=1 ) similar_prompt = FewShotPromptTemplate( # We provide an ExampleSelector instead of examples. example_selector=example_selector, example_prompt=example_prompt, prefix=\"Give the antonym of every input\", suffix=\"Input: {adjective}\\nOutput:\", input_variables=[\"adjective\"], ) ``` ``` Running Chroma using direct local API. Using DuckDB in-memory for database. Data will be transient. ``` ```python # Input is a feeling, so should select the happy/sad example print(similar_prompt.format(adjective=\"worried\")) ``` ``` Give the antonym of every input Input: happy Output: sad Input: worried Output: ``` ```python # Input is a measurement, so should select the tall/short example print(similar_prompt.format(adjective=\"large\")) ``` ``` Give the antonym of every input Input: tall Output: short Input: large Output: ``` ```python # You can add new examples to the SemanticSimilarityExampleSelector as well similar_prompt.example_selector.add_example({\"input\": \"enthusiastic\", \"output\": \"apathetic\"}) print(similar_prompt.format(adjective=\"passionate\")) ``` ``` Give the antonym of every input Input: enthusiastic Output: apathetic Input: passionate Output: ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/prompts/index.mdx"}, "data": "--- sidebar_position: 0 --- # Prompts A prompt for a language model is a set of instructions or input provided by a user to guide the model's response, helping it understand the context and generate relevant and coherent language-based output, such as answering questions, completing sentences, or engaging in a conversation. LangChain provides several classes and functions to help construct and work with prompts. - [Prompt templates](/docs/modules/model_io/prompts/prompt_templates/): Parametrized model inputs - [Example selectors](/docs/modules/model_io/prompts/example_selectors/): Dynamically select examples to include in prompts"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/prompts/prompt_templates/few_shot_examples.mdx"}, "data": "# Few-shot prompt templates In this tutorial, we'll learn how to create a prompt template that uses few-shot examples. A few-shot prompt template can be constructed from either a set of examples, or from an Example Selector object. ### Use Case In this tutorial, we'll configure few-shot examples for self-ask with search. ## Using an example set ### Create the example set To get started, create a list of few-shot examples. Each example should be a dictionary with the keys being the input variables and the values being the values for those input variables. ```python from langchain.prompts.few_shot import FewShotPromptTemplate from langchain.prompts.prompt import PromptTemplate examples = [ { \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\", \"answer\": \"\"\" Are follow up questions needed here: Yes. Follow up: How old was Muhammad Ali when he died? Intermediate answer: Muhammad Ali was 74 years old when he died. Follow up: How old was Alan Turing when he died? Intermediate answer: Alan Turing was 41 years old when he died. So the final answer is: Muhammad Ali \"\"\" }, { \"question\": \"When was the founder of craigslist born?\", \"answer\": \"\"\" Are follow up questions needed here: Yes. Follow up: Who was the founder of craigslist? Intermediate answer: Craigslist was founded by Craig Newmark. Follow up: When was Craig Newmark born? Intermediate answer: Craig Newmark was born on December 6, 1952. So the final answer is: December 6, 1952 \"\"\" }, { \"question\": \"Who was the maternal grandfather of George Washington?\", \"answer\": \"\"\" Are follow up questions needed here: Yes. Follow up: Who was the mother of George Washington? Intermediate answer: The mother of George Washington was Mary Ball Washington. Follow up: Who was the father of Mary Ball Washington? Intermediate answer: The father of Mary Ball Washington was Joseph Ball. So the final answer is: Joseph Ball \"\"\" }, { \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\", \"answer\": \"\"\" Are follow up questions needed here: Yes. Follow up: Who is the director of Jaws? Intermediate Answer: The director of Jaws is Steven Spielberg. Follow up: Where is Steven Spielberg from? Intermediate Answer: The United States. Follow up: Who is the director of Casino Royale? Intermediate Answer: The director of Casino Royale is Martin Campbell. Follow up: Where is Martin Campbell from? Intermediate Answer: New Zealand. So the final answer is: No \"\"\" } ] ``` ### Create a formatter for the few-shot examples Configure a formatter that will format the few-shot examples into a string. This formatter should be a `PromptTemplate` object. ```python example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\") print(example_prompt.format(**examples[0])) ``` ``` Question: Who lived longer, Muhammad Ali or Alan Turing? Are follow up questions needed here: Yes. Follow up: How old was Muhammad Ali when he died? Intermediate answer: Muhammad Ali was 74 years old when he died. Follow up: How old was Alan Turing when he died? Intermediate answer: Alan Turing was 41 years old when he died. So the final answer is: Muhammad Ali ``` ### Feed examples and formatter to `FewShotPromptTemplate` Finally, create a `FewShotPromptTemplate` object. This object takes in the few-shot examples and the formatter for the few-shot examples. ```python prompt = FewShotPromptTemplate( examples=examples, example_prompt=example_prompt, suffix=\"Question: {input}\", input_variables=[\"input\"] ) print(prompt.format(input=\"Who was the father of Mary Ball Washington?\")) ``` ``` Question: Who lived longer, Muhammad Ali or Alan Turing? Are follow up questions needed here: Yes. Follow up: How old was Muhammad Ali when he died? Intermediate answer: Muhammad Ali was 74 years old when he died. Follow up: How old was Alan Turing when he died? Intermediate answer: Alan Turing was 41 years old when he died. So the final answer is: Muhammad Ali Question: When was the founder of craigslist born? Are follow up questions needed here: Yes. Follow up: Who was the founder of craigslist? Intermediate answer: Craigslist was founded by Craig Newmark. Follow up: When was Craig Newmark born? Intermediate answer: Craig Newmark was born on December 6, 1952. So the final answer is: December 6, 1952 Question: Who was the maternal grandfather of George Washington? Are follow up questions needed here: Yes. Follow up: Who was the mother of George Washington? Intermediate answer: The mother of George Washington was Mary Ball Washington. Follow up: Who was the father of Mary Ball Washington? Intermediate answer: The father of Mary Ball Washington was Joseph Ball. So the final answer is: Joseph Ball Question: Are both the directors of Jaws and Casino Royale from the same country? Are follow up questions needed here: Yes. Follow up: Who is the director of Jaws? Intermediate Answer: The director of Jaws is Steven Spielberg. Follow up: Where is Steven Spielberg from? Intermediate Answer: The United States. Follow up: Who is the director of Casino Royale? Intermediate Answer: The director of Casino Royale is Martin Campbell. Follow up: Where is Martin Campbell from? Intermediate Answer: New Zealand. So the final answer is: No Question: Who was the father of Mary Ball Washington? ``` ## Using an example selector ### Feed examples into `ExampleSelector` We will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the `FewShotPromptTemplate` object, we will feed them into an `ExampleSelector` object. In this tutorial, we will use the `SemanticSimilarityExampleSelector` class. This class selects few-shot examples based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few-shot examples, as well as a vector store to perform the nearest neighbor search. ```python from langchain.prompts.example_selector import SemanticSimilarityExampleSelector from langchain.vectorstores import Chroma from langchain.embeddings import OpenAIEmbeddings example_selector = SemanticSimilarityExampleSelector.from_examples( # This is the list of examples available to select from. examples, # This is the embedding class used to produce embeddings which are used to measure semantic similarity. OpenAIEmbeddings(), # This is the VectorStore class that is used to store the embeddings and do a similarity search over. Chroma, # This is the number of examples to produce. k=1 ) # Select the most similar example to the input. question = \"Who was the father of Mary Ball Washington?\" selected_examples = example_selector.select_examples({\"question\": question}) print(f\"Examples most similar to the input: {question}\") for example in selected_examples: print(\"\\n\") for k, v in example.items(): print(f\"{k}: {v}\") ``` ``` Running Chroma using direct local API. Using DuckDB in-memory for database. Data will be transient. Examples most similar to the input: Who was the father of Mary Ball Washington? question: Who was the maternal grandfather of George Washington? answer: Are follow up questions needed here: Yes. Follow up: Who was the mother of George Washington? Intermediate answer: The mother of George Washington was Mary Ball Washington. Follow up: Who was the father of Mary Ball Washington? Intermediate answer: The father of Mary Ball Washington was Joseph Ball. So the final answer is: Joseph Ball ``` ### Feed example selector into `FewShotPromptTemplate` Finally, create a `FewShotPromptTemplate` object. This object takes in the example selector and the formatter for the few-shot examples. ```python prompt = FewShotPromptTemplate( example_selector=example_selector, example_prompt=example_prompt, suffix=\"Question: {input}\", input_variables=[\"input\"] ) print(prompt.format(input=\"Who was the father of Mary Ball Washington?\")) ``` ``` Question: Who was the maternal grandfather of George Washington? Are follow up questions needed here: Yes. Follow up: Who was the mother of George Washington? Intermediate answer: The mother of George Washington was Mary Ball Washington. Follow up: Who was the father of Mary Ball Washington? Intermediate answer: The father of Mary Ball Washington was Joseph Ball. So the final answer is: Joseph Ball Question: Who was the father of Mary Ball Washington? ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/prompts/prompt_templates/format_output.mdx"}, "data": "# Format template output The output of the format method is available as a string, list of messages and `ChatPromptValue` As string: ```python output = chat_prompt.format(input_language=\"English\", output_language=\"French\", text=\"I love programming.\") output ``` ``` 'System: You are a helpful assistant that translates English to French.\\nHuman: I love programming.' ``` ```python # or alternatively output_2 = chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_string() assert output == output_2 ``` As list of Message objects: ```python chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages() ``` ``` [SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}), HumanMessage(content='I love programming.', additional_kwargs={})] ``` As `ChatPromptValue`: ```python chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\") ``` ``` ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}), HumanMessage(content='I love programming.', additional_kwargs={})]) ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/prompts/prompt_templates/formats.mdx"}, "data": "# Template formats `PromptTemplate` by default uses Python f-string as its template format. However, it can also use other formats like `jinja2`, specified through the `template_format` argument. To use the `jinja2` template: ```python from langchain.prompts import PromptTemplate jinja2_template = \"Tell me a {{ adjective }} joke about {{ content }}\" prompt = PromptTemplate.from_template(jinja2_template, template_format=\"jinja2\") prompt.format(adjective=\"funny\", content=\"chickens\") # Output: Tell me a funny joke about chickens. ``` To use the Python f-string template: ```python from langchain.prompts import PromptTemplate fstring_template = \"\"\"Tell me a {adjective} joke about {content}\"\"\" prompt = PromptTemplate.from_template(fstring_template) prompt.format(adjective=\"funny\", content=\"chickens\") # Output: Tell me a funny joke about chickens. ``` Currently, only `jinja2` and `f-string` are supported. For other formats, kindly raise an issue on the [Github page]("}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/prompts/prompt_templates/msg_prompt_templates.mdx"}, "data": "# Types of `MessagePromptTemplate` LangChain provides different types of `MessagePromptTemplate`. The most commonly used are `AIMessagePromptTemplate`, `SystemMessagePromptTemplate` and `HumanMessagePromptTemplate`, which create an AI message, system message and human message respectively. However, in cases where the chat model supports taking chat message with arbitrary role, you can use `ChatMessagePromptTemplate`, which allows user to specify the role name. ```python from langchain.prompts import ChatMessagePromptTemplate prompt = \"May the {subject} be with you\" chat_message_prompt = ChatMessagePromptTemplate.from_template(role=\"Jedi\", template=prompt) chat_message_prompt.format(subject=\"force\") ``` ``` ChatMessage(content='May the force be with you', additional_kwargs={}, role='Jedi') ``` LangChain also provides `MessagesPlaceholder`, which gives you full control of what messages to be rendered during formatting. This can be useful when you are uncertain of what role you should be using for your message prompt templates or when you wish to insert a list of messages during formatting. ```python from langchain.prompts import MessagesPlaceholder human_prompt = \"Summarize our conversation so far in {word_count} words.\" human_message_template = HumanMessagePromptTemplate.from_template(human_prompt) chat_prompt = ChatPromptTemplate.from_messages([MessagesPlaceholder(variable_name=\"conversation\"), human_message_template]) ``` ```python human_message = HumanMessage(content=\"What is the best way to learn programming?\") ai_message = AIMessage(content=\"\"\"\\ 1. Choose a programming language: Decide on a programming language that you want to learn. 2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures. 3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\ \"\"\") chat_prompt.format_prompt(conversation=[human_message, ai_message], word_count=\"10\").to_messages() ``` ``` [HumanMessage(content='What is the best way to learn programming?', additional_kwargs={}), AIMessage(content='1. Choose a programming language: Decide on a programming language that you want to learn. \\n\\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\\n\\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience', additional_kwargs={}), HumanMessage(content='Summarize our conversation so far in 10 words.', additional_kwargs={})] ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/prompts/prompt_templates/partial.mdx"}, "data": "# Partial prompt templates Like other methods, it can make sense to \"partial\" a prompt template - e.g. pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values. LangChain supports this in two ways: 1. Partial formatting with string values. 2. Partial formatting with functions that return string values. These two different ways support different use cases. In the examples below, we go over the motivations for both use cases as well as how to do it in LangChain. ## Partial with strings One common use case for wanting to partial a prompt template is if you get some of the variables before others. For example, suppose you have a prompt template that requires two variables, `foo` and `baz`. If you get the `foo` value early on in the chain, but the `baz` value later, it can be annoying to wait until you have both variables in the same place to pass them to the prompt template. Instead, you can partial the prompt template with the `foo` value, and then pass the partialed prompt template along and just use that. Below is an example of doing this: ```python from langchain.prompts import PromptTemplate ``` ```python prompt = PromptTemplate(template=\"{foo}{bar}\", input_variables=[\"foo\", \"bar\"]) partial_prompt = prompt.partial(foo=\"foo\"); print(partial_prompt.format(bar=\"baz\")) ``` ``` foobaz ``` You can also just initialize the prompt with the partialed variables. ```python prompt = PromptTemplate(template=\"{foo}{bar}\", input_variables=[\"bar\"], partial_variables={\"foo\": \"foo\"}) print(prompt.format(bar=\"baz\")) ``` ``` foobaz ``` ## Partial with functions The other common use is to partial with a function. The use case for this is when you have a variable you know that you always want to fetch in a common way. A prime example of this is with date or time. Imagine you have a prompt which you always want to have the current date. You can't hard code it in the prompt, and passing it along with the other input variables is a bit annoying. In this case, it's very handy to be able to partial the prompt with a function that always returns the current date. ```python from datetime import datetime def _get_datetime(): now = datetime.now() return now.strftime(\"%m/%d/%Y, %H:%M:%S\") ``` ```python prompt = PromptTemplate( template=\"Tell me a {adjective} joke about the day {date}\", input_variables=[\"adjective\", \"date\"] ); partial_prompt = prompt.partial(date=_get_datetime) print(partial_prompt.format(adjective=\"funny\")) ``` ``` Tell me a funny joke about the day 02/27/2023, 22:15:16 ``` You can also just initialize the prompt with the partialed variables, which often makes more sense in this workflow. ```python prompt = PromptTemplate( template=\"Tell me a {adjective} joke about the day {date}\", input_variables=[\"adjective\"], partial_variables={\"date\": _get_datetime} ); print(prompt.format(adjective=\"funny\")) ``` ``` Tell me a funny joke about the day 02/27/2023, 22:15:16 ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/prompts/prompt_templates/prompt_composition.mdx"}, "data": "# Composition This notebook goes over how to compose multiple prompts together. This can be useful when you want to reuse parts of prompts. This can be done with a PipelinePrompt. A PipelinePrompt consists of two main parts: - Final prompt: The final prompt that is returned - Pipeline prompts: A list of tuples, consisting of a string name and a prompt template. Each prompt template will be formatted and then passed to future prompt templates as a variable with the same name. ```python from langchain.prompts.pipeline import PipelinePromptTemplate from langchain.prompts.prompt import PromptTemplate ``` ```python full_template = \"\"\"{introduction} {example} {start}\"\"\" full_prompt = PromptTemplate.from_template(full_template) ``` ```python introduction_template = \"\"\"You are impersonating {person}.\"\"\" introduction_prompt = PromptTemplate.from_template(introduction_template) ``` ```python example_template = \"\"\"Here's an example of an interaction: Q: {example_q} A: {example_a}\"\"\" example_prompt = PromptTemplate.from_template(example_template) ``` ```python start_template = \"\"\"Now, do this for real! Q: {input} A:\"\"\" start_prompt = PromptTemplate.from_template(start_template) ``` ```python input_prompts = [ (\"introduction\", introduction_prompt), (\"example\", example_prompt), (\"start\", start_prompt) ] pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts) ``` ```python pipeline_prompt.input_variables ``` ``` ['example_a', 'person', 'example_q', 'input'] ``` ```python print(pipeline_prompt.format( person=\"Elon Musk\", example_q=\"What's your favorite car?\", example_a=\"Tesla\", input=\"What's your favorite social media site?\" )) ``` ``` You are impersonating Elon Musk. Here's an example of an interaction: Q: What's your favorite car? A: Tesla Now, do this for real! Q: What's your favorite social media site? A: ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/modules/model_io/prompts/prompt_templates/validate.mdx"}, "data": "# Validate template By default, `PromptTemplate` will validate the `template` string by checking whether the `input_variables` match the variables defined in `template`. You can disable this behavior by setting `validate_template` to `False`. ```python template = \"I am learning langchain because {reason}.\" prompt_template = PromptTemplate(template=template, input_variables=[\"reason\", \"foo\"]) # ValueError due to extra variables prompt_template = PromptTemplate(template=template, input_variables=[\"reason\", \"foo\"], validate_template=False) # No error ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/security.md"}, "data": "# Security LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources. ## Best Practices When building such applications developers should remember to follow good security practices: * [**Limit Permissions**]( Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), etc. as appropriate for your application. * **Anticipate Potential Misuse**: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it\u2019s safest to assume that any LLM able to use those credentials may in fact delete data. * [**Defense in Depth**]( No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It\u2019s best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use. Risks of not doing so include, but are not limited to: * Data corruption or loss. * Unauthorized access to confidential information. * Compromised performance or availability of critical resources. Example scenarios with mitigation strategies: * A user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container. * A user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse. * A user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials. If you're building applications that access external resources like file systems, APIs or databases, consider speaking with your company's security team to determine how to best design and secure your applications. ## Reporting a Vulnerability Please report security vulnerabilities by email to security@langchain.dev. This will ensure the issue is promptly triaged and acted upon as needed. ## Enterprise solutions LangChain may offer enterprise solutions for customers who have additional security requirements. Please contact us at sales@langchain.dev."}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/use_cases/graph/index.mdx"}, "data": "--- sidebar-position: 1 --- # Graph querying Graph databases give us a powerful way to represent and query real-world relationships. There are a number of chains that make it easy to use LLMs to interact with various graph DBs. import DocCardList from \"@theme/DocCardList\";"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/use_cases/question_answering/analyze_document.mdx"}, "data": "# Analyze a single long document The AnalyzeDocumentChain takes in a single document, splits it up, and then runs it through a CombineDocumentsChain. ```python with open(\"../../state_of_the_union.txt\") as f: state_of_the_union = f.read() ``` ```python from langchain.llms import OpenAI from langchain.chains import AnalyzeDocumentChain llm = OpenAI(temperature=0) ``` ```python from langchain.chains.question_answering import load_qa_chain ``` ```python qa_chain = load_qa_chain(llm, chain_type=\"map_reduce\") ``` ```python qa_document_chain = AnalyzeDocumentChain(combine_docs_chain=qa_chain) ``` ```python qa_document_chain.run(input_document=state_of_the_union, question=\"what did the president say about justice breyer?\") ``` ``` ' The president thanked Justice Breyer for his service.' ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/use_cases/question_answering/chat_vector_db.mdx"}, "data": "--- sidebar_position: 2 --- # Remembering chat history The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component. It first combines the chat history (either explicitly passed in or retrieved from the provided memory) and the question into a standalone question, then looks up relevant documents from the retriever, and finally passes those documents and the question to a question-answering chain to return a response. To create one, you will need a retriever. In the below example, we will create one from a vector store, which can be created from embeddings. ```python from langchain.embeddings.openai import OpenAIEmbeddings from langchain.vectorstores import Chroma from langchain.text_splitter import CharacterTextSplitter from langchain.llms import OpenAI from langchain.chains import ConversationalRetrievalChain ``` Load in documents. You can replace this with a loader for whatever type of data you want ```python from langchain.document_loaders import TextLoader loader = TextLoader(\"../../state_of_the_union.txt\") documents = loader.load() ``` If you had multiple loaders that you wanted to combine, you do something like: ```python # loaders = [....] # docs = [] # for loader in loaders: # docs.extend(loader.load()) ``` We now split the documents, create embeddings for them, and put them in a vectorstore. This allows us to do semantic search over them. ```python text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) documents = text_splitter.split_documents(documents) embeddings = OpenAIEmbeddings() vectorstore = Chroma.from_documents(documents, embeddings) ``` ``` Using embedded DuckDB without persistence: data will be transient ``` We can now create a memory object, which is necessary to track the inputs/outputs and hold a conversation. ```python from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True) ``` We now initialize the `ConversationalRetrievalChain` ```python qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), memory=memory) ``` ```python query = \"What did the president say about Ketanji Brown Jackson\" result = qa({\"question\": query}) ``` ```python result[\"answer\"] ``` ``` \" The president said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\" ``` ```python query = \"Did he mention who she succeeded\" result = qa({\"question\": query}) ``` ```python result['answer'] ``` ``` ' Ketanji Brown Jackson succeeded Justice Stephen Breyer on the United States Supreme Court.' ``` ## Pass in chat history In the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. In order to do this, we need to initialize a chain without any memory object. ```python qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever()) ``` Here's an example of asking a question with no chat history ```python chat_history = [] query = \"What did the president say about Ketanji Brown Jackson\" result = qa({\"question\": query, \"chat_history\": chat_history}) ``` ```python result[\"answer\"] ``` ``` \" The president said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\" ``` Here's an example of asking a question with some chat history ```python chat_history = [(query, result[\"answer\"])] query = \"Did he mention who she succeeded\" result = qa({\"question\": query, \"chat_history\": chat_history}) ``` ```python result['answer'] ``` ``` ' Ketanji Brown Jackson succeeded Justice Stephen Breyer on the United States Supreme Court.' ``` ## Using a different model for condensing the question This chain has two steps. First, it condenses the current question and the chat history into a standalone question. This is necessary to create a standanlone vector to use for retrieval. After that, it does retrieval and then answers the question using retrieval augmented generation with a separate model. Part of the power of the declarative nature of LangChain is that you can easily use a separate language model for each call. This can be useful to use a cheaper and faster model for the simpler task of condensing the question, and then a more expensive model for answering the question. Here is an example of doing so. ```python from langchain.chat_models import ChatOpenAI ``` ```python qa = ConversationalRetrievalChain.from_llm( ChatOpenAI(temperature=0, model=\"gpt-4\"), vectorstore.as_retriever(), condense_question_llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo'), ) ``` ```python chat_history = [] query = \"What did the president say about Ketanji Brown Jackson\" result = qa({\"question\": query, \"chat_history\": chat_history}) ``` ```python chat_history = [(query, result[\"answer\"])] query = \"Did he mention who she succeeded\" result = qa({\"question\": query, \"chat_history\": chat_history}) ``` ## Using a custom prompt for condensing the question By default, ConversationalRetrievalQA uses CONDENSE_QUESTION_PROMPT to condense a question. Here is the implementation of this in the docs ```python from langchain.prompts.prompt import PromptTemplate _template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language. Chat History: {chat_history} Follow Up Input: {question} Standalone question:\"\"\" CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template) ``` But instead of this any custom template can be used to further augment information in the question or instruct the LLM to do something. Here is an example ```python from langchain.prompts.prompt import PromptTemplate ``` ```python custom_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. At the end of standalone question add this 'Answer the question in German language.' If you do not know the answer reply with 'I am sorry'. Chat History: {chat_history} Follow Up Input: {question} Standalone question:\"\"\" ``` ```python CUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(custom_template) ``` ```python model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.3) embeddings = OpenAIEmbeddings() vectordb = Chroma(embedding_function=embeddings, persist_directory=directory) memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True) qa = ConversationalRetrievalChain.from_llm( model, vectordb.as_retriever(), condense_question_prompt=CUSTOM_QUESTION_PROMPT, memory=memory ) ``` ```python query = \"What did the president say about Ketanji Brown Jackson\" result = qa({\"question\": query}) ``` ```python query = \"Did he mention who she succeeded\" result = qa({\"question\": query}) ``` ## Return Source Documents You can also easily return source documents from the ConversationalRetrievalChain. This is useful for when you want to inspect what documents were returned. ```python qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True) ``` ```python chat_history = [] query = \"What did the president say about Ketanji Brown Jackson\" result = qa({\"question\": query, \"chat_history\": chat_history}) ``` ```python result['source_documents'][0] ``` ``` Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.', metadata={'source': '../../state_of_the_union.txt'}) ``` ## ConversationalRetrievalChain with `search_distance` If you are using a vector store that supports filtering by search distance, you can add a threshold value parameter. ```python vectordbkwargs = {\"search_distance\": 0.9} ``` ```python qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True) chat_history = [] query = \"What did the president say about Ketanji Brown Jackson\" result = qa({\"question\": query, \"chat_history\": chat_history, \"vectordbkwargs\": vectordbkwargs}) ``` ## ConversationalRetrievalChain with `map_reduce` We can also use different types of combine document chains with the ConversationalRetrievalChain chain. ```python from langchain.chains import LLMChain from langchain.chains.question_answering import load_qa_chain from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT ``` ```python llm = OpenAI(temperature=0) question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT) doc_chain = load_qa_chain(llm, chain_type=\"map_reduce\") chain = ConversationalRetrievalChain( retriever=vectorstore.as_retriever(), question_generator=question_generator, combine_docs_chain=doc_chain, ) ``` ```python chat_history = [] query = \"What did the president say about Ketanji Brown Jackson\" result = chain({\"question\": query, \"chat_history\": chat_history}) ``` ```python result['answer'] ``` ``` \" The president said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, from a family of public school educators and police officers, a consensus builder, and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\" ``` ## ConversationalRetrievalChain with Question Answering with sources You can also use this chain with the question answering with sources chain. ```python from langchain.chains.qa_with_sources import load_qa_with_sources_chain ``` ```python llm = OpenAI(temperature=0) question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT) doc_chain = load_qa_with_sources_chain(llm, chain_type=\"map_reduce\") chain = ConversationalRetrievalChain( retriever=vectorstore.as_retriever(), question_generator=question_generator, combine_docs_chain=doc_chain, ) ``` ```python chat_history = [] query = \"What did the president say about Ketanji Brown Jackson\" result = chain({\"question\": query, \"chat_history\": chat_history}) ``` ```python result['answer'] ``` ``` \" The president said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, from a family of public school educators and police officers, a consensus builder, and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\nSOURCES: ../../state_of_the_union.txt\" ``` ## ConversationalRetrievalChain with streaming to `stdout` Output from the chain will be streamed to `stdout` token by token in this example. ```python from langchain.chains.llm import LLMChain from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT from langchain.chains.question_answering import load_qa_chain # Construct a ConversationalRetrievalChain with a streaming llm for combine docs # and a separate, non-streaming llm for question generation llm = OpenAI(temperature=0) streaming_llm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0) question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT) doc_chain = load_qa_chain(streaming_llm, chain_type=\"stuff\", prompt=QA_PROMPT) qa = ConversationalRetrievalChain( retriever=vectorstore.as_retriever(), combine_docs_chain=doc_chain, question_generator=question_generator) ``` ```python chat_history = [] query = \"What did the president say about Ketanji Brown Jackson\" result = qa({\"question\": query, \"chat_history\": chat_history}) ``` ``` The president said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. ``` ```python chat_history = [(query, result[\"answer\"])] query = \"Did he mention who she succeeded\" result = qa({\"question\": query, \"chat_history\": chat_history}) ``` ``` Ketanji Brown Jackson succeeded Justice Stephen Breyer on the United States Supreme Court. ``` ## get_chat_history Function You can also specify a `get_chat_history` function, which can be used to format the chat_history string. ```python def get_chat_history(inputs) -> str: res = [] for human, ai in inputs: res.append(f\"Human:{human}\\nAI:{ai}\") return \"\\n\".join(res) qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), get_chat_history=get_chat_history) ``` ```python chat_history = [] query = \"What did the president say about Ketanji Brown Jackson\" result = qa({\"question\": query, \"chat_history\": chat_history}) ``` ```python result['answer'] ``` ``` \" The president said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\" ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/use_cases/question_answering/in_memory_question_answering.mdx"}, "data": "# RAG over in-memory documents Here we walk through how to use LangChain for question answering over a list of documents. Under the hood we'll be using our [Document chains](/docs/modules/chains/document/). ## Prepare Data First we prepare the data. For this example we do similarity search over a vector database, but these documents could be fetched in any manner (the point of this notebook to highlight what to do AFTER you fetch the documents). ```python from langchain.embeddings.openai import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import Chroma from langchain.docstore.document import Document from langchain.prompts import PromptTemplate from langchain.indexes.vectorstore import VectorstoreIndexCreator ``` ```python with open(\"../../state_of_the_union.txt\") as f: state_of_the_union = f.read() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_text(state_of_the_union) embeddings = OpenAIEmbeddings() ``` ```python docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]).as_retriever() ``` ``` Running Chroma using direct local API. Using DuckDB in-memory for database. Data will be transient. ``` ```python query = \"What did the president say about Justice Breyer\" docs = docsearch.get_relevant_documents(query) ``` ```python from langchain.chains.question_answering import load_qa_chain from langchain.llms import OpenAI ``` ## Quickstart If you just want to get started as quickly as possible, this is the recommended way to do it: ```python chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\") query = \"What did the president say about Justice Breyer\" chain.run(input_documents=docs, question=query) ``` ``` ' The president said that Justice Breyer has dedicated his life to serve the country and thanked him for his service.' ``` If you want more control and understanding over what is happening, please see the information below. ## The `stuff` Chain This sections shows results of using the `stuff` Chain to do question answering. ```python chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\") ``` ```python query = \"What did the president say about Justice Breyer\" chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True) ``` ``` {'output_text': ' The president said that Justice Breyer has dedicated his life to serve the country and thanked him for his service.'} ``` **Custom Prompts** You can also use your own prompts with this chain. In this example, we will respond in Italian. ```python prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. {context} Question: {question} Answer in Italian:\"\"\" PROMPT = PromptTemplate( template=prompt_template, input_variables=[\"context\", \"question\"] ) chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\", prompt=PROMPT) chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True) ``` ``` {'output_text': ' Il presidente ha detto che Justice Breyer ha dedicato la sua vita a servire questo paese e ha ricevuto una vasta gamma di supporto.'} ``` ## The `map_reduce` Chain This sections shows results of using the `map_reduce` Chain to do question answering. ```python chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_reduce\") ``` ```python query = \"What did the president say about Justice Breyer\" chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True) ``` ``` {'output_text': ' The president said that Justice Breyer is an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court, and thanked him for his service.'} ``` **Intermediate Steps** We can also return the intermediate steps for `map_reduce` chains, should we want to inspect them. This is done with the `return_map_steps` variable. ```python chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_map_steps=True) ``` ```python chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True) ``` ``` {'intermediate_steps': [' \"Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\"', ' A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.', ' None', ' None'], 'output_text': ' The president said that Justice Breyer is an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court, and thanked him for his service.'} ``` **Custom Prompts** You can also use your own prompts with this chain. In this example, we will respond in Italian. ```python question_prompt_template = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text translated into italian. {context} Question: {question} Relevant text, if any, in Italian:\"\"\" QUESTION_PROMPT = PromptTemplate( template=question_prompt_template, input_variables=[\"context\", \"question\"] ) combine_prompt_template = \"\"\"Given the following extracted parts of a long document and a question, create a final answer italian. If you don't know the answer, just say that you don't know. Don't try to make up an answer. QUESTION: {question} ========= {summaries} ========= Answer in Italian:\"\"\" COMBINE_PROMPT = PromptTemplate( template=combine_prompt_template, input_variables=[\"summaries\", \"question\"] ) chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_map_steps=True, question_prompt=QUESTION_PROMPT, combine_prompt=COMBINE_PROMPT) chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True) ``` ``` {'intermediate_steps': [\"\\nStasera vorrei onorare qualcuno che ha dedicato la sua vita a servire questo paese: il giustizia Stephen Breyer - un veterano dell'esercito, uno studioso costituzionale e un giustizia in uscita della Corte Suprema degli Stati Uniti. Giustizia Breyer, grazie per il tuo servizio.\", '\\nNessun testo pertinente.', ' Non ha detto nulla riguardo a Justice Breyer.', \" Non c'\u00e8 testo pertinente.\"], 'output_text': ' Non ha detto nulla riguardo a Justice Breyer.'} ``` **Batch Size** When using the `map_reduce` chain, one thing to keep in mind is the batch size you are using during the map step. If this is too high, it could cause rate limiting errors. You can control this by setting the batch size on the LLM used. Note that this only applies for LLMs with this parameter. Below is an example of doing so: ```python llm = OpenAI(batch_size=5, temperature=0) ``` ## The `refine` Chain This sections shows results of using the `refine` Chain to do question answering. ```python chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"refine\") ``` ```python query = \"What did the president say about Justice Breyer\" chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True) ``` ``` {'output_text': '\\n\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country, his legacy of excellence, and his commitment to advancing liberty and justice, as well as for his support of the Equality Act and his commitment to protecting the rights of LGBTQ+ Americans. He also praised Justice Breyer for his role in helping to pass the Bipartisan Infrastructure Law, which he said would be the most sweeping investment to rebuild America in history and would help the country compete for the jobs of the 21st Century.'} ``` **Intermediate Steps** We can also return the intermediate steps for `refine` chains, should we want to inspect them. This is done with the `return_refine_steps` variable. ```python chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"refine\", return_refine_steps=True) ``` ```python chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True) ``` ``` {'intermediate_steps': ['\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country and his legacy of excellence.', '\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country, his legacy of excellence, and his commitment to advancing liberty and justice.', '\\n\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country, his legacy of excellence, and his commitment to advancing liberty and justice, as well as for his support of the Equality Act and his commitment to protecting the rights of LGBTQ+ Americans.', '\\n\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country, his legacy of excellence, and his commitment to advancing liberty and justice, as well as for his support of the Equality Act and his commitment to protecting the rights of LGBTQ+ Americans. He also praised Justice Breyer for his role in helping to pass the Bipartisan Infrastructure Law, which is the most sweeping investment to rebuild America in history.'], 'output_text': '\\n\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country, his legacy of excellence, and his commitment to advancing liberty and justice, as well as for his support of the Equality Act and his commitment to protecting the rights of LGBTQ+ Americans. He also praised Justice Breyer for his role in helping to pass the Bipartisan Infrastructure Law, which is the most sweeping investment to rebuild America in history.'} ``` **Custom Prompts** You can also use your own prompts with this chain. In this example, we will respond in Italian. ```python refine_prompt_template = ( \"The original question is as follows: {question}\\n\" \"We have provided an existing answer: {existing_answer}\\n\" \"We have the opportunity to refine the existing answer\" \"(only if needed) with some more context below.\\n\" \"------------\\n\" \"{context_str}\\n\" \"------------\\n\" \"Given the new context, refine the original answer to better \" \"answer the question. \" \"If the context isn't useful, return the original answer. Reply in Italian.\" ) refine_prompt = PromptTemplate( input_variables=[\"question\", \"existing_answer\", \"context_str\"], template=refine_prompt_template, ) initial_qa_template = ( \"Context information is below. \\n\" \"---------------------\\n\" \"{context_str}\" \"\\n---------------------\\n\" \"Given the context information and not prior knowledge, \" \"answer the question: {question}\\nYour answer should be in Italian.\\n\" ) initial_qa_prompt = PromptTemplate( input_variables=[\"context_str\", \"question\"], template=initial_qa_template ) chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"refine\", return_refine_steps=True, question_prompt=initial_qa_prompt, refine_prompt=refine_prompt) chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True) ``` ``` {'intermediate_steps': ['\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese e ha reso omaggio al suo servizio.', \"\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha reso omaggio al suo servizio e ha sostenuto la nomina di una top litigatrice in pratica privata, un ex difensore pubblico federale e una famiglia di insegnanti e agenti di polizia delle scuole pubbliche. Ha anche sottolineato l'importanza di avanzare la libert\u00e0 e la giustizia attraverso la sicurezza delle frontiere e la risoluzione del sistema di immigrazione.\", \"\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha reso omaggio al suo servizio e ha sostenuto la nomina di una top litigatrice in pratica privata, un ex difensore pubblico federale e una famiglia di insegnanti e agenti di polizia delle scuole pubbliche. Ha anche sottolineato l'importanza di avanzare la libert\u00e0 e la giustizia attraverso la sicurezza delle frontiere, la risoluzione del sistema di immigrazione, la protezione degli americani LGBTQ+ e l'approvazione dell'Equality Act. Ha inoltre sottolineato l'importanza di lavorare insieme per sconfiggere l'epidemia di oppiacei.\", \"\\n\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha reso omaggio al suo servizio e ha sostenuto la nomina di una top litigatrice in pratica privata, un ex difensore pubblico federale e una famiglia di insegnanti e agenti di polizia delle scuole pubbliche. Ha anche sottolineato l'importanza di avanzare la libert\u00e0 e la giustizia attraverso la sicurezza delle frontiere, la risoluzione del sistema di immigrazione, la protezione degli americani LGBTQ+ e l'approvazione dell'Equality Act. Ha inoltre sottolineato l'importanza di lavorare insieme per sconfiggere l'epidemia di oppiacei e per investire in America, educare gli americani, far crescere la forza lavoro e costruire l'economia dal\"], 'output_text': \"\\n\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha reso omaggio al suo servizio e ha sostenuto la nomina di una top litigatrice in pratica privata, un ex difensore pubblico federale e una famiglia di insegnanti e agenti di polizia delle scuole pubbliche. Ha anche sottolineato l'importanza di avanzare la libert\u00e0 e la giustizia attraverso la sicurezza delle frontiere, la risoluzione del sistema di immigrazione, la protezione degli americani LGBTQ+ e l'approvazione dell'Equality Act. Ha inoltre sottolineato l'importanza di lavorare insieme per sconfiggere l'epidemia di oppiacei e per investire in America, educare gli americani, far crescere la forza lavoro e costruire l'economia dal\"} ``` ## The `map-rerank` Chain This sections shows results of using the `map-rerank` Chain to do question answering with sources. ```python chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_rerank\", return_intermediate_steps=True) ``` ```python query = \"What did the president say about Justice Breyer\" results = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True) ``` ```python results[\"output_text\"] ``` ``` ' The President thanked Justice Breyer for his service and honored him for dedicating his life to serve the country.' ``` ```python results[\"intermediate_steps\"] ``` ``` [{'answer': ' The President thanked Justice Breyer for his service and honored him for dedicating his life to serve the country.', 'score': '100'}, {'answer': ' This document does not answer the question', 'score': '0'}, {'answer': ' This document does not answer the question', 'score': '0'}, {'answer': ' This document does not answer the question', 'score': '0'}] ``` **Custom Prompts** You can also use your own prompts with this chain. In this example, we will respond in Italian. ```python from langchain.output_parsers import RegexParser output_parser = RegexParser( regex=r\"(.*?)\\nScore: (.*)\", output_keys=[\"answer\", \"score\"], ) prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format: Question: [question here] Helpful Answer In Italian: [answer here] Score: [score between 0 and 100] Begin! Context: --------- {context} --------- Question: {question} Helpful Answer In Italian:\"\"\" PROMPT = PromptTemplate( template=prompt_template, input_variables=[\"context\", \"question\"], output_parser=output_parser, ) chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_rerank\", return_intermediate_steps=True, prompt=PROMPT) query = \"What did the president say about Justice Breyer\" chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True) ``` ``` {'intermediate_steps': [{'answer': ' Il presidente ha detto che Justice Breyer ha dedicato la sua vita a servire questo paese.', 'score': '100'}, {'answer': ' Il presidente non ha detto nulla sulla Giustizia Breyer.', 'score': '100'}, {'answer': ' Non so.', 'score': '0'}, {'answer': ' Non so.', 'score': '0'}], 'output_text': ' Il presidente ha detto che Justice Breyer ha dedicato la sua vita a servire questo paese.'} ``` ## Document QA with sources We can also perform document QA and return the sources that were used to answer the question. To do this we'll just need to make sure each document has a \"source\" key in the metadata, and we'll use the `load_qa_with_sources` helper to construct our chain: ```python docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]) query = \"What did the president say about Justice Breyer\" docs = docsearch.similarity_search(query) ``` ```python from langchain.chains.qa_with_sources import load_qa_with_sources_chain chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\") query = \"What did the president say about Justice Breyer\" chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True) ``` ``` {'output_text': ' The president thanked Justice Breyer for his service.\\nSOURCES: 30-pl'} ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/use_cases/question_answering/multi_retrieval_qa_router.mdx"}, "data": "# Dynamically select from multiple retrievers This notebook demonstrates how to use the `RouterChain` paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the `MultiRetrievalQAChain` to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it. ```python from langchain.chains.router import MultiRetrievalQAChain from langchain.llms import OpenAI ``` ```python from langchain.embeddings import OpenAIEmbeddings from langchain.document_loaders import TextLoader from langchain.vectorstores import FAISS sou_docs = TextLoader('../../state_of_the_union.txt').load_and_split() sou_retriever = FAISS.from_documents(sou_docs, OpenAIEmbeddings()).as_retriever() pg_docs = TextLoader('../../paul_graham_essay.txt').load_and_split() pg_retriever = FAISS.from_documents(pg_docs, OpenAIEmbeddings()).as_retriever() personal_texts = [ \"I love apple pie\", \"My favorite color is fuchsia\", \"My dream is to become a professional dancer\", \"I broke my arm when I was 12\", \"My parents are from Peru\", ] personal_retriever = FAISS.from_texts(personal_texts, OpenAIEmbeddings()).as_retriever() ``` ```python retriever_infos = [ { \"name\": \"state of the union\", \"description\": \"Good for answering questions about the 2023 State of the Union address\", \"retriever\": sou_retriever }, { \"name\": \"pg essay\", \"description\": \"Good for answering questions about Paul Graham's essay on his career\", \"retriever\": pg_retriever }, { \"name\": \"personal\", \"description\": \"Good for answering questions about me\", \"retriever\": personal_retriever } ] ``` ```python chain = MultiRetrievalQAChain.from_retrievers(OpenAI(), retriever_infos, verbose=True) ``` ```python print(chain.run(\"What did the president say about the economy?\")) ``` ``` > Entering new MultiRetrievalQAChain chain... state of the union: {'query': 'What did the president say about the economy in the 2023 State of the Union address?'} > Finished chain. The president said that the economy was stronger than it had been a year prior, and that the American Rescue Plan helped create record job growth and fuel economic relief for millions of Americans. He also proposed a plan to fight inflation and lower costs for families, including cutting the cost of prescription drugs and energy, providing investments and tax credits for energy efficiency, and increasing access to child care and Pre-K. ``` ```python print(chain.run(\"What is something Paul Graham regrets about his work?\")) ``` ``` > Entering new MultiRetrievalQAChain chain... pg essay: {'query': 'What is something Paul Graham regrets about his work?'} > Finished chain. Paul Graham regrets that he did not take a vacation after selling his company, instead of immediately starting to paint. ``` ```python print(chain.run(\"What is my background?\")) ``` ``` > Entering new MultiRetrievalQAChain chain... personal: {'query': 'What is my background?'} > Finished chain. Your background is Peruvian. ``` ```python print(chain.run(\"What year was the Internet created in?\")) ``` ``` > Entering new MultiRetrievalQAChain chain... None: {'query': 'What year was the Internet created in?'} > Finished chain. The Internet was created in 1969 through a project called ARPANET, which was funded by the United States Department of Defense. However, the World Wide Web, which is often confused with the Internet, was created in 1989 by British computer scientist Tim Berners-Lee. ```"}
{"metadata": {"owner": "langchain-ai", "repo": "langchain", "path": "docs/docs/use_cases/question_answering/vector_db_qa.mdx"}, "data": "--- sidebar_position: 1 --- # Using a Retriever This example showcases question answering over an index. ```python from langchain.chains import RetrievalQA from langchain.document_loaders import TextLoader from langchain.embeddings.openai import OpenAIEmbeddings from langchain.llms import OpenAI from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import Chroma ``` ```python loader = TextLoader(\"../../state_of_the_union.txt\") documents = loader.load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_documents(documents) embeddings = OpenAIEmbeddings() docsearch = Chroma.from_documents(texts, embeddings) qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever()) ``` ```python query = \"What did the president say about Ketanji Brown Jackson\" qa.run(query) ``` ``` \" The president said that she is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support, from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\" ``` ## Chain Type You can easily specify different chain types to load and use in the RetrievalQA chain. For a more detailed walkthrough of these types, please see [this notebook](/docs/modules/chains/additional/question_answering). There are two ways to load different chain types. First, you can specify the chain type argument in the `from_chain_type` method. This allows you to pass in the name of the chain type you want to use. For example, in the below we change the chain type to `map_reduce`. ```python qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"map_reduce\", retriever=docsearch.as_retriever()) ``` ```python query = \"What did the president say about Ketanji Brown Jackson\" qa.run(query) ``` ``` \" The president said that Judge Ketanji Brown Jackson is one of our nation's top legal minds, a former top litigator in private practice and a former federal public defender, from a family of public school educators and police officers, a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\" ``` The above way allows you to really simply change the chain_type, but it doesn't provide a ton of flexibility over parameters to that chain type. If you want to control those parameters, you can load the chain directly (as you did in [this notebook](/docs/modules/chains/additional/question_answering)) and then pass that directly to the RetrievalQA chain with the `combine_documents_chain` parameter. For example: ```python from langchain.chains.question_answering import load_qa_chain qa_chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\") qa = RetrievalQA(combine_documents_chain=qa_chain, retriever=docsearch.as_retriever()) ``` ```python query = \"What did the president say about Ketanji Brown Jackson\" qa.run(query) ``` ``` \" The president said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\" ``` ## Custom Prompts You can pass in custom prompts to do question answering. These prompts are the same prompts as you can pass into the [base question answering chain](/docs/modules/chains/additional/question_answering) ```python from langchain.prompts import PromptTemplate prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. {context} Question: {question} Answer in Italian:\"\"\" PROMPT = PromptTemplate( template=prompt_template, input_variables=[\"context\", \"question\"] ) ``` ```python chain_type_kwargs = {\"prompt\": PROMPT} qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever(), chain_type_kwargs=chain_type_kwargs) ``` ```python query = \"What did the president say about Ketanji Brown Jackson\" qa.run(query) ``` ``` \" Il presidente ha detto che Ketanji Brown Jackson \u00e8 una delle menti legali pi\u00f9 importanti del paese, che continuer\u00e0 l'eccellenza di Justice Breyer e che ha ricevuto un ampio sostegno, da Fraternal Order of Police a ex giudici nominati da democratici e repubblicani.\" ``` ## Vectorstore Retriever Options You can adjust how documents are retrieved from your vectorstore depending on the specific task. There are two main ways to retrieve documents relevant to a query- Similarity Search and Max Marginal Relevance Search (MMR Search). Similarity Search is the default, but you can use MMR by adding the `search_type` parameter: ```python docsearch.as_retriever(search_type=\"mmr\") ``` You can also modify the search by passing specific search arguments through the retriever to the search function, using the `search_kwargs` keyword argument. - `k` defines how many documents are returned; defaults to 4. - `score_threshold` allows you to set a minimum relevance for documents returned by the retriever, if you are using the \"similarity_score_threshold\" search type. - `fetch_k` determines the amount of documents to pass to the MMR algorithm; defaults to 20. - `lambda_mult` controls the diversity of results returned by the MMR algorithm, with 1 being minimum diversity and 0 being maximum. Defaults to 0.5. - `filter` allows you to define a filter on what documents should be retrieved, based on the documents' metadata. This has no effect if the Vectorstore doesn't store any metadata. Some examples for how these parameters can be used: ```python # Retrieve more documents with higher diversity- useful if your dataset has many similar documents docsearch.as_retriever(search_type=\"mmr\", search_kwargs={'k': 6, 'lambda_mult': 0.25}) # Fetch more documents for the MMR algorithm to consider, but only return the top 5 docsearch.as_retriever(search_type=\"mmr\", search_kwargs={'k': 5, 'fetch_k': 50}) # Only retrieve documents that have a relevance score above a certain threshold docsearch.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8}) # Only get the single most similar document from the dataset docsearch.as_retriever(search_kwargs={'k': 1}) # Use a filter to only retrieve documents from a specific paper docsearch.as_retriever(search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}) ``` ## Return Source Documents Additionally, we can return the source documents used to answer the question by specifying an optional parameter when constructing the chain. ```python qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever(search_type=\"mmr\", search_kwargs={'fetch_k': 30}), return_source_documents=True) ``` ```python query = \"What did the president say about Ketanji Brown Jackson\" result = qa({\"query\": query}) ``` ```python result[\"result\"] ``` ``` \" The president said that Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice and a former federal public defender from a family of public school educators and police officers, and that she has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\" ``` ```python result[\"source_documents\"] ``` ``` [Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.', lookup_str='', metadata={'source': '../../state_of_the_union.txt'}, lookup_index=0), Document(page_content='A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\n\\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling. \\n\\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers. \\n\\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\n\\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.', lookup_str='', metadata={'source': '../../state_of_the_union.txt'}, lookup_index=0), Document(page_content='And for our LGBTQ+ Americans, let\u2019s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \\n\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\n\\nWhile it often appears that we never agree, that isn\u2019t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \\n\\nAnd soon, we\u2019ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \\n\\nSo tonight I\u2019m offering a Unity Agenda for the Nation. Four big things we can do together. \\n\\nFirst, beat the opioid epidemic.', lookup_str='', metadata={'source': '../../state_of_the_union.txt'}, lookup_index=0), Document(page_content='Tonight, I\u2019m announcing a crackdown on these companies overcharging American businesses and consumers. \\n\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up. \\n\\nThat ends on my watch. \\n\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \\n\\nWe\u2019ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \\n\\nLet\u2019s pass the Paycheck Fairness Act and paid leave. \\n\\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \\n\\nLet\u2019s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill\u2014our First Lady who teaches full-time\u2014calls America\u2019s best-kept secret: community colleges.', lookup_str='', metadata={'source': '../../state_of_the_union.txt'}, lookup_index=0)] ``` Alternatively, if our document have a \"source\" metadata key, we can use the `RetrievalQAWithSourcesChain` to cite our sources: ```python docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": f\"{i}-pl\"} for i in range(len(texts))]) ``` ```python from langchain.chains import RetrievalQAWithSourcesChain from langchain.llms import OpenAI chain = RetrievalQAWithSourcesChain.from_chain_type(OpenAI(temperature=0), chain_type=\"stuff\", retriever=docsearch.as_retriever()) ``` ```python chain({\"question\": \"What did the president say about Justice Breyer\"}, return_only_outputs=True) ``` ``` {'answer': ' The president honored Justice Breyer for his service and mentioned his legacy of excellence.\\n', 'sources': '31-pl'} ```"}
